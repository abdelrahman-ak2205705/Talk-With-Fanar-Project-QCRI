{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e5796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# import openai\n",
    "from openai import AzureOpenAI\n",
    "# !pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3174aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def APIKeyManager(model_type, key_path):\n",
    "    \n",
    "    load_dotenv(dotenv_path=key_path, override=True)\n",
    "    if model_type=='azure':\n",
    "        client = AzureOpenAI(\n",
    "            api_version=os.environ[\"AZURE_API_VERSION\"],\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_API_KEY\"],\n",
    "        )\n",
    "        return client\n",
    "    elif model_type=='fanar':\n",
    "        client = OpenAI(\n",
    "            base_url = \"https://api.fanar.qa/v1\",\n",
    "            api_key  = os.environ[\"FANAR_API_KEY\"],\n",
    "        )\n",
    "        client.default_params = {\"model\": \"Fanar-C-1-8.7B\"}\n",
    "        return client    \n",
    "    elif model_type=='gemini':\n",
    "        pass\n",
    "    return client\n",
    "\n",
    "# Load environment variables\n",
    "model_type=\"fanar\"\n",
    "deployment = APIKeyManager(model_type, \"./azure.env\")\n",
    "model = \"Fanar-C-1-8.7B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a84582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class TopicClassifier:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def classify_topic(self, topic, information):\n",
    "        \"\"\"\n",
    "        Classify podcast topic and determine optimal approach\n",
    "        \n",
    "        Args:\n",
    "            topic: Main topic of the podcast episode\n",
    "            information: Background information about the topic\n",
    "            \n",
    "        Returns:\n",
    "            JSON with classification results\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in analyzing and classifying topics for Arabic podcast production.\n",
    "\n",
    "Task: Analyze the following topic and determine the best approach for an Arabic podcast.\n",
    "\n",
    "Topic: {topic}\n",
    "Background Information: {information}\n",
    "\n",
    "Analyze the topic and return the result in JSON format with these exact keys:\n",
    "\n",
    "{{\n",
    "    \"primary_category\": \"Main category from the available options\",\n",
    "    \"category_justification\": \"Reason for choosing this category\",\n",
    "    \"optimal_style\": \"Best discussion style from available options\",\n",
    "    \"discourse_pattern\": \"Appropriate discourse pattern\",\n",
    "    \"audience_engagement_goal\": \"Audience engagement objective\",\n",
    "    \"cultural_sensitivity_level\": \"Cultural sensitivity level\",\n",
    "    \"controversy_potential\": \"Controversy potential level\",\n",
    "    \"key_discussion_angles\": [\n",
    "        \"First main discussion angle\",\n",
    "        \"Second point of interest for Arabic audiences\"\n",
    "    ],\n",
    "    \"natural_tension_points\": [\n",
    "        \"First natural tension point in the topic\",\n",
    "        \"Second aspect that might generate healthy debate\"\n",
    "    ],\n",
    "    \"cultural_connection_opportunities\": [\n",
    "        \"First opportunity to connect with Arabic culture\",\n",
    "        \"Second relevant local or regional reference\"\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Available Categories (choose one):\n",
    "1. \"العلوم والتكنولوجيا\" - For technical, scientific topics and innovations\n",
    "2. \"السياسة والشؤون العامة\" - For political topics, current events, public affairs\n",
    "3. \"القضايا الاجتماعية\" - For social topics, relationships, values, social challenges\n",
    "4. \"الرياضة والترفيه\" - For sports, arts, entertainment topics\n",
    "5. \"التاريخ والثقافة\" - For historical, heritage, cultural topics\n",
    "\n",
    "Available Styles (choose one):\n",
    "- \"حواري\" - Natural friendly dialogue between host and guest\n",
    "- \"تعليمي\" - Focus on explanation and education in entertaining way\n",
    "- \"ترفيهي\" - Fun and light with humorous touches\n",
    "- \"تحليلي\" - Deep, specialized analytical discussion\n",
    "\n",
    "Discourse Patterns (choose one):\n",
    "- \"رسمي\" - Formal and respectful language\n",
    "- \"ودي\" - Warm and familiar language\n",
    "- \"جدلي\" - Lively discussion with multiple viewpoints\n",
    "- \"سردي\" - Storytelling and narrative style\n",
    "\n",
    "Cultural Sensitivity Levels (choose one):\n",
    "- \"عالي\" - Requires extreme caution in handling\n",
    "- \"متوسط\" - Needs moderate cultural consideration\n",
    "- \"منخفض\" - Generally acceptable topic\n",
    "\n",
    "Controversy Potential (choose one):\n",
    "- \"عالية\" - Inherently controversial topic\n",
    "- \"متوسطة\" - May generate some disagreements\n",
    "- \"منخفضة\" - Generally acceptable topic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All JSON values must be in Modern Standard Arabic (MSA)\n",
    "- JSON keys must be in English\n",
    "- Use ONLY English commas (,) - NEVER Arabic commas (،)\n",
    "- Use ONLY standard double quotes (\") - NEVER Arabic quotes\n",
    "- Do NOT include any explanatory text before or after JSON\n",
    "- Do NOT include confidence scores like \"الثقة: 95%\"\n",
    "- Do NOT include ```json markers\n",
    "- Return ONLY valid JSON that can be parsed by json.loads()\n",
    "- Analyze the topic deeply considering Arabic cultural context\n",
    "- Focus on what makes the topic appealing to Arabic audiences\n",
    "- Optimal episode duration is 10 minutes\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in analyzing topics for Arabic podcasts. Return ONLY valid JSON with English punctuation. No explanatory text. No confidence scores. No Arabic commas.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3  # Lower temperature for more consistent classification\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Clean the JSON response to ensure it's parseable\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before the first { and after the last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('،', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove confidence scores and meta text\n",
    "        meta_patterns = [\n",
    "            r'الثقة:\\s*\\d+%',\n",
    "            r'الدقة:\\s*\\d+%',\n",
    "            r'معدل الثقة:\\s*\\d+%',\n",
    "            r'\\n.*الثقة.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*'\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def classify_with_validation(self, topic, information):\n",
    "        \"\"\"\n",
    "        Classify topic with automatic validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get classification\n",
    "                classification_result = self.classify_topic(topic, information)\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(classification_result)\n",
    "                \n",
    "                # Validate required fields\n",
    "                required_fields = [\n",
    "                    \"primary_category\", \"category_justification\", \"optimal_style\",\n",
    "                    \"discourse_pattern\", \"audience_engagement_goal\", \n",
    "                    \"cultural_sensitivity_level\", \"controversy_potential\",\n",
    "                    \"key_discussion_angles\", \"natural_tension_points\",\n",
    "                    \"cultural_connection_opportunities\"\n",
    "                ]\n",
    "                \n",
    "                missing_fields = [field for field in required_fields if field not in parsed_result]\n",
    "                \n",
    "                if not missing_fields:\n",
    "                    print(f\"✅ Classification successful on attempt {attempt + 1}\")\n",
    "                    return classification_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1}: Missing fields: {missing_fields}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(classification_result[:500])\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"📝 Using fallback classification...\")\n",
    "        fallback_result = self._get_fallback_classification(topic)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _get_fallback_classification(self, topic):\n",
    "        \"\"\"Provide fallback classification if all attempts fail\"\"\"\n",
    "        return {\n",
    "            \"primary_category\": \"القضايا الاجتماعية\",\n",
    "            \"category_justification\": \"تصنيف افتراضي للموضوع المطروح\",\n",
    "            \"optimal_style\": \"حواري\",\n",
    "            \"discourse_pattern\": \"ودي\",\n",
    "            \"audience_engagement_goal\": \"زيادة الوعي والفهم حول الموضوع\",\n",
    "            \"cultural_sensitivity_level\": \"متوسط\",\n",
    "            \"controversy_potential\": \"متوسطة\",\n",
    "            \"key_discussion_angles\": [\n",
    "                \"الجوانب الأساسية للموضوع\",\n",
    "                \"التأثيرات على المجتمع العربي\"\n",
    "            ],\n",
    "            \"natural_tension_points\": [\n",
    "                \"وجهات النظر المختلفة حول الموضوع\",\n",
    "                \"التحديات والفرص المرتبطة\"\n",
    "            ],\n",
    "            \"cultural_connection_opportunities\": [\n",
    "                \"الربط بالقيم العربية التقليدية\",\n",
    "                \"التجارب المحلية ذات الصلة\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def analyze_classification_quality(self, parsed_result):\n",
    "        \"\"\"Analyze the quality of the classification result\"\"\"\n",
    "        analysis = {\n",
    "            \"category_appropriateness\": self._assess_category_fit(parsed_result.get(\"primary_category\", \"\")),\n",
    "            \"style_consistency\": self._assess_style_choice(parsed_result.get(\"optimal_style\", \"\")),\n",
    "            \"cultural_awareness\": len(parsed_result.get(\"cultural_connection_opportunities\", [])),\n",
    "            \"discussion_depth\": len(parsed_result.get(\"key_discussion_angles\", [])),\n",
    "            \"sensitivity_awareness\": parsed_result.get(\"cultural_sensitivity_level\", \"\") != \"\",\n",
    "            \"engagement_focus\": parsed_result.get(\"audience_engagement_goal\", \"\") != \"\"\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        score_factors = [\n",
    "            analysis[\"category_appropriateness\"] > 0,\n",
    "            analysis[\"style_consistency\"],\n",
    "            analysis[\"cultural_awareness\"] >= 2,\n",
    "            analysis[\"discussion_depth\"] >= 2,\n",
    "            analysis[\"sensitivity_awareness\"],\n",
    "            analysis[\"engagement_focus\"]\n",
    "        ]\n",
    "        \n",
    "        analysis[\"overall_score\"] = sum(score_factors) * 100 // len(score_factors)\n",
    "        analysis[\"quality_grade\"] = (\n",
    "            \"ممتاز\" if analysis[\"overall_score\"] >= 85 else\n",
    "            \"جيد\" if analysis[\"overall_score\"] >= 70 else\n",
    "            \"مقبول\" if analysis[\"overall_score\"] >= 55 else\n",
    "            \"يحتاج تحسين\"\n",
    "        )\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _assess_category_fit(self, category):\n",
    "        \"\"\"Assess if the category choice seems appropriate\"\"\"\n",
    "        valid_categories = [\n",
    "            \"العلوم والتكنولوجيا\", \"السياسة والشؤون العامة\", \n",
    "            \"القضايا الاجتماعية\", \"الرياضة والترفيه\", \"التاريخ والثقافة\"\n",
    "        ]\n",
    "        return 1 if category in valid_categories else 0\n",
    "\n",
    "    def _assess_style_choice(self, style):\n",
    "        \"\"\"Assess if the style choice is valid\"\"\"\n",
    "        valid_styles = [\"حواري\", \"تعليمي\", \"ترفيهي\", \"تحليلي\"]\n",
    "        return style in valid_styles\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_topic_classifier(deployment, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the topic classifier with comprehensive validation\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Topic Classifier with Enhanced Validation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    classifier = TopicClassifier(deployment, model_name)\n",
    "    \n",
    "    # Test topic\n",
    "    topic = \"الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\"\n",
    "    \n",
    "    information = '''\n",
    "مع انتشار تقنيات الذكاء الاصطناعي بسرعة في العالم العربي، تزداد المخاوف حول تأثيرها على الهوية الثقافية واللغة العربية. \n",
    "تشير الدراسات إلى أن 78% من المحتوى الرقمي باللغة الإنجليزية، بينما المحتوى العربي لا يتجاوز 3%. \n",
    "معظم نماذج الذكاء الاصطناعي الحالية مدربة على بيانات غربية، مما يثير تساؤلات حول قدرتها على فهم السياق الثقافي العربي.\n",
    "في المقابل، تسعى دول مثل الإمارات والسعودية لتطوير نماذج ذكاء اصطناعي عربية مثل \"جايس\" و\"الحوراء\" لمواجهة هذا التحدي.\n",
    "التحدي الأكبر يكمن في كيفية الاستفادة من هذه التقنيات لتعزيز الثقافة العربية بدلاً من تهميشها، وضمان أن تخدم الذكاء الاصطناعي قيمنا ومبادئنا.\n",
    "'''\n",
    "    \n",
    "    # Run classification with validation\n",
    "    classification_result, parsed_result = classifier.classify_with_validation(topic, information)\n",
    "    \n",
    "    print(\"📊 Classification Results:\")\n",
    "    print(f\"Primary Category: {parsed_result.get('primary_category', 'N/A')}\")\n",
    "    print(f\"Optimal Style: {parsed_result.get('optimal_style', 'N/A')}\")\n",
    "    print(f\"Discourse Pattern: {parsed_result.get('discourse_pattern', 'N/A')}\")\n",
    "    print(f\"Cultural Sensitivity: {parsed_result.get('cultural_sensitivity_level', 'N/A')}\")\n",
    "    print(f\"Controversy Potential: {parsed_result.get('controversy_potential', 'N/A')}\")\n",
    "    \n",
    "    # Analyze quality\n",
    "    quality_analysis = classifier.analyze_classification_quality(parsed_result)\n",
    "    print(f\"\\n📈 Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Cultural Awareness: {quality_analysis['cultural_awareness']} connections\")\n",
    "    print(f\"Discussion Angles: {quality_analysis['discussion_depth']} angles\")\n",
    "    \n",
    "    # Show key discussion points\n",
    "    print(f\"\\n🎯 Key Discussion Angles:\")\n",
    "    for i, angle in enumerate(parsed_result.get('key_discussion_angles', []), 1):\n",
    "        print(f\"  {i}. {angle}\")\n",
    "    \n",
    "    print(f\"\\n🌍 Cultural Connections:\")\n",
    "    for i, connection in enumerate(parsed_result.get('cultural_connection_opportunities', []), 1):\n",
    "        print(f\"  {i}. {connection}\")\n",
    "    \n",
    "    return classification_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# classifier = TopicClassifier(deployment, \"Fanar-C-1-8.7B\")\n",
    "# classification_result, parsed_result = classifier.classify_with_validation(topic, information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bde0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Result:\n",
      "{\n",
      "  \"primary_category\": \"العلوم_والتكنولوجيا\",\n",
      "  \"category_justification\": \"يتناول الموضوع تحديات وتطبيقات الذكاء الاصطناعي مع التركيز على دورها في المجتمع العربي.\",\n",
      "  \"optimal_style\": \"تعليمي\",\n",
      "  \"discourse_pattern\": \"جدلي\",\n",
      "  \"audience_engagement_goal\": \"توعية المستمعين بأهمية واستخدامات الذكاء الاصطناعي بشكل مسؤول.\",\n",
      "  \"cultural_sensitivity_level\": \"متوسط\",\n",
      "  \"controversy_potential\": \"متوسطة\",\n",
      "  \"key_discussion_angles\": [\"تأثيرات الذكاء الاصطناعي على اللغة والثقافة العربية\", \"استراتيجيات تطوير نماذج ذكاء اصطناعي محلية\"],\n",
      "  \"natural_tension_points\": [\"مقارنة بين قوة البيانات الغربية ونقص الموارد العربية\", \"دور الحكومات والمطورين العرب في تعزيز الابتكار المحلي\"],\n",
      "  \"cultural_connection_opportunities\": [\"مناقشة الأمثلة التاريخية للتفوق الفكري العربي\", \"إبراز قصص النجاح الحديثة للمبتكرين العرب\"]\n",
      "}\n",
      "✅ Category: العلوم_والتكنولوجيا\n",
      "✅ Style: تعليمي\n"
     ]
    }
   ],
   "source": [
    "# Testing Instructions:\n",
    "\n",
    "# To test Step 1, add this to a new cell in your notebook:\n",
    "\n",
    "# Test Step 1: Topic Classification\n",
    "\n",
    "# Test with the singlehood topic\n",
    "topic = \"الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\"\n",
    "\n",
    "information = '''\n",
    "مع انتشار تقنيات الذكاء الاصطناعي بسرعة في العالم العربي، تزداد المخاوف حول تأثيرها على الهوية الثقافية واللغة العربية. \n",
    "تشير الدراسات إلى أن 78% من المحتوى الرقمي باللغة الإنجليزية، بينما المحتوى العربي لا يتجاوز 3%. \n",
    "معظم نماذج الذكاء الاصطناعي الحالية مدربة على بيانات غربية، مما يثير تساؤلات حول قدرتها على فهم السياق الثقافي العربي.\n",
    "في المقابل، تسعى دول مثل الإمارات والسعودية لتطوير نماذج ذكاء اصطناعي عربية مثل \"جايس\" و\"الحوراء\" لمواجهة هذا التحدي.\n",
    "التحدي الأكبر يكمن في كيفية الاستفادة من هذه التقنيات لتعزيز الثقافة العربية بدلاً من تهميشها، وضمان أن تخدم الذكاء الاصطناعي قيمنا ومبادئنا.\n",
    "'''\n",
    "\n",
    "classifier = TopicClassifier(deployment, model)\n",
    "classification_result = classifier.classify_topic(topic, information)\n",
    "print(\"Classification Result:\")\n",
    "print(classification_result)\n",
    "\n",
    "try:\n",
    "    parsed_result = json.loads(classification_result)\n",
    "    print(f\"✅ Category: {parsed_result['primary_category']}\")\n",
    "    print(f\"✅ Style: {parsed_result['optimal_style']}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"❌ JSON parsing failed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9ee26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class SimplePersonaGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def generate_personas(self, topic, information, classification_result):\n",
    "        \"\"\"\n",
    "        Generate simple but effective host and guest personas\n",
    "        \n",
    "        Args:\n",
    "            topic: Main topic of the podcast episode\n",
    "            information: Background information about the topic\n",
    "            classification_result: JSON string from Step 1 classification\n",
    "            \n",
    "        Returns:\n",
    "            JSON with simple host and guest personas\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse classification to understand the requirements\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid classification JSON provided\")\n",
    "        \n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        discourse_pattern = classification.get(\"discourse_pattern\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in designing Arabic podcast personas.\n",
    "\n",
    "Task: Create simple and suitable host and guest personas for this topic.\n",
    "\n",
    "Topic: {topic}\n",
    "Information: {information}\n",
    "Category: {primary_category}\n",
    "Required Style: {optimal_style}\n",
    "Discourse Pattern: {discourse_pattern}\n",
    "Cultural Sensitivity: {cultural_sensitivity}\n",
    "\n",
    "Return the result in this exact JSON format:\n",
    "\n",
    "{{\n",
    "    \"host\": {{\n",
    "        \"name\": \"Host's Arabic name\",\n",
    "        \"age\": numeric_age,\n",
    "        \"background\": \"Brief background in one sentence\",\n",
    "        \"personality\": \"Personality description in one sentence\",\n",
    "        \"speaking_style\": \"Speaking style in one sentence\"\n",
    "    }},\n",
    "    \"guest\": {{\n",
    "        \"name\": \"Guest's Arabic name\", \n",
    "        \"age\": numeric_age,\n",
    "        \"background\": \"Brief background in one sentence\",\n",
    "        \"expertise\": \"Area of expertise in one sentence\",\n",
    "        \"personality\": \"Personality description in one sentence\",\n",
    "        \"speaking_style\": \"Speaking style in one sentence\"\n",
    "    }},\n",
    "    \"why_good_match\": \"Why this host and guest are suitable for this topic - one sentence\"\n",
    "}}\n",
    "\n",
    "Requirements:\n",
    "- Use familiar Arabic names (like أحمد، محمد، فاطمة، نور، علي، لمى، سارة، خالد)\n",
    "- Simple and believable personas\n",
    "- Suitable for the topic and required style: {optimal_style}\n",
    "- Host should be curious and guest should be expert or have experience\n",
    "- All JSON values must be in Modern Standard Arabic (MSA)\n",
    "- JSON keys should be in English\n",
    "- Use ONLY English commas (,) - NEVER Arabic commas (،)\n",
    "- Use ONLY standard double quotes (\") - NEVER Arabic quotes\n",
    "- Age should be realistic numbers (25-55 range)\n",
    "- Do NOT include ```json markers\n",
    "- Do NOT include confidence scores or extra text\n",
    "- Return only valid JSON\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Host personality should match {optimal_style} style\n",
    "- Guest expertise should be relevant to: {topic}\n",
    "- Consider cultural sensitivity level: {cultural_sensitivity}\n",
    "- Make personas realistic and relatable to Arabic audiences\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are an expert in designing simple and effective podcast personas. Style: {optimal_style}. Always provide JSON values in Modern Standard Arabic while keeping keys in English. No extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Clean the JSON response to ensure it's parseable\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before the first { and after the last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('،', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove confidence scores and meta text\n",
    "        meta_patterns = [\n",
    "            r'الثقة:\\s*\\d+%',\n",
    "            r'الدقة:\\s*\\d+%',\n",
    "            r'معدل الثقة:\\s*\\d+%',\n",
    "            r'\\n.*الثقة.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*',\n",
    "            r'ملاحظة:.*',\n",
    "            r'تعليق:.*'\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def generate_personas_with_validation(self, topic, information, classification_result):\n",
    "        \"\"\"\n",
    "        Generate personas with automatic validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get personas\n",
    "                personas_result = self.generate_personas(topic, information, classification_result)\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(personas_result)\n",
    "                \n",
    "                # Validate required fields\n",
    "                validation_result = self._validate_personas(parsed_result)\n",
    "                \n",
    "                if validation_result[\"is_valid\"]:\n",
    "                    print(f\"✅ Persona generation successful on attempt {attempt + 1}\")\n",
    "                    return personas_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1}: Validation issues: {validation_result['issues']}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(personas_result[:500])\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"📝 Using fallback personas...\")\n",
    "        fallback_result = self._get_fallback_personas(topic, classification_result)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _validate_personas(self, parsed_result):\n",
    "        \"\"\"Validate the generated personas\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check main structure\n",
    "        if \"host\" not in parsed_result:\n",
    "            issues.append(\"Missing host section\")\n",
    "        if \"guest\" not in parsed_result:\n",
    "            issues.append(\"Missing guest section\")\n",
    "        if \"why_good_match\" not in parsed_result:\n",
    "            issues.append(\"Missing why_good_match section\")\n",
    "        \n",
    "        # Check host fields\n",
    "        host = parsed_result.get(\"host\", {})\n",
    "        required_host_fields = [\"name\", \"age\", \"background\", \"personality\", \"speaking_style\"]\n",
    "        for field in required_host_fields:\n",
    "            if field not in host or not host[field]:\n",
    "                issues.append(f\"Missing or empty host.{field}\")\n",
    "        \n",
    "        # Check guest fields\n",
    "        guest = parsed_result.get(\"guest\", {})\n",
    "        required_guest_fields = [\"name\", \"age\", \"background\", \"expertise\", \"personality\", \"speaking_style\"]\n",
    "        for field in required_guest_fields:\n",
    "            if field not in guest or not guest[field]:\n",
    "                issues.append(f\"Missing or empty guest.{field}\")\n",
    "        \n",
    "        # Check age validity\n",
    "        if \"age\" in host:\n",
    "            try:\n",
    "                age = int(host[\"age\"])\n",
    "                if age < 20 or age > 70:\n",
    "                    issues.append(f\"Host age {age} unrealistic (should be 20-70)\")\n",
    "            except (ValueError, TypeError):\n",
    "                issues.append(\"Host age should be a number\")\n",
    "        \n",
    "        if \"age\" in guest:\n",
    "            try:\n",
    "                age = int(guest[\"age\"])\n",
    "                if age < 20 or age > 70:\n",
    "                    issues.append(f\"Guest age {age} unrealistic (should be 20-70)\")\n",
    "            except (ValueError, TypeError):\n",
    "                issues.append(\"Guest age should be a number\")\n",
    "        \n",
    "        # Check for Arabic content\n",
    "        all_text = \" \".join([\n",
    "            str(host.get(\"name\", \"\")), str(host.get(\"background\", \"\")),\n",
    "            str(guest.get(\"name\", \"\")), str(guest.get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"why_good_match\", \"\"))\n",
    "        ])\n",
    "        \n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', all_text))\n",
    "        if arabic_chars < 10:\n",
    "            issues.append(\"Insufficient Arabic content\")\n",
    "        \n",
    "        return {\n",
    "            \"is_valid\": len(issues) == 0,\n",
    "            \"issues\": issues,\n",
    "            \"score\": max(0, 100 - len(issues) * 15)\n",
    "        }\n",
    "\n",
    "    def _get_fallback_personas(self, topic, classification_result):\n",
    "        \"\"\"Provide fallback personas if generation fails\"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            optimal_style = classification.get(\"optimal_style\", \"حواري\")\n",
    "        except:\n",
    "            optimal_style = \"حواري\"\n",
    "        \n",
    "        # Determine appropriate personas based on style\n",
    "        if optimal_style == \"تعليمي\":\n",
    "            host_personality = \"مقدم متحمس للتعلم ويطرح أسئلة واضحة\"\n",
    "            guest_personality = \"خبير صبور يشرح المعلومات بطريقة مبسطة\"\n",
    "        elif optimal_style == \"تحليلي\":\n",
    "            host_personality = \"مقدم مفكر يطرح أسئلة عميقة ومدروسة\"\n",
    "            guest_personality = \"محلل خبير يقدم رؤى متخصصة ومعمقة\"\n",
    "        elif optimal_style == \"ترفيهي\":\n",
    "            host_personality = \"مقدم مرح ومتفاعل يضيف روح الدعابة\"\n",
    "            guest_personality = \"ضيف ودود وطريف يشارك تجاربه بمرح\"\n",
    "        else:  # حواري\n",
    "            host_personality = \"مقدم ودود وفضولي يحب الحوار الطبيعي\"\n",
    "            guest_personality = \"ضيف متفتح ومتعاون يشارك خبراته بصراحة\"\n",
    "        \n",
    "        return {\n",
    "            \"host\": {\n",
    "                \"name\": \"أحمد السالم\",\n",
    "                \"age\": 35,\n",
    "                \"background\": \"مقدم برامج إذاعية مع خبرة في المواضيع المتنوعة\",\n",
    "                \"personality\": host_personality,\n",
    "                \"speaking_style\": \"يتحدث بوضوح ويطرح أسئلة مفتوحة لإثراء الحوار\"\n",
    "            },\n",
    "            \"guest\": {\n",
    "                \"name\": \"نور العلي\",\n",
    "                \"age\": 40,\n",
    "                \"background\": \"خبير ومختص في مجال الموضوع المطروح\",\n",
    "                \"expertise\": \"لديه معرفة عميقة وتجربة عملية في مجال النقاش\",\n",
    "                \"personality\": guest_personality,\n",
    "                \"speaking_style\": \"يعبر عن أفكاره بوضوح ويستخدم أمثلة من الواقع\"\n",
    "            },\n",
    "            \"why_good_match\": \"المقدم يجيد طرح الأسئلة والضيف لديه الخبرة للإجابة بفعالية\"\n",
    "        }\n",
    "\n",
    "    def analyze_persona_quality(self, parsed_result):\n",
    "        \"\"\"Analyze the quality of generated personas\"\"\"\n",
    "        analysis = {\n",
    "            \"name_authenticity\": self._assess_arabic_names(parsed_result),\n",
    "            \"age_realism\": self._assess_age_realism(parsed_result),\n",
    "            \"background_relevance\": self._assess_background_relevance(parsed_result),\n",
    "            \"personality_distinctiveness\": self._assess_personality_distinctiveness(parsed_result),\n",
    "            \"style_alignment\": self._assess_style_alignment(parsed_result),\n",
    "            \"cultural_appropriateness\": self._assess_cultural_appropriateness(parsed_result)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        score_factors = [\n",
    "            analysis[\"name_authenticity\"],\n",
    "            analysis[\"age_realism\"],\n",
    "            analysis[\"background_relevance\"] > 0,\n",
    "            analysis[\"personality_distinctiveness\"],\n",
    "            analysis[\"style_alignment\"] > 0,\n",
    "            analysis[\"cultural_appropriateness\"]\n",
    "        ]\n",
    "        \n",
    "        analysis[\"overall_score\"] = sum(score_factors) * 100 // len(score_factors)\n",
    "        analysis[\"quality_grade\"] = (\n",
    "            \"ممتاز\" if analysis[\"overall_score\"] >= 85 else\n",
    "            \"جيد\" if analysis[\"overall_score\"] >= 70 else\n",
    "            \"مقبول\" if analysis[\"overall_score\"] >= 55 else\n",
    "            \"يحتاج تحسين\"\n",
    "        )\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _assess_arabic_names(self, parsed_result):\n",
    "        \"\"\"Check if names are authentic Arabic names\"\"\"\n",
    "        host_name = parsed_result.get(\"host\", {}).get(\"name\", \"\")\n",
    "        guest_name = parsed_result.get(\"guest\", {}).get(\"name\", \"\")\n",
    "        \n",
    "        common_names = [\n",
    "            \"أحمد\", \"محمد\", \"علي\", \"خالد\", \"عمر\", \"يوسف\", \"حسن\", \"كريم\",\n",
    "            \"فاطمة\", \"عائشة\", \"نور\", \"لمى\", \"سارة\", \"مريم\", \"زينب\", \"رقية\"\n",
    "        ]\n",
    "        \n",
    "        host_authentic = any(name in host_name for name in common_names)\n",
    "        guest_authentic = any(name in guest_name for name in common_names)\n",
    "        \n",
    "        return host_authentic and guest_authentic\n",
    "\n",
    "    def _assess_age_realism(self, parsed_result):\n",
    "        \"\"\"Check if ages are realistic\"\"\"\n",
    "        try:\n",
    "            host_age = int(parsed_result.get(\"host\", {}).get(\"age\", 0))\n",
    "            guest_age = int(parsed_result.get(\"guest\", {}).get(\"age\", 0))\n",
    "            return 25 <= host_age <= 55 and 25 <= guest_age <= 65\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _assess_background_relevance(self, parsed_result):\n",
    "        \"\"\"Assess if backgrounds are relevant and detailed\"\"\"\n",
    "        host_bg = parsed_result.get(\"host\", {}).get(\"background\", \"\")\n",
    "        guest_bg = parsed_result.get(\"guest\", {}).get(\"background\", \"\")\n",
    "        guest_exp = parsed_result.get(\"guest\", {}).get(\"expertise\", \"\")\n",
    "        \n",
    "        relevance_score = 0\n",
    "        if len(host_bg) > 20:\n",
    "            relevance_score += 1\n",
    "        if len(guest_bg) > 20:\n",
    "            relevance_score += 1\n",
    "        if len(guest_exp) > 20:\n",
    "            relevance_score += 1\n",
    "        \n",
    "        return relevance_score\n",
    "\n",
    "    def _assess_personality_distinctiveness(self, parsed_result):\n",
    "        \"\"\"Check if host and guest have distinct personalities\"\"\"\n",
    "        host_personality = parsed_result.get(\"host\", {}).get(\"personality\", \"\")\n",
    "        guest_personality = parsed_result.get(\"guest\", {}).get(\"personality\", \"\")\n",
    "        \n",
    "        # Simple check: personalities should be different\n",
    "        similarity = len(set(host_personality.split()) & set(guest_personality.split()))\n",
    "        total_words = len(set(host_personality.split()) | set(guest_personality.split()))\n",
    "        \n",
    "        return similarity / total_words < 0.5 if total_words > 0 else False\n",
    "\n",
    "    def _assess_style_alignment(self, parsed_result):\n",
    "        \"\"\"Assess if personas align with the intended style\"\"\"\n",
    "        host_style = parsed_result.get(\"host\", {}).get(\"speaking_style\", \"\")\n",
    "        guest_style = parsed_result.get(\"guest\", {}).get(\"speaking_style\", \"\")\n",
    "        \n",
    "        style_indicators = {\n",
    "            \"حواري\": [\"ودود\", \"طبيعي\", \"تفاعل\", \"حوار\"],\n",
    "            \"تعليمي\": [\"تعليم\", \"شرح\", \"توضيح\", \"تبسيط\"],\n",
    "            \"تحليلي\": [\"تحليل\", \"عمق\", \"تخصص\", \"دقة\"],\n",
    "            \"ترفيهي\": [\"مرح\", \"دعابة\", \"ترفيه\", \"خفة\"]\n",
    "        }\n",
    "        \n",
    "        # This is a simplified assessment\n",
    "        return len(host_style) > 15 and len(guest_style) > 15\n",
    "\n",
    "    def _assess_cultural_appropriateness(self, parsed_result):\n",
    "        \"\"\"Check if personas are culturally appropriate\"\"\"\n",
    "        all_content = \" \".join([\n",
    "            str(parsed_result.get(\"host\", {}).get(\"name\", \"\")),\n",
    "            str(parsed_result.get(\"host\", {}).get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"guest\", {}).get(\"name\", \"\")),\n",
    "            str(parsed_result.get(\"guest\", {}).get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"why_good_match\", \"\"))\n",
    "        ])\n",
    "        \n",
    "        # Check for Arabic content\n",
    "        arabic_ratio = len(re.findall(r'[\\u0600-\\u06FF]', all_content)) / len(all_content) if all_content else 0\n",
    "        return arabic_ratio > 0.3\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_persona_generator(deployment, topic, information, classification_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the persona generator with comprehensive validation\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Persona Generator with Enhanced Validation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = SimplePersonaGenerator(deployment, model_name)\n",
    "    \n",
    "    # Run persona generation with validation\n",
    "    personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)\n",
    "    \n",
    "    print(\"👥 Generated Personas:\")\n",
    "    host = parsed_result.get(\"host\", {})\n",
    "    guest = parsed_result.get(\"guest\", {})\n",
    "    \n",
    "    print(f\"\\n🎤 Host: {host.get('name', 'N/A')} (عمر {host.get('age', 'N/A')})\")\n",
    "    print(f\"   الخلفية: {host.get('background', 'N/A')}\")\n",
    "    print(f\"   الشخصية: {host.get('personality', 'N/A')}\")\n",
    "    print(f\"   أسلوب الحديث: {host.get('speaking_style', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Guest: {guest.get('name', 'N/A')} (عمر {guest.get('age', 'N/A')})\")\n",
    "    print(f\"   الخلفية: {guest.get('background', 'N/A')}\")\n",
    "    print(f\"   الخبرة: {guest.get('expertise', 'N/A')}\")\n",
    "    print(f\"   الشخصية: {guest.get('personality', 'N/A')}\")\n",
    "    print(f\"   أسلوب الحديث: {guest.get('speaking_style', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n🤝 Why Good Match: {parsed_result.get('why_good_match', 'N/A')}\")\n",
    "    \n",
    "    # Analyze quality\n",
    "    quality_analysis = generator.analyze_persona_quality(parsed_result)\n",
    "    print(f\"\\n📈 Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Name Authenticity: {'✅' if quality_analysis['name_authenticity'] else '❌'}\")\n",
    "    print(f\"Age Realism: {'✅' if quality_analysis['age_realism'] else '❌'}\")\n",
    "    print(f\"Background Relevance: {quality_analysis['background_relevance']}/3\")\n",
    "    print(f\"Personality Distinctiveness: {'✅' if quality_analysis['personality_distinctiveness'] else '❌'}\")\n",
    "    \n",
    "    return personas_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# generator = SimplePersonaGenerator(deployment, \"Fanar-C-1-8.7B\")\n",
    "# personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f699140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Persona generation successful on attempt 1\n",
      "Personas Result:\n",
      "{\n",
      "  \"host\": {\n",
      "    \"name\": \"أحمد بن علي\",\n",
      "    \"age\": 40,\n",
      "    \"background\": \"مقدم برامج إذاعية معروف بشغفه بالتكنولوجيا والقضايا الاجتماعية\",\n",
      "    \"personality\": \"شخص مرح ومتحمس يسعى لفهم التحديات المعاصرة\",\n",
      "    \"speaking_style\": \"متفاعل مع الجمهور ويستخدم الأمثلة اليومية لشرح المفاهيم التقنية\"\n",
      "  },\n",
      "  \"guest\": {\n",
      "    \"name\": \"د. فاتن راشد\",\n",
      "    \"age\": 38,\n",
      "    \"background\": \"باحثة متخصصة في الذكاء الاصطناعي وتطبيقاته اللغوية والثقافية\",\n",
      "    \"expertise\": \"تبحث عن طرق لحماية اللغة العربية وسياقها الثقافي باستخدام الذكاء الاصطناعي\",\n",
      "    \"personality\": \"عالمة دقيقة ومنطقية لكن لها حس إنساني قوي\",\n",
      "    \"speaking_style\": \"توضيح علمي دقيق ممزوج بتوضيحات عملية وشرح للمخاطر والمنافع\"\n",
      "  },\n",
      "  \"why_good_match\": \"أحمد وفاطن مناسبان للنقاش لأن الأول لديه فضول اجتماعي وثقافي ويفهمهما الجمهور أما الثانية فهي خبيرة تكنولوجية ومعرفية قادرة على تقديم وجهة نظر علمية عميقة.\"\n",
      "}\n",
      "Host: أحمد بن علي (40 years)\n",
      "Guest: د. فاتن راشد (38 years)\n"
     ]
    }
   ],
   "source": [
    "generator = SimplePersonaGenerator(deployment, model)\n",
    "personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)\n",
    "print(\"Personas Result:\")\n",
    "print(personas_result)\n",
    "    # Results are guaranteed to be valid JSON with all required fields\n",
    "host = parsed_result['host']\n",
    "guest = parsed_result['guest']\n",
    "print(f\"Host: {host['name']} ({host['age']} years)\")\n",
    "print(f\"Guest: {guest['name']} ({guest['age']} years)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab52d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class FixedConversationStructureGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def generate_conversation_structure(self, topic, information, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Step 3: Generate core conversation structure with Arabic-only content\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse inputs\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided for classification or personas\")\n",
    "        \n",
    "        # Extract key info\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        discourse_pattern = classification.get(\"discourse_pattern\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "        host_background = host.get('background', '')\n",
    "        guest_background = guest.get('background', '')\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in designing conversation structures for Arabic podcasts.\n",
    "\n",
    "CRITICAL LANGUAGE REQUIREMENTS:\n",
    "- Use ONLY Arabic language (Modern Standard Arabic)\n",
    "- NO English words, phrases, or sentences\n",
    "- NO Chinese, Japanese, or any other foreign languages\n",
    "- NO foreign characters, symbols, or punctuation\n",
    "- Arabic text ONLY with standard JSON punctuation (, : \" {{ }})\n",
    "\n",
    "Task: Create a conversation structure for this Arabic podcast episode.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "\n",
    "Generate ONLY this JSON structure with Arabic content:\n",
    "\n",
    "{{\n",
    "    \"episode_topic\": \"Arabic episode topic here\",\n",
    "    \"personas\": {{\n",
    "        \"host\": {{\n",
    "            \"name\": \"{host_name}\",\n",
    "            \"background\": \"Arabic background description\",\n",
    "            \"speaking_style\": \"Arabic speaking style description\"\n",
    "        }},\n",
    "        \"guest\": {{\n",
    "            \"name\": \"{guest_name}\",\n",
    "            \"background\": \"Arabic background description\",\n",
    "            \"speaking_style\": \"Arabic speaking style description\"\n",
    "        }}\n",
    "    }},\n",
    "    \"conversation_flow\": {{\n",
    "        \"intro1\": {{\n",
    "            \"opening_line\": \"Arabic opening line for host\",\n",
    "            \"podcast_introduction\": \"Arabic podcast introduction\",\n",
    "            \"episode_hook\": \"Arabic engaging hook about topic\"\n",
    "        }},\n",
    "        \"intro2\": {{\n",
    "            \"topic_introduction\": \"Arabic topic introduction\",\n",
    "            \"guest_welcome\": \"Arabic welcome message for guest\",\n",
    "            \"guest_bio_highlight\": \"Arabic guest background highlight\"\n",
    "        }},\n",
    "        \"main_discussion\": [\n",
    "            {{\n",
    "                \"point_title\": \"Arabic first discussion point\",\n",
    "                \"personal_angle\": \"Arabic personal connection\"\n",
    "            }},\n",
    "            {{\n",
    "                \"point_title\": \"Arabic second discussion point\", \n",
    "                \"personal_angle\": \"Arabic personal angle\"\n",
    "            }},\n",
    "            {{\n",
    "                \"point_title\": \"Arabic third discussion point\",\n",
    "                \"personal_angle\": \"Arabic concluding angle\"\n",
    "            }}\n",
    "        ],\n",
    "        \"closing\": {{\n",
    "            \"conclusion\": {{\n",
    "                \"main_takeaways\": \"Arabic main takeaways\",\n",
    "                \"guest_final_message\": \"Arabic guest final message\",\n",
    "                \"host_closing_thoughts\": \"Arabic host closing thoughts\"\n",
    "            }},\n",
    "            \"outro\": {{\n",
    "                \"guest_appreciation\": \"Arabic thank guest message\",\n",
    "                \"audience_thanks\": \"Arabic thank audience message\",\n",
    "                \"call_to_action\": \"Arabic call for engagement\",\n",
    "                \"final_goodbye\": \"Arabic final goodbye\"\n",
    "            }}\n",
    "        }}\n",
    "    }},\n",
    "    \"cultural_context\": {{\n",
    "        \"proverbs_sayings\": [\n",
    "            \"Arabic proverb related to topic\",\n",
    "            \"Arabic wisdom saying\"\n",
    "        ],\n",
    "        \"regional_references\": [\n",
    "            \"Arabic local reference related to topic\",\n",
    "            \"Arabic regional experience\"\n",
    "        ]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENTS:\n",
    "- Use ONLY English commas (,) not Arabic commas (،)\n",
    "- Use ONLY standard double quotes (\") not Arabic quotes\n",
    "- Return ONLY the JSON structure above\n",
    "- NO explanatory text before or after JSON\n",
    "- NO confidence scores or meta-text\n",
    "- NO ```json markers or code blocks\n",
    "- All Arabic text must be grammatically correct MSA\n",
    "\n",
    "Replace all placeholder text with actual Arabic content specific to the topic: {topic}\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Generate Arabic podcast conversation structure. Return ONLY valid JSON with Arabic content. NO foreign languages. Use English JSON punctuation only.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3  # Lower temperature for more predictable output\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Enhanced Arabic-only JSON cleaning method\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents for JSON\n",
    "        clean_json = clean_json.replace('،', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove foreign language characters (Chinese, Japanese, etc.)\n",
    "        # Keep only Arabic Unicode ranges + basic JSON syntax\n",
    "        foreign_patterns = [\n",
    "            r'[\\u4e00-\\u9fff]',  # Chinese characters\n",
    "            r'[\\u3040-\\u309f]',  # Hiragana\n",
    "            r'[\\u30a0-\\u30ff]',  # Katakana\n",
    "            r'[\\u3000-\\u303f]',  # Japanese punctuation\n",
    "            r'[\\uff00-\\uffef]',  # Fullwidth characters\n",
    "            r'[\\u2000-\\u206f]',  # General punctuation (some problematic ones)\n",
    "        ]\n",
    "        \n",
    "        for pattern in foreign_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json)\n",
    "        \n",
    "        # Remove specific problematic characters we've seen\n",
    "        problematic_chars = ['千', '浮', '起', '提', '您', '足', '于', '、', '！', 'Indeed', 'how']\n",
    "        for char in problematic_chars:\n",
    "            clean_json = clean_json.replace(char, '')\n",
    "        \n",
    "        # Remove English words mixed in Arabic text (basic detection)\n",
    "        # This is a simple approach - remove common English words found in previous outputs\n",
    "        english_words = [\n",
    "            'Translation:', 'Hello', 'everyone', 'welcome', 'back', 'Indeed',\n",
    "            'how', 'preserving', 'culture', 'and', 'identity', 'in', 'digital', 'age',\n",
    "            'NLP', 'để', 'tweaking', 'idees', 'AbdulRahman', 'Fatima'\n",
    "        ]\n",
    "        \n",
    "        for word in english_words:\n",
    "            clean_json = clean_json.replace(word, '')\n",
    "        \n",
    "        # Remove meta-text patterns\n",
    "        meta_patterns = [\n",
    "            r'الثقة:\\s*\\d+%',\n",
    "            r'الدقة:\\s*\\d+%',\n",
    "            r'معدل الثقة:\\s*\\d+%',\n",
    "            r'\\n.*الثقة.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*',\n",
    "            r'ملاحظة:.*',\n",
    "            r'تعليق:.*',\n",
    "            r'\\(Translation:.*?\\)',\n",
    "            r'\\*\\*.*?\\*\\*',  # Remove markdown bold\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Fix missing commas between properties\n",
    "        clean_json = re.sub(r'\"\\s*\\n\\s*\"', '\",\\n\"', clean_json)\n",
    "        \n",
    "        # Remove multiple spaces\n",
    "        clean_json = re.sub(r'\\s+', ' ', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def _validate_arabic_only(self, text):\n",
    "        \"\"\"Validate that text contains only Arabic and basic punctuation\"\"\"\n",
    "        if not text:\n",
    "            return False, \"Empty text\"\n",
    "        \n",
    "        # Check for foreign language characters\n",
    "        foreign_patterns = [\n",
    "            (r'[\\u4e00-\\u9fff]', \"Chinese characters detected\"),\n",
    "            (r'[\\u3040-\\u309f\\u30a0-\\u30ff]', \"Japanese characters detected\"),\n",
    "            (r'\\b[a-zA-Z]{2,}\\b', \"English words detected\"),\n",
    "            (r'[千浮起提您足于、！]', \"Specific foreign characters detected\")\n",
    "        ]\n",
    "        \n",
    "        for pattern, message in foreign_patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return False, message\n",
    "        \n",
    "        # Check for minimum Arabic content\n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "        total_chars = len(re.sub(r'[\\s\\{\\}\",:\\[\\]]', '', text))  # Exclude JSON syntax\n",
    "        \n",
    "        if total_chars > 0:\n",
    "            arabic_ratio = arabic_chars / total_chars\n",
    "            if arabic_ratio < 0.8:  # At least 80% Arabic\n",
    "                return False, f\"Insufficient Arabic content: {arabic_ratio:.2%}\"\n",
    "        \n",
    "        return True, \"Arabic validation successful\"\n",
    "\n",
    "    def generate_conversation_structure_with_validation(self, topic, information, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Generate conversation structure with Arabic-only validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get structure\n",
    "                structure_result = self.generate_conversation_structure(topic, information, classification_result, personas_result)\n",
    "                \n",
    "                # Validate Arabic-only content\n",
    "                is_arabic_valid, arabic_message = self._validate_arabic_only(structure_result)\n",
    "                if not is_arabic_valid:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1}: Arabic validation failed: {arabic_message}\")\n",
    "                    continue\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(structure_result)\n",
    "                \n",
    "                # Validate structure completeness\n",
    "                is_structure_valid, structure_message = self.validate_conversation_structure(structure_result)\n",
    "                \n",
    "                if is_structure_valid:\n",
    "                    print(f\"✅ Conversation structure generation successful on attempt {attempt + 1}\")\n",
    "                    return structure_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1}: Structure validation failed: {structure_message}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(structure_result[:300] + \"...\" if len(structure_result) > 300 else structure_result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"📝 Using fallback conversation structure...\")\n",
    "        fallback_result = self._get_fallback_structure(topic, classification_result, personas_result)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _get_fallback_structure(self, topic, classification_result, personas_result):\n",
    "        \"\"\"Provide Arabic-only fallback conversation structure\"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "            optimal_style = classification.get(\"optimal_style\", \"حواري\")\n",
    "        except:\n",
    "            optimal_style = \"حواري\"\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "        \n",
    "        return {\n",
    "            \"episode_topic\": f\"نقاش حول {topic}\",\n",
    "            \"personas\": {\n",
    "                \"host\": {\n",
    "                    \"name\": host_name,\n",
    "                    \"background\": host.get('background', 'مقدم برامج إذاعية متخصص'),\n",
    "                    \"speaking_style\": host.get('speaking_style', 'يتحدث بوضوح ويطرح أسئلة مدروسة')\n",
    "                },\n",
    "                \"guest\": {\n",
    "                    \"name\": guest_name,\n",
    "                    \"background\": guest.get('background', 'خبير متخصص في الموضوع'),\n",
    "                    \"speaking_style\": guest.get('speaking_style', 'يشرح بوضوح ويقدم أمثلة عملية')\n",
    "                }\n",
    "            },\n",
    "            \"conversation_flow\": {\n",
    "                \"intro1\": {\n",
    "                    \"opening_line\": f\"مرحباً بكم مستمعينا الكرام، معكم {host_name} في حلقة جديدة\",\n",
    "                    \"podcast_introduction\": \"نناقش اليوم موضوعاً مهماً يهم الجميع ويستحق التأمل\",\n",
    "                    \"episode_hook\": f\"موضوع حلقتنا اليوم هو {topic} وأثره على حياتنا\"\n",
    "                },\n",
    "                \"intro2\": {\n",
    "                    \"topic_introduction\": f\"سنتحدث اليوم عن {topic} وجوانبه المختلفة والمهمة\",\n",
    "                    \"guest_welcome\": f\"معي اليوم الضيف المتميز {guest_name}، أهلاً وسهلاً بك\",\n",
    "                    \"guest_bio_highlight\": f\"{guest_name} خبير متخصص في هذا المجال ولديه خبرة واسعة\"\n",
    "                },\n",
    "                \"main_discussion\": [\n",
    "                    {\n",
    "                        \"point_title\": \"الجانب الأول والأساسي للموضوع\",\n",
    "                        \"personal_angle\": \"كيف يؤثر هذا الموضوع على حياتنا اليومية وتجاربنا\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"point_title\": \"الجانب الثاني والتحديات المرتبطة\",\n",
    "                        \"personal_angle\": \"التحديات والفرص المتاحة في هذا المجال\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"point_title\": \"الجانب الثالث والحلول المقترحة\",\n",
    "                        \"personal_angle\": \"النصائح والتوجيهات العملية للمستقبل\"\n",
    "                    }\n",
    "                ],\n",
    "                \"closing\": {\n",
    "                    \"conclusion\": {\n",
    "                        \"main_takeaways\": \"الخلاصات المهمة والنقاط الأساسية من نقاشنا اليوم\",\n",
    "                        \"guest_final_message\": \"رسالة أخيرة ومهمة من الضيف لجمهور المستمعين\",\n",
    "                        \"host_closing_thoughts\": \"أفكار ختامية وتأملات من المقدم حول الموضوع\"\n",
    "                    },\n",
    "                    \"outro\": {\n",
    "                        \"guest_appreciation\": f\"شكراً جزيلاً {guest_name} على هذا النقاش المفيد والثري\",\n",
    "                        \"audience_thanks\": \"شكراً لكم مستمعينا الكرام على متابعتكم واهتمامكم\",\n",
    "                        \"call_to_action\": \"تفاعلوا معنا وشاركونا آراءكم عبر وسائل التواصل الاجتماعي\",\n",
    "                        \"final_goodbye\": \"إلى اللقاء في حلقة قادمة بإذن الله\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"cultural_context\": {\n",
    "                \"proverbs_sayings\": [\n",
    "                    \"العلم نور والجهل ظلام\",\n",
    "                    \"في التأني السلامة وفي العجلة الندامة\"\n",
    "                ],\n",
    "                \"regional_references\": [\n",
    "                    \"التجربة العربية الغنية في هذا المجال\",\n",
    "                    \"الخبرات المحلية والإقليمية ذات الصلة بالموضوع\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def validate_conversation_structure(self, structure_json):\n",
    "        \"\"\"Enhanced validation for conversation structure\"\"\"\n",
    "        required_keys = [\"episode_topic\", \"personas\", \"conversation_flow\", \"cultural_context\"]\n",
    "        \n",
    "        conversation_flow_required = [\"intro1\", \"intro2\", \"main_discussion\", \"closing\"]\n",
    "        intro1_required = [\"opening_line\", \"podcast_introduction\", \"episode_hook\"]\n",
    "        intro2_required = [\"topic_introduction\", \"guest_welcome\", \"guest_bio_highlight\"]\n",
    "        \n",
    "        try:\n",
    "            structure = json.loads(structure_json)\n",
    "            missing_keys = []\n",
    "            \n",
    "            # Check main structure\n",
    "            for key in required_keys:\n",
    "                if key not in structure:\n",
    "                    missing_keys.append(key)\n",
    "            \n",
    "            # Check conversation flow\n",
    "            if \"conversation_flow\" in structure:\n",
    "                conv_flow = structure[\"conversation_flow\"]\n",
    "                for key in conversation_flow_required:\n",
    "                    if key not in conv_flow:\n",
    "                        missing_keys.append(f\"conversation_flow.{key}\")\n",
    "                \n",
    "                # Check intro1\n",
    "                if \"intro1\" in conv_flow:\n",
    "                    intro1 = conv_flow[\"intro1\"]\n",
    "                    for key in intro1_required:\n",
    "                        if key not in intro1:\n",
    "                            missing_keys.append(f\"intro1.{key}\")\n",
    "                \n",
    "                # Check intro2\n",
    "                if \"intro2\" in conv_flow:\n",
    "                    intro2 = conv_flow[\"intro2\"]\n",
    "                    for key in intro2_required:\n",
    "                        if key not in intro2:\n",
    "                            missing_keys.append(f\"intro2.{key}\")\n",
    "                \n",
    "                # Check main discussion\n",
    "                if \"main_discussion\" in conv_flow:\n",
    "                    main_disc = conv_flow[\"main_discussion\"]\n",
    "                    if not isinstance(main_disc, list) or len(main_disc) < 3:\n",
    "                        missing_keys.append(\"main_discussion (need at least 3 points)\")\n",
    "                    else:\n",
    "                        for i, point in enumerate(main_disc):\n",
    "                            if \"point_title\" not in point:\n",
    "                                missing_keys.append(f\"main_discussion[{i}].point_title\")\n",
    "                            if \"personal_angle\" not in point:\n",
    "                                missing_keys.append(f\"main_discussion[{i}].personal_angle\")\n",
    "            \n",
    "            if missing_keys:\n",
    "                return False, f\"Missing required keys: {missing_keys}\"\n",
    "            \n",
    "            return True, \"Conversation structure validation successful\"\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            return False, f\"Invalid JSON format: {e}\"\n",
    "\n",
    "    def analyze_structure_quality(self, structure_json):\n",
    "        \"\"\"Enhanced quality analysis with Arabic-only validation\"\"\"\n",
    "        try:\n",
    "            structure = json.loads(structure_json)\n",
    "            \n",
    "            analysis = {\n",
    "                \"structure_completeness\": 0,\n",
    "                \"content_quality\": 0,\n",
    "                \"cultural_integration\": 0,\n",
    "                \"arabic_purity\": 0\n",
    "            }\n",
    "            \n",
    "            # Check completeness\n",
    "            conv_flow = structure.get(\"conversation_flow\", {})\n",
    "            completeness_indicators = [\n",
    "                bool(conv_flow.get(\"intro1\")),\n",
    "                bool(conv_flow.get(\"intro2\")),\n",
    "                bool(conv_flow.get(\"main_discussion\")),\n",
    "                bool(conv_flow.get(\"closing\")),\n",
    "                len(conv_flow.get(\"main_discussion\", [])) >= 3\n",
    "            ]\n",
    "            analysis[\"structure_completeness\"] = sum(completeness_indicators) * 20\n",
    "            \n",
    "            # Check content quality\n",
    "            intro1 = conv_flow.get(\"intro1\", {})\n",
    "            intro2 = conv_flow.get(\"intro2\", {})\n",
    "            quality_indicators = [\n",
    "                len(intro1.get(\"opening_line\", \"\")) > 15,\n",
    "                len(intro1.get(\"episode_hook\", \"\")) > 15,\n",
    "                len(intro2.get(\"topic_introduction\", \"\")) > 15,\n",
    "                len(intro2.get(\"guest_welcome\", \"\")) > 10\n",
    "            ]\n",
    "            analysis[\"content_quality\"] = sum(quality_indicators) * 25\n",
    "            \n",
    "            # Check cultural integration\n",
    "            cultural = structure.get(\"cultural_context\", {})\n",
    "            cultural_indicators = [\n",
    "                len(cultural.get(\"proverbs_sayings\", [])) >= 1,\n",
    "                len(cultural.get(\"regional_references\", [])) >= 1\n",
    "            ]\n",
    "            analysis[\"cultural_integration\"] = sum(cultural_indicators) * 50\n",
    "            \n",
    "            # Check Arabic purity\n",
    "            full_text = json.dumps(structure, ensure_ascii=False)\n",
    "            is_arabic_pure, _ = self._validate_arabic_only(full_text)\n",
    "            analysis[\"arabic_purity\"] = 100 if is_arabic_pure else 0\n",
    "            \n",
    "            # Calculate overall score\n",
    "            analysis[\"overall_score\"] = min(100, sum([\n",
    "                analysis[\"structure_completeness\"],\n",
    "                analysis[\"content_quality\"],\n",
    "                analysis[\"cultural_integration\"],\n",
    "                analysis[\"arabic_purity\"]\n",
    "            ]) // 4)\n",
    "            \n",
    "            analysis[\"quality_grade\"] = (\n",
    "                \"ممتاز\" if analysis[\"overall_score\"] >= 90 else\n",
    "                \"جيد جداً\" if analysis[\"overall_score\"] >= 80 else\n",
    "                \"جيد\" if analysis[\"overall_score\"] >= 70 else\n",
    "                \"مقبول\" if analysis[\"overall_score\"] >= 60 else\n",
    "                \"يحتاج تحسين\"\n",
    "            )\n",
    "            \n",
    "            analysis[\"ready_for_next_step\"] = analysis[\"overall_score\"] >= 75\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except:\n",
    "            return {\"error\": \"Could not analyze structure quality\"}\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_fixed_conversation_structure_generator(deployment, topic, information, classification_result, personas_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the fixed conversation structure generator with Arabic-only validation\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Fixed Conversation Structure Generator...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = FixedConversationStructureGenerator(deployment, model_name)\n",
    "    \n",
    "    # Run generation with validation\n",
    "    structure_result, parsed_result = generator.generate_conversation_structure_with_validation(\n",
    "        topic, information, classification_result, personas_result\n",
    "    )\n",
    "    \n",
    "    print(\"📋 Generated Structure:\")\n",
    "    print(f\"Episode Topic: {parsed_result.get('episode_topic', 'N/A')}\")\n",
    "    \n",
    "    # Show conversation flow\n",
    "    conv_flow = parsed_result.get(\"conversation_flow\", {})\n",
    "    intro1 = conv_flow.get(\"intro1\", {})\n",
    "    main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "    \n",
    "    print(f\"\\n🎬 Intro1 Opening: {intro1.get('opening_line', 'N/A')[:80]}...\")\n",
    "    print(f\"📝 Discussion Points: {len(main_discussion)}\")\n",
    "    for i, point in enumerate(main_discussion, 1):\n",
    "        print(f\"  {i}. {point.get('point_title', 'N/A')[:60]}...\")\n",
    "    \n",
    "    # Show cultural context\n",
    "    cultural = parsed_result.get(\"cultural_context\", {})\n",
    "    proverbs = cultural.get(\"proverbs_sayings\", [])\n",
    "    print(f\"\\n🏛️ Cultural Proverbs: {len(proverbs)}\")\n",
    "    for proverb in proverbs:\n",
    "        print(f\"  • {proverb}\")\n",
    "    \n",
    "    # Enhanced quality analysis\n",
    "    quality_analysis = generator.analyze_structure_quality(structure_result)\n",
    "    print(f\"\\n📈 Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Structure Completeness: {quality_analysis['structure_completeness']}/100\")\n",
    "    print(f\"Content Quality: {quality_analysis['content_quality']}/100\")\n",
    "    print(f\"Cultural Integration: {quality_analysis['cultural_integration']}/100\")\n",
    "    print(f\"Arabic Purity: {quality_analysis['arabic_purity']}/100\")\n",
    "    print(f\"Ready for Next Step: {'✅' if quality_analysis['ready_for_next_step'] else '❌'}\")\n",
    "    \n",
    "    # Arabic validation check\n",
    "    is_arabic_valid, arabic_message = generator._validate_arabic_only(structure_result)\n",
    "    print(f\"\\n🔍 Arabic Validation: {'✅' if is_arabic_valid else '❌'} {arabic_message}\")\n",
    "    \n",
    "    return structure_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# generator = FixedConversationStructureGenerator(deployment, \"Fanar-C-1-8.7B\")\n",
    "# structure_result, parsed_result = generator.generate_conversation_structure_with_validation(topic, information, classification_result, personas_result)\n",
    "\n",
    "# Test the fixed generator\n",
    "# test_result = test_fixed_conversation_structure_generator(deployment, topic, information, classification_result, personas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09091744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Attempt 1: Arabic validation failed: English words detected\n",
      "⚠️ Attempt 2: Arabic validation failed: English words detected\n",
      "⚠️ Attempt 3: Arabic validation failed: English words detected\n",
      "📝 Using fallback conversation structure...\n",
      "Conversation Structure Result:\n",
      "{\n",
      "  \"episode_topic\": \"نقاش حول الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"أحمد بن علي\",\n",
      "      \"background\": \"مقدم برامج إذاعية معروف بشغفه بالتكنولوجيا والقضايا الاجتماعية\",\n",
      "      \"speaking_style\": \"متفاعل مع الجمهور ويستخدم الأمثلة اليومية لشرح المفاهيم التقنية\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"د. فاتن راشد\",\n",
      "      \"background\": \"باحثة متخصصة في الذكاء الاصطناعي وتطبيقاته اللغوية والثقافية\",\n",
      "      \"speaking_style\": \"توضيح علمي دقيق ممزوج بتوضيحات عملية وشرح للمخاطر والمنافع\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"مرحباً بكم مستمعينا الكرام، معكم أحمد بن علي في حلقة جديدة\",\n",
      "      \"podcast_introduction\": \"نناقش اليوم موضوعاً مهماً يهم الجميع ويستحق التأمل\",\n",
      "      \"episode_hook\": \"موضوع حلقتنا اليوم هو الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي وأثره على حياتنا\"\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"سنتحدث اليوم عن الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي وجوانبه المختلفة والمهمة\",\n",
      "      \"guest_welcome\": \"معي اليوم الضيف المتميز د. فاتن راشد، أهلاً وسهلاً بك\",\n",
      "      \"guest_bio_highlight\": \"د. فاتن راشد خبير متخصص في هذا المجال ولديه خبرة واسعة\"\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"الجانب الأول والأساسي للموضوع\",\n",
      "        \"personal_angle\": \"كيف يؤثر هذا الموضوع على حياتنا اليومية وتجاربنا\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثاني والتحديات المرتبطة\",\n",
      "        \"personal_angle\": \"التحديات والفرص المتاحة في هذا المجال\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثالث والحلول المقترحة\",\n",
      "        \"personal_angle\": \"النصائح والتوجيهات العملية للمستقبل\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"الخلاصات المهمة والنقاط الأساسية من نقاشنا اليوم\",\n",
      "        \"guest_final_message\": \"رسالة أخيرة ومهمة من الضيف لجمهور المستمعين\",\n",
      "        \"host_closing_thoughts\": \"أفكار ختامية وتأملات من المقدم حول الموضوع\"\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"شكراً جزيلاً د. فاتن راشد على هذا النقاش المفيد والثري\",\n",
      "        \"audience_thanks\": \"شكراً لكم مستمعينا الكرام على متابعتكم واهتمامكم\",\n",
      "        \"call_to_action\": \"تفاعلوا معنا وشاركونا آراءكم عبر وسائل التواصل الاجتماعي\",\n",
      "        \"final_goodbye\": \"إلى اللقاء في حلقة قادمة بإذن الله\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"العلم نور والجهل ظلام\",\n",
      "      \"في التأني السلامة وفي العجلة الندامة\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"التجربة العربية الغنية في هذا المجال\",\n",
      "      \"الخبرات المحلية والإقليمية ذات الصلة بالموضوع\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "🧪 Testing Fixed Conversation Structure Generator...\n",
      "============================================================\n",
      "⚠️ Attempt 1: Arabic validation failed: English words detected\n",
      "⚠️ Attempt 2: Arabic validation failed: English words detected\n",
      "⚠️ Attempt 3: JSON parsing error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw response for debugging:\n",
      "{\n",
      "📝 Using fallback conversation structure...\n",
      "📋 Generated Structure:\n",
      "Episode Topic: نقاش حول الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\n",
      "\n",
      "🎬 Intro1 Opening: مرحباً بكم مستمعينا الكرام، معكم أحمد بن علي في حلقة جديدة...\n",
      "📝 Discussion Points: 3\n",
      "  1. الجانب الأول والأساسي للموضوع...\n",
      "  2. الجانب الثاني والتحديات المرتبطة...\n",
      "  3. الجانب الثالث والحلول المقترحة...\n",
      "\n",
      "🏛️ Cultural Proverbs: 2\n",
      "  • العلم نور والجهل ظلام\n",
      "  • في التأني السلامة وفي العجلة الندامة\n",
      "\n",
      "📈 Quality Analysis:\n",
      "Overall Score: 75/100\n",
      "Quality Grade: جيد\n",
      "Structure Completeness: 100/100\n",
      "Content Quality: 100/100\n",
      "Cultural Integration: 100/100\n",
      "Arabic Purity: 0/100\n",
      "Ready for Next Step: ✅\n",
      "\n",
      "🔍 Arabic Validation: ❌ English words detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize with Arabic-only focus\n",
    "generator = FixedConversationStructureGenerator(deployment, model)\n",
    "\n",
    "# Generate with validation\n",
    "structure_result, parsed_result = generator.generate_conversation_structure_with_validation(\n",
    "    topic, information, classification_result, personas_result\n",
    ")\n",
    "print(\"Conversation Structure Result:\")\n",
    "print(structure_result)\n",
    "# Test with comprehensive checks\n",
    "test_result = test_fixed_conversation_structure_generator(\n",
    "    deployment, topic, information, classification_result, personas_result\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c22de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cece3b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Minimal enhancement (fastest, most concise)\\nenhancer_minimal = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"minimal\")\\nresult_minimal = enhancer_minimal.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Standard enhancement (balanced)\\nenhancer_standard = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"standard\")\\nresult_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Full enhancement (comprehensive but largest)\\nenhancer_full = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"full\")\\nresult_full = enhancer_full.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Test with specific level\\nenhanced_result = test_enhanced_dialogue_content_enhancer(\\n    deployment, topic, information, classification_result, personas_result, structure_result, \\n    model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"\\n)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "class SectionalDialogueContentEnhancer:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\", enhancement_level=\"minimal\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "        self.enhancement_level = enhancement_level  # \"minimal\", \"standard\", \"full\"\n",
    "\n",
    "    def enhance_intro_sections(self, topic, classification_result, personas_result, intro1, intro2):\n",
    "        \"\"\"\n",
    "        Chunk 1: Enhance intro1 and intro2 sections (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "        \n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"2\"\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"3\"\n",
    "        else:  # full\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"3-4\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast introductions.\n",
    "\n",
    "Task: Enhance ONLY the intro sections with natural dialogue elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current intro1: {json.dumps(intro1, ensure_ascii=False)}\n",
    "Current intro2: {json.dumps(intro2, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "\n",
    "For intro1, ADD this field:\n",
    "- \"{intro1_fields}\": [{item_count} natural spontaneous phrases that the host might use when opening, in MSA]\n",
    "\n",
    "For intro2, ADD this field:  \n",
    "- \"{intro2_fields}\": [{item_count} ways to connect this topic to Arab culture/values, in MSA]\n",
    "\n",
    "Return the enhanced sections in this exact format:\n",
    "{{\n",
    "    \"intro1\": {{\n",
    "        [keep all existing intro1 fields],\n",
    "        \"{intro1_fields}\": [new content]\n",
    "    }},\n",
    "    \"intro2\": {{\n",
    "        [keep all existing intro2 fields],\n",
    "        \"{intro2_fields}\": [new content]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing content unchanged\n",
    "- Add only the specified new fields\n",
    "- All new values in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content specific to topic: {topic}\n",
    "- Match the {optimal_style} style\n",
    "- Keep arrays short and impactful\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast intros. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_main_discussion_point(self, topic, classification_result, personas_result, discussion_point, point_index):\n",
    "        \"\"\"\n",
    "        Chunk 2: Enhance individual main discussion points (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"]\"\"\"\n",
    "            field_count = \"2\"\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"],\n",
    "    \"natural_transitions\": \"transition phrase in MSA\\\"\"\"\"\n",
    "            field_count = \"3\"\n",
    "        else:  # full\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"disagreement_points\": \"disagreement description in MSA\",\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"],\n",
    "    \"natural_transitions\": \"transition phrase in MSA\",\n",
    "    \"emotional_triggers\": \"emotional description in MSA\\\"\"\"\"\n",
    "            field_count = \"5\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast discussion points.\n",
    "\n",
    "Task: Enhance ONE discussion point with rich dialogue elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Point #{point_index + 1}\n",
    "\n",
    "Current discussion point: {json.dumps(discussion_point, ensure_ascii=False)}\n",
    "\n",
    "Add EXACTLY these {field_count} fields. Keep all existing fields unchanged:\n",
    "\n",
    "{{\n",
    "    [all existing fields from discussion_point],{enhancement_fields}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing fields exactly as they are\n",
    "- Add only the {field_count} new fields shown above\n",
    "- All new content in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation ONLY (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content relevant to topic: {topic}\n",
    "- Keep responses concise and actionable\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast discussion points. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_closing_sections(self, topic, classification_result, personas_result, closing_section):\n",
    "        \"\"\"\n",
    "        Chunk 3: Enhance closing (conclusion + outro) sections (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            conclusion_fields = '\"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\"'\n",
    "            outro_fields = '\"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\"'\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            conclusion_fields = '''\n",
    "        \"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\",\n",
    "        \"key_insights\": [\"insight 1 in MSA\", \"insight 2 in MSA\"]'''\n",
    "            outro_fields = '\"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\"'\n",
    "        else:  # full\n",
    "            conclusion_fields = '''\n",
    "        \"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\",\n",
    "        \"key_insights\": [2-3 key insights that should be highlighted in the wrap-up, in MSA]'''\n",
    "            outro_fields = '''\n",
    "        \"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\",\n",
    "        \"connection_building\": \"ways to build ongoing connection with the audience, in MSA\"'''\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast closings.\n",
    "\n",
    "Task: Enhance the closing section with natural wrap-up elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current closing: {json.dumps(closing_section, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "\n",
    "For conclusion subsection, ADD:\n",
    "{conclusion_fields}\n",
    "\n",
    "For outro subsection, ADD:\n",
    "{outro_fields}\n",
    "\n",
    "Return enhanced closing in this exact format:\n",
    "{{\n",
    "    \"conclusion\": {{\n",
    "        [keep all existing conclusion fields],\n",
    "        {conclusion_fields}\n",
    "    }},\n",
    "    \"outro\": {{\n",
    "        [keep all existing outro fields],\n",
    "        {outro_fields}\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing content unchanged\n",
    "- Add only the specified new fields\n",
    "- All new values in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content feel conclusive and satisfying\n",
    "- Keep insights concise and actionable\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast closings. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def create_global_elements(self, topic, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Chunk 4: Create global elements (SIMPLIFIED AND REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\"\n",
    "        ],\n",
    "        \"personal_stories\": [\n",
    "            \"first personal story in MSA\",\n",
    "            \"second personal story in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\"\n",
    "        ],\n",
    "        \"storytelling_moments\": [\n",
    "            \"first storytelling moment in MSA\",\n",
    "            \"second storytelling moment in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "        else:  # full\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\",\n",
    "            \"third natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\", \n",
    "            \"third emotional reaction in MSA\"\n",
    "        ],\n",
    "        \"personal_stories\": [\n",
    "            \"first personal story in MSA\",\n",
    "            \"second personal story in MSA\"\n",
    "        ],\n",
    "        \"humorous_moments\": [\n",
    "            \"first humorous moment in MSA\",\n",
    "            \"second humorous moment in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"personality_interactions\": {\n",
    "        \"host_strengths\": \"host strengths description in MSA\",\n",
    "        \"guest_expertise\": \"guest expertise description in MSA\",\n",
    "        \"natural_chemistry\": \"chemistry description in MSA\",\n",
    "        \"tension_points\": \"tension points description in MSA\",\n",
    "        \"collaboration_moments\": \"collaboration description in MSA\"\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\",\n",
    "            \"third questioning style in MSA\"\n",
    "        ],\n",
    "        \"storytelling_moments\": [\n",
    "            \"first storytelling moment in MSA\",\n",
    "            \"second storytelling moment in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\",\n",
    "            \"third engagement technique in MSA\"\n",
    "        ],\n",
    "        \"emotional_peaks\": [\n",
    "            \"first emotional peak in MSA\",\n",
    "            \"second emotional peak in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in creating global dialogue elements for Arabic podcasts.\n",
    "\n",
    "Task: Create global sections that enhance the overall conversation flow.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "Enhancement Level: {self.enhancement_level}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Create EXACTLY this JSON structure with proper English punctuation:\n",
    "\n",
    "{structure}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Replace placeholder text with actual content in Modern Standard Arabic (MSA)\n",
    "- Use ONLY English commas (,) and standard quotes (\")\n",
    "- NO Arabic commas (،) or special punctuation\n",
    "- NO extra text before or after JSON\n",
    "- NO explanatory text\n",
    "- Make content specific to {host_name}, {guest_name}, and topic: {topic}\n",
    "- Follow the EXACT structure shown above\n",
    "- Keep content concise and actionable\n",
    "- Ensure all arrays have exactly the specified number of items\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You create global dialogue elements. Return ONLY valid JSON with English punctuation. No Arabic commas. No extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_dialogue_content(self, topic, information, classification_result, personas_result, structure_result):\n",
    "        \"\"\"\n",
    "        Main orchestration method: Coordinates all chunks with configurable enhancement levels\n",
    "        \"\"\"\n",
    "        print(f\"🔧 Starting sectional dialogue enhancement (Level: {self.enhancement_level})...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            structure = json.loads(structure_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid structure JSON provided\")\n",
    "        \n",
    "        # Extract sections\n",
    "        conv_flow = structure.get(\"conversation_flow\", {})\n",
    "        intro1 = conv_flow.get(\"intro1\", {})\n",
    "        intro2 = conv_flow.get(\"intro2\", {})\n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        closing = conv_flow.get(\"closing\", {})\n",
    "        \n",
    "        # Chunk 1: Enhance intro sections\n",
    "        print(\"📝 Chunk 1: Enhancing intro sections...\")\n",
    "        try:\n",
    "            enhanced_intros_json = self.enhance_intro_sections(\n",
    "                topic, classification_result, personas_result, intro1, intro2\n",
    "            )\n",
    "            enhanced_intros = json.loads(enhanced_intros_json)\n",
    "            \n",
    "            # Update structure\n",
    "            structure[\"conversation_flow\"][\"intro1\"].update(enhanced_intros.get(\"intro1\", {}))\n",
    "            structure[\"conversation_flow\"][\"intro2\"].update(enhanced_intros.get(\"intro2\", {}))\n",
    "            print(\"✅ Intro sections enhanced successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing intros: {e}\")\n",
    "        \n",
    "        # Small delay between chunks\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 2: Enhance main discussion points (one by one)\n",
    "        print(\"📝 Chunk 2: Enhancing main discussion points...\")\n",
    "        enhanced_discussion_points = []\n",
    "        \n",
    "        for i, point in enumerate(main_discussion):\n",
    "            print(f\"  Enhancing discussion point {i+1}/{len(main_discussion)}...\")\n",
    "            try:\n",
    "                enhanced_point_json = self.enhance_main_discussion_point(\n",
    "                    topic, classification_result, personas_result, point, i\n",
    "                )\n",
    "                enhanced_point = json.loads(enhanced_point_json)\n",
    "                enhanced_discussion_points.append(enhanced_point)\n",
    "                print(f\"  ✅ Point {i+1} enhanced successfully\")\n",
    "                \n",
    "                # Small delay between points\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Error enhancing point {i+1}: {e}\")\n",
    "                print(f\"  🔄 Using fallback enhancement for point {i+1}...\")\n",
    "                \n",
    "                # Try fallback enhancement for this point\n",
    "                enhanced_point = self._create_fallback_discussion_point(\n",
    "                    topic, classification_result, personas_result, point, i\n",
    "                )\n",
    "                enhanced_discussion_points.append(enhanced_point)\n",
    "                print(f\"  ✅ Point {i+1} enhanced with fallback method\")\n",
    "        \n",
    "        # Update structure with enhanced discussion points\n",
    "        structure[\"conversation_flow\"][\"main_discussion\"] = enhanced_discussion_points\n",
    "        print(\"✅ All main discussion points processed\")\n",
    "        \n",
    "        # Small delay between chunks\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 3: Enhance closing sections\n",
    "        print(\"📝 Chunk 3: Enhancing closing sections...\")\n",
    "        try:\n",
    "            enhanced_closing_json = self.enhance_closing_sections(\n",
    "                topic, classification_result, personas_result, closing\n",
    "            )\n",
    "            enhanced_closing = json.loads(enhanced_closing_json)\n",
    "            \n",
    "            # Update structure\n",
    "            structure[\"conversation_flow\"][\"closing\"].update(enhanced_closing)\n",
    "            print(\"✅ Closing sections enhanced successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing closing: {e}\")\n",
    "        \n",
    "        # Small delay between chunks  \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 4: Create global elements\n",
    "        print(\"📝 Chunk 4: Creating global elements...\")\n",
    "        try:\n",
    "            global_elements_json = self.create_global_elements(\n",
    "                topic, classification_result, personas_result\n",
    "            )\n",
    "            global_elements = json.loads(global_elements_json)\n",
    "            \n",
    "            # Add global elements to structure\n",
    "            structure.update(global_elements)\n",
    "            print(\"✅ Global elements created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error creating global elements: {e}\")\n",
    "            print(\"🔄 Attempting to create fallback global elements...\")\n",
    "            \n",
    "            # Create fallback global elements\n",
    "            try:\n",
    "                fallback_elements = self._create_fallback_global_elements(\n",
    "                    topic, classification_result, personas_result\n",
    "                )\n",
    "                structure.update(fallback_elements)\n",
    "                print(\"✅ Fallback global elements created successfully\")\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"⚠️ Fallback also failed: {fallback_error}\")\n",
    "                print(\"📝 Using minimal default global elements...\")\n",
    "                # Add minimal default elements so validation doesn't fail\n",
    "                structure.update(self._get_minimal_global_elements())\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(f\"🎉 Sectional dialogue enhancement completed! (Level: {self.enhancement_level})\")\n",
    "        \n",
    "        return json.dumps(structure, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Helper method to clean JSON response - Enhanced version\"\"\"\n",
    "        response = response.strip()\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic commas and punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('،', ',')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        import re\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json\n",
    "\n",
    "    def validate_enhanced_content(self, enhanced_json):\n",
    "        \"\"\"Validate the enhanced dialogue content (adapted for different levels)\"\"\"\n",
    "        try:\n",
    "            enhanced = json.loads(enhanced_json)\n",
    "            \n",
    "            missing_elements = []\n",
    "            \n",
    "            # Check global sections (varies by level)\n",
    "            if self.enhancement_level == \"minimal\":\n",
    "                required_global = [\"spontaneous_moments\", \"dialogue_techniques\"]\n",
    "            elif self.enhancement_level == \"standard\":\n",
    "                required_global = [\"spontaneous_moments\", \"dialogue_techniques\"]\n",
    "            else:  # full\n",
    "                required_global = [\"spontaneous_moments\", \"personality_interactions\", \"dialogue_techniques\"]\n",
    "                \n",
    "            for element in required_global:\n",
    "                if element not in enhanced:\n",
    "                    missing_elements.append(element)\n",
    "            \n",
    "            # Check enhanced conversation flow\n",
    "            conv_flow = enhanced.get(\"conversation_flow\", {})\n",
    "            \n",
    "            # Check intro1 enhancements\n",
    "            intro1 = conv_flow.get(\"intro1\", {})\n",
    "            if \"spontaneity_elements\" not in intro1:\n",
    "                missing_elements.append(\"intro1.spontaneity_elements\")\n",
    "            \n",
    "            # Check intro2 enhancements  \n",
    "            intro2 = conv_flow.get(\"intro2\", {})\n",
    "            if \"cultural_connections\" not in intro2:\n",
    "                missing_elements.append(\"intro2.cultural_connections\")\n",
    "            \n",
    "            # Check main discussion enhancements (varies by level)\n",
    "            main_disc = conv_flow.get(\"main_discussion\", [])\n",
    "            if self.enhancement_level == \"minimal\":\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"cultural_references\"]\n",
    "            elif self.enhancement_level == \"standard\":\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"cultural_references\", \"natural_transitions\"]\n",
    "            else:  # full\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"disagreement_points\", \"cultural_references\", \"natural_transitions\", \"emotional_triggers\"]\n",
    "            \n",
    "            for i, point in enumerate(main_disc):\n",
    "                for field in required_point_fields:\n",
    "                    if field not in point:\n",
    "                        missing_elements.append(f\"main_discussion[{i}].{field}\")\n",
    "            \n",
    "            # Check closing enhancements (varies by level)\n",
    "            closing = conv_flow.get(\"closing\", {})\n",
    "            conclusion = closing.get(\"conclusion\", {})\n",
    "            outro = closing.get(\"outro\", {})\n",
    "            \n",
    "            if \"emotional_closure\" not in conclusion:\n",
    "                missing_elements.append(\"closing.conclusion.emotional_closure\")\n",
    "            if \"memorable_ending\" not in outro:\n",
    "                missing_elements.append(\"closing.outro.memorable_ending\")\n",
    "                \n",
    "            # Additional checks for standard/full levels\n",
    "            if self.enhancement_level in [\"standard\", \"full\"]:\n",
    "                if \"key_insights\" not in conclusion:\n",
    "                    missing_elements.append(\"closing.conclusion.key_insights\")\n",
    "            if self.enhancement_level == \"full\":\n",
    "                if \"connection_building\" not in outro:\n",
    "                    missing_elements.append(\"closing.outro.connection_building\")\n",
    "            \n",
    "            if missing_elements:\n",
    "                return False, f\"Missing enhanced elements: {missing_elements}\"\n",
    "            \n",
    "            return True, f\"Sectional dialogue content enhancement validation successful (Level: {self.enhancement_level})\"\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            return False, \"Invalid JSON format in enhanced content\"\n",
    "\n",
    "    def _create_fallback_discussion_point(self, topic, classification_result, personas_result, discussion_point, point_index):\n",
    "        \"\"\"Create fallback enhancement for a single discussion point (level-aware)\"\"\"\n",
    "        enhanced_point = discussion_point.copy()\n",
    "        \n",
    "        # Add minimal enhancements based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"هذا يثير تساؤلاً مهماً\",\n",
    "                    \"دعني أشارككم تجربة في هذا المجال\"\n",
    "                ],\n",
    "                \"cultural_references\": [\n",
    "                    \"كما يقول المثل: العلم نور\",\n",
    "                    \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
    "                ]\n",
    "            })\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"هذا يثير تساؤلاً مهماً\",\n",
    "                    \"دعني أشارككم تجربة في هذا المجال\"\n",
    "                ],\n",
    "                \"cultural_references\": [\n",
    "                    \"كما يقول المثل: العلم نور\",\n",
    "                    \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
    "                ],\n",
    "                \"natural_transitions\": \"هذا يقودنا إلى نقطة مهمة أخرى\"\n",
    "            })\n",
    "        else:  # full\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"هذا يثير تساؤلاً مهماً\",\n",
    "                    \"دعني أشارككم تجربة في هذا المجال\"\n",
    "                ],\n",
    "                \"disagreement_points\": \"قد تختلف وجهات النظر حول أفضل طريقة للتعامل مع هذه القضية\",\n",
    "                \"cultural_references\": [\n",
    "                    \"كما يقول المثل: العلم نور\",\n",
    "                    \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
    "                ],\n",
    "                \"natural_transitions\": \"هذا يقودنا إلى نقطة مهمة أخرى\",\n",
    "                \"emotional_triggers\": \"هذا الموضوع يلامس قلوب كل من يهتم بمستقبل ثقافتنا\"\n",
    "            })\n",
    "        \n",
    "        return enhanced_point\n",
    "\n",
    "    def _create_fallback_global_elements(self, topic, classification_result, personas_result):\n",
    "        \"\"\"Create fallback global elements based on enhancement level\"\"\"\n",
    "        try:\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            personas = {}\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "        \n",
    "        fallback_elements = {\n",
    "            \"spontaneous_moments\": {\n",
    "                \"natural_interruptions\": [\n",
    "                    \"اسمحوا لي أن أضيف نقطة هنا\",\n",
    "                    \"هذا يذكرني بموقف مشابه\"\n",
    "                ],\n",
    "                \"emotional_reactions\": [\n",
    "                    \"هذا مؤثر فعلاً\",\n",
    "                    \"لم أفكر في الأمر من هذه الزاوية\"\n",
    "                ]\n",
    "            },\n",
    "            \"dialogue_techniques\": {\n",
    "                \"questioning_styles\": [\n",
    "                    \"أسئلة مفتوحة لتعميق النقاش\",\n",
    "                    \"أسئلة تحليلية للوصول للجذور\"\n",
    "                ],\n",
    "                \"audience_engagement\": [\n",
    "                    \"طرح أسئلة يفكر فيها المستمع\",\n",
    "                    \"استخدام أمثلة من الواقع\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add more elements for standard/full levels\n",
    "        if self.enhancement_level in [\"standard\", \"full\"]:\n",
    "            fallback_elements[\"spontaneous_moments\"][\"personal_stories\"] = [\n",
    "                \"أتذكر موقفاً مشابهاً حدث معي\",\n",
    "                \"في تجربتي الشخصية وجدت أن\"\n",
    "            ]\n",
    "            fallback_elements[\"dialogue_techniques\"][\"storytelling_moments\"] = [\n",
    "                \"سرد تجارب شخصية ذات صلة\",\n",
    "                \"قصص نجاح ملهمة\"\n",
    "            ]\n",
    "        \n",
    "        if self.enhancement_level == \"full\":\n",
    "            fallback_elements[\"spontaneous_moments\"][\"humorous_moments\"] = [\n",
    "                \"هذا يذكرني بنكتة لطيفة\",\n",
    "                \"الموقف له جانب طريف\"\n",
    "            ]\n",
    "            fallback_elements[\"personality_interactions\"] = {\n",
    "                \"host_strengths\": f\"{host_name} ماهر في طرح الأسئلة المناسبة وتوجيه الحوار\",\n",
    "                \"guest_expertise\": f\"{guest_name} يقدم معرفة عميقة في مجال تخصصه\",\n",
    "                \"natural_chemistry\": \"يتفاعل المقدم والضيف بطريقة طبيعية ومريحة\",\n",
    "                \"tension_points\": \"قد يختلفان في بعض وجهات النظر مما يثري النقاش\",\n",
    "                \"collaboration_moments\": \"يبنيان على أفكار بعضهما البعض لإثراء المحتوى\"\n",
    "            }\n",
    "            fallback_elements[\"dialogue_techniques\"][\"emotional_peaks\"] = [\n",
    "                \"لحظات تأملية عميقة\",\n",
    "                \"قصص مؤثرة تلامس القلب\"\n",
    "            ]\n",
    "        \n",
    "        return fallback_elements\n",
    "\n",
    "    def _get_minimal_global_elements(self):\n",
    "        \"\"\"Return minimal default global elements\"\"\"\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"اسمحوا لي أن أضيف نقطة هنا\",\n",
    "                        \"هذا يذكرني بموقف مشابه\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"هذا مؤثر فعلاً\",\n",
    "                        \"لم أفكر في الأمر من هذه الزاوية\"\n",
    "                    ]\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"أسئلة مفتوحة لتعميق النقاش\",\n",
    "                        \"أسئلة تحليلية للوصول للجذور\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"طرح أسئلة يفكر فيها المستمع\",\n",
    "                        \"استخدام أمثلة من الواقع\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"اسمحوا لي أن أضيف نقطة هنا\",\n",
    "                        \"هذا يذكرني بموقف مشابه\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"هذا مؤثر فعلاً\",\n",
    "                        \"لم أفكر في الأمر من هذه الزاوية\"\n",
    "                    ],\n",
    "                    \"personal_stories\": [\n",
    "                        \"أتذكر موقفاً مشابهاً حدث معي\",\n",
    "                        \"في تجربتي الشخصية وجدت أن\"\n",
    "                    ]\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"أسئلة مفتوحة لتعميق النقاش\",\n",
    "                        \"أسئلة تحليلية للوصول للجذور\"\n",
    "                    ],\n",
    "                    \"storytelling_moments\": [\n",
    "                        \"سرد تجارب شخصية ذات صلة\",\n",
    "                        \"قصص نجاح ملهمة\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"طرح أسئلة يفكر فيها المستمع\",\n",
    "                        \"استخدام أمثلة من الواقع\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        else:  # full\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"اسمحوا لي أن أضيف نقطة هنا\",\n",
    "                        \"هذا يذكرني بموقف مشابه\",\n",
    "                        \"انتظر، هذا مهم جداً\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"هذا مؤثر فعلاً\",\n",
    "                        \"لم أفكر في الأمر من هذه الزاوية\",\n",
    "                        \"أتفق معك تماماً\"\n",
    "                    ],\n",
    "                    \"personal_stories\": [\n",
    "                        \"أتذكر موقفاً مشابهاً حدث معي\",\n",
    "                        \"في تجربتي الشخصية وجدت أن\"\n",
    "                    ],\n",
    "                    \"humorous_moments\": [\n",
    "                        \"هذا يذكرني بنكتة لطيفة\",\n",
    "                        \"الموقف له جانب طريف\"\n",
    "                    ]\n",
    "                },\n",
    "                \"personality_interactions\": {\n",
    "                    \"host_strengths\": \"المقدم ماهر في طرح الأسئلة المناسبة وتوجيه الحوار\",\n",
    "                    \"guest_expertise\": \"الضيف يقدم معرفة عميقة في مجال تخصصه\",\n",
    "                    \"natural_chemistry\": \"يتفاعل المقدم والضيف بطريقة طبيعية ومريحة\",\n",
    "                    \"tension_points\": \"قد يختلفان في بعض وجهات النظر مما يثري النقاش\",\n",
    "                    \"collaboration_moments\": \"يبنيان على أفكار بعضهما البعض لإثراء المحتوى\"\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"أسئلة مفتوحة لتعميق النقاش\",\n",
    "                        \"أسئلة تحليلية للوصول للجذور\",\n",
    "                        \"أسئلة شخصية لإضافة البعد الإنساني\"\n",
    "                    ],\n",
    "                    \"storytelling_moments\": [\n",
    "                        \"سرد تجارب شخصية ذات صلة\",\n",
    "                        \"قصص نجاح ملهمة\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"طرح أسئلة يفكر فيها المستمع\",\n",
    "                        \"استخدام أمثلة من الواقع\",\n",
    "                        \"دعوة المستمعين للتفاعل\"\n",
    "                    ],\n",
    "                    \"emotional_peaks\": [\n",
    "                        \"لحظات تأملية عميقة\",\n",
    "                        \"قصص مؤثرة تلامس القلب\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Enhanced Testing Function with Level Selection\n",
    "def test_enhanced_dialogue_content_enhancer(deployment, topic, information, classification_result, personas_result, structure_result, model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"):\n",
    "    \"\"\"\n",
    "    Test the enhanced dialogue content enhancer with level selection\n",
    "    \"\"\"\n",
    "    print(f\"🧪 Testing Enhanced Dialogue Content Enhancer (Level: {enhancement_level})...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    enhancer = SectionalDialogueContentEnhancer(deployment, model_name, enhancement_level)\n",
    "    \n",
    "    # Run enhancement\n",
    "    enhanced_result = enhancer.enhance_dialogue_content(\n",
    "        topic, information, classification_result, personas_result, structure_result\n",
    "    )\n",
    "    \n",
    "    # Validate enhanced content\n",
    "    is_valid, validation_message = enhancer.validate_enhanced_content(enhanced_result)\n",
    "    \n",
    "    print(f\"\\n📊 Enhancement Results (Level: {enhancement_level}):\")\n",
    "    print(f\"Validation: {'✅ Valid' if is_valid else '❌ Invalid'}\")\n",
    "    print(f\"Message: {validation_message}\")\n",
    "    \n",
    "    # Quick content analysis\n",
    "    try:\n",
    "        enhanced_data = json.loads(enhanced_result)\n",
    "        \n",
    "        # Count enhancement fields\n",
    "        conv_flow = enhanced_data.get(\"conversation_flow\", {})\n",
    "        intro1_enhancements = len([k for k in conv_flow.get(\"intro1\", {}).keys() if k not in [\"opening_line\", \"podcast_introduction\", \"episode_hook\", \"tone_guidance\"]])\n",
    "        intro2_enhancements = len([k for k in conv_flow.get(\"intro2\", {}).keys() if k not in [\"topic_introduction\", \"guest_welcome\", \"guest_bio_highlight\", \"transition_to_discussion\"]])\n",
    "        \n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        discussion_enhancements = 0\n",
    "        for point in main_discussion:\n",
    "            discussion_enhancements += len([k for k in point.keys() if k not in [\"point_title\", \"personal_angle\"]])\n",
    "        \n",
    "        closing_enhancements = 0\n",
    "        closing = conv_flow.get(\"closing\", {})\n",
    "        conclusion = closing.get(\"conclusion\", {})\n",
    "        outro = closing.get(\"outro\", {})\n",
    "        closing_enhancements += len([k for k in conclusion.keys() if k not in [\"main_takeaways\", \"guest_final_message\", \"host_closing_thoughts\"]])\n",
    "        closing_enhancements += len([k for k in outro.keys() if k not in [\"guest_appreciation\", \"audience_thanks\", \"call_to_action\", \"final_goodbye\"]])\n",
    "        \n",
    "        global_sections = len([k for k in enhanced_data.keys() if k not in [\"episode_topic\", \"personas\", \"conversation_flow\", \"cultural_context\", \"language_style\", \"technical_notes\"]])\n",
    "        \n",
    "        print(f\"\\n📈 Enhancement Statistics:\")\n",
    "        print(f\"Intro1 Enhancements: {intro1_enhancements}\")\n",
    "        print(f\"Intro2 Enhancements: {intro2_enhancements}\")\n",
    "        print(f\"Discussion Enhancements: {discussion_enhancements}\")\n",
    "        print(f\"Closing Enhancements: {closing_enhancements}\")\n",
    "        print(f\"Global Sections Added: {global_sections}\")\n",
    "        \n",
    "        # Estimate size reduction vs original\n",
    "        total_enhancements = intro1_enhancements + intro2_enhancements + discussion_enhancements + closing_enhancements + global_sections\n",
    "        \n",
    "        if enhancement_level == \"minimal\":\n",
    "            expected_vs_full = \"~60% smaller than full enhancement\"\n",
    "        elif enhancement_level == \"standard\":\n",
    "            expected_vs_full = \"~40% smaller than full enhancement\"\n",
    "        else:\n",
    "            expected_vs_full = \"Full enhancement level\"\n",
    "        \n",
    "        print(f\"Total Enhancement Fields: {total_enhancements}\")\n",
    "        print(f\"Size vs Full: {expected_vs_full}\")\n",
    "        \n",
    "        # Show sample enhanced content\n",
    "        print(f\"\\n🎯 Sample Enhanced Content:\")\n",
    "        intro1 = conv_flow.get(\"intro1\", {})\n",
    "        if \"spontaneity_elements\" in intro1:\n",
    "            spont_elements = intro1[\"spontaneity_elements\"]\n",
    "            print(f\"Intro1 Spontaneity: {len(spont_elements)} elements\")\n",
    "            for i, element in enumerate(spont_elements, 1):\n",
    "                print(f\"  {i}. {element[:60]}...\")\n",
    "        \n",
    "        global_spont = enhanced_data.get(\"spontaneous_moments\", {})\n",
    "        if \"natural_interruptions\" in global_spont:\n",
    "            interruptions = global_spont[\"natural_interruptions\"]\n",
    "            print(f\"Natural Interruptions: {len(interruptions)} items\")\n",
    "            for i, interruption in enumerate(interruptions, 1):\n",
    "                print(f\"  {i}. {interruption[:60]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error analyzing enhanced content: {e}\")\n",
    "    \n",
    "    return enhanced_result\n",
    "\n",
    "# Usage examples for different enhancement levels:\n",
    "\"\"\"\n",
    "# Minimal enhancement (fastest, most concise)\n",
    "enhancer_minimal = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"minimal\")\n",
    "result_minimal = enhancer_minimal.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Standard enhancement (balanced)\n",
    "enhancer_standard = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"standard\")\n",
    "result_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Full enhancement (comprehensive but largest)\n",
    "enhancer_full = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"full\")\n",
    "result_full = enhancer_full.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Test with specific level\n",
    "enhanced_result = test_enhanced_dialogue_content_enhancer(\n",
    "    deployment, topic, information, classification_result, personas_result, structure_result, \n",
    "    model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07febcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Starting sectional dialogue enhancement (Level: standard)...\n",
      "==================================================\n",
      "📝 Chunk 1: Enhancing intro sections...\n",
      "✅ Intro sections enhanced successfully\n",
      "📝 Chunk 2: Enhancing main discussion points...\n",
      "  Enhancing discussion point 1/3...\n",
      "  ✅ Point 1 enhanced successfully\n",
      "  Enhancing discussion point 2/3...\n",
      "  ✅ Point 2 enhanced successfully\n",
      "  Enhancing discussion point 3/3...\n",
      "  ⚠️ Error enhancing point 3: Expecting ',' delimiter: line 4 column 42 (char 153)\n",
      "  🔄 Using fallback enhancement for point 3...\n",
      "  ✅ Point 3 enhanced with fallback method\n",
      "✅ All main discussion points processed\n",
      "📝 Chunk 3: Enhancing closing sections...\n",
      "✅ Closing sections enhanced successfully\n",
      "📝 Chunk 4: Creating global elements...\n",
      "✅ Global elements created successfully\n",
      "==================================================\n",
      "🎉 Sectional dialogue enhancement completed! (Level: standard)\n",
      "Standard Enhancement Result:\n",
      "{\n",
      "  \"episode_topic\": \"نقاش حول الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"أحمد بن علي\",\n",
      "      \"background\": \"مقدم برامج إذاعية معروف بشغفه بالتكنولوجيا والقضايا الاجتماعية\",\n",
      "      \"speaking_style\": \"متفاعل مع الجمهور ويستخدم الأمثلة اليومية لشرح المفاهيم التقنية\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"د. فاتن راشد\",\n",
      "      \"background\": \"باحثة متخصصة في الذكاء الاصطناعي وتطبيقاته اللغوية والثقافية\",\n",
      "      \"speaking_style\": \"توضيح علمي دقيق ممزوج بتوضيحات عملية وشرح للمخاطر والمنافع\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"مرحباً بكم مجدداً مستمعينا الأعزاء!\",\n",
      "      \"podcast_introduction\": \"أتناول اليوم قضية حيوية تتداخل فيها التكنولوجيا بالعادات الثقافية\",\n",
      "      \"episode_hook\": \"موضوع الحلقة: الذكاء الاصطناعي والتحديات التي تطرحها حضارتنا العربية في العصر الرقمي.\",\n",
      "      \"spontaneity_elements\": [\n",
      "        \"أعلم أن هذه القضية تثير الكثير من الأسئلة بين مجتمعنا العربي\",\n",
      "        \"دعونا نبدأ هذه الرحلة المعرفية مع ضيفتنا الفاضلة!\",\n",
      "        \"أشعر حقا بأن فهم تأثير الذكاء الاصطناعي ضروري لحماية تراثنا\"\n",
      "      ]\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"اليوم نستعرض رحلة الذكاء الاصطناعي وكيف يتفاعل مع الهوية العربية وثقافاتها الغنية\",\n",
      "      \"guest_welcome\": \"مرحبًا يا دكتور فاتن! يسعدني وجودك معنا للحديث حول هذا الموضوع الحساس\",\n",
      "      \"guest_bio_highlight\": \"تملك خبرات عميقة في مجال تلاقح الذكاء الاصطناعي باللغة والعادات العربية\",\n",
      "      \"cultural_connections\": [\n",
      "        \"يتطلب هذا النقاش معرفة عميقة بتراثنا الأدبي والشعر الجميل الذي نعيشه\",\n",
      "        \"من المهم أن نتذكر دائمًا جذورنا الإسلامية والأخلاقية أثناء اختراع المستقبل\",\n",
      "        \"يمكن لهذا الحديث أيضًا إلقاء الضوء على أهمية الفنون كالخط والعمارة في مواجهة تكنولوجيات القرن الجديد.\"\n",
      "      ]\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"الجانب الأول والأساسي للموضوع\",\n",
      "        \"personal_angle\": \"كيف يؤثر هذا الموضوع على حياتنا اليومية وتجاربنا\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"مثلاً, تخيل أن الذكاء الاصطناعي بدأ يوظف الألفاظ الشعبية المميزة\",\n",
      "          \"أحياناً نلاحظ تعديلات غير مقصودة في النصوص المقدسة عبر أدوات التحرير بالذكاء الاصطناعي\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"قد يُذكر هنا بيت شعر عن الوحدة الوطنية\",\n",
      "          \"يمكن ربط الحوار بأعمال روائيين عرب مثل نجيب محفوظ\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"وهذا يقودنا إلى طرح السؤال المثير للتفكير...\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثاني والتحديات المرتبطة\",\n",
      "        \"personal_angle\": \"التحديات والفرص المتاحة في هذا المجال\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"كيف يمكن أن يتداخل التعلم الآلي مع القيم الإسلامية التقليدية؟\",\n",
      "          \"ما هي الحالات التي قد يفقد فيها الذكاء الاصطناعي الفهم الدقيق للثقافة العربية؟\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"مفهوم 'العقل' في الفلسفة العربية الإسلامية\",\n",
      "          \"أمثلة من الأدب العربي الكلاسيكي مثل كتاب ألف ليلة وليلة\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"قبل الغوص بشكل أعمق في هذه التحديات, دعونا نستذكر...\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثالث والحلول المقترحة\",\n",
      "        \"personal_angle\": \"النصائح والتوجيهات العملية للمستقبل\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"هذا يثير تساؤلاً مهماً\",\n",
      "          \"دعني أشارككم تجربة في هذا المجال\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"كما يقول المثل: العلم نور\",\n",
      "          \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"هذا يقودنا إلى نقطة مهمة أخرى\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"الخلاصات الرئيسية لهذا الحوار تشدد على أهمية الوعي بالذكاء الاصطناعي وحفظ الهوية الثقافية.\",\n",
      "        \"guest_final_message\": \"دعونا نواصل تبادل المعرفة والحفاظ على ثقافتنا بينما نسعى لاستخدام التكنولوجيا لصالحنا.\",\n",
      "        \"host_closing_thoughts\": \"من المحادثات مثل هذه, يمكننا أن نفهم أفضل كيفية الاستفادة من الذكاء الاصطناعي دون الإخلال بتقاليدنا وقيمنا.\",\n",
      "        \"emotional_closure\": \"نسأل الله أن يلهمنا استخدام التقدم التقني بحكمة وفائدة.\",\n",
      "        \"key_insights\": [\n",
      "          \"التعرف على الأثر المتزايد للذكاء الاصطناعي في حياتنا هو الخطوة الأولى نحو استخدامه بشكل مسؤول.\",\n",
      "          \"دمج القيم الثقافية والفكرانية العربية مع تكنولوجيات جديدة يساعد في بناء مجتمع رقمي أكثر شمولاً.\"\n",
      "        ]\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"شكرا مرة أخرى للدكتورة فاتن راشد لمشاركتها الثاقبة.\",\n",
      "        \"audience_thanks\": \"شكراً لكل الذين انضموا إلينا في هذه الحلقة الطموحة.\",\n",
      "        \"call_to_action\": \"شارك أفكارك حول دور الذكاء الاصطناعي في حماية الهوية العربية على منصات التواصل لدينا.\",\n",
      "        \"memorable_ending\": \"نتوحد جميعًا لننير طريق عصرنا الرقمي بأضواء حضارتنا العربية الغنية.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"العلم نور والجهل ظلام\",\n",
      "      \"في التأني السلامة وفي العجلة الندامة\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"التجربة العربية الغنية في هذا المجال\",\n",
      "      \"الخبرات المحلية والإقليمية ذات الصلة بالموضوع\"\n",
      "    ]\n",
      "  },\n",
      "  \"spontaneous_moments\": {\n",
      "    \"natural_interruptions\": [\n",
      "      \"أحمد, مع كل تقدّم التكنولوجيا, هل يمكن أن يشعر الحضور أنّ الذكاء الصناعي قد يُنسِينا لغتنا الأم?\",\n",
      "      \"فاتن, ما رأيك بالدور الذي يلعبُه الذكرى الشخصية والأواصر الثقافية في منع الخسارة المحتملة للهوية\"\n",
      "    ],\n",
      "    \"emotional_reactions\": [\n",
      "      \"أحمد(بتعاطف), هذه مخاوف حقيقية ولكن دعنا لا ننسي الفرص التي يوفرها التطور!\",\n",
      "      \"فاتن(بإلحاح), يجب علينا إلهام الشباب لتحفيز الإبداع والتفاعل عبر الأجيال للحفاظ على جذورنا\"\n",
      "    ],\n",
      "    \"personal_stories\": [\n",
      "      \"أوضحَ أحمد قصته عن استخدام ذكائه الخاص لتعزيز الشعر العربي الفصحى.\",\n",
      "      \"تشاركتِ د. فاتن تجربتها مع إنشاء برنامج يحفظ ويروِي الحكم الأدبية العربية\"\n",
      "    ]\n",
      "  },\n",
      "  \"dialogue_techniques\": {\n",
      "    \"questioning_styles\": [\n",
      "      \"أحمد يوجه سؤالاً مفتوحاً لأستاذتنا:\",\n",
      "      \"تحثّينا الدكتورة فاتن السماع من خلال طرحها لسؤال مغلق.\"\n",
      "    ],\n",
      "    \"storytelling_moments\": [\n",
      "      \"يعبر أحمد بصراحةعن قصة تفرده مع اللغة العربية و الذكريات الطفولة,\",\n",
      "      \"تحدِّد الطبيبة فاتن جانب التعليم بالمشاركة بتجربة شخصية حول أهمَّة تعلم الأطفال للغة الآباء\"\n",
      "    ],\n",
      "    \"audience_engagement\": [\n",
      "      \"يجذب احمد الجمهور بإشراكهم بسؤاله:\",\n",
      "      \"تشجع فاطمةالمستمعين لتشارك أفكارهم أو أسئتهم المتعلقةبالtopic.\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Standard enhancement (balanced)\n",
    "enhancer_standard = SectionalDialogueContentEnhancer(deployment, model, \"standard\")\n",
    "result_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "print(\"Standard Enhancement Result:\")\n",
    "print(result_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a852810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "class MinimalPolishEnhancer:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def enhance_spontaneous_moments_values(self, topic, classification_result, personas_result, current_spontaneous_moments):\n",
    "        \"\"\"\n",
    "        Chunk 1: Enhance values in existing spontaneous_moments (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast dialogue quality.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing spontaneous_moments structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current spontaneous moments: {json.dumps(current_spontaneous_moments, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make each phrase more natural and topic-specific\n",
    "- Connect phrases directly to the topic: {topic}\n",
    "- Make content specific to {host_name} and {guest_name}'s backgrounds\n",
    "- Ensure phrases sound more authentic and conversational\n",
    "\n",
    "Return the enhanced structure with the same fields but better values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these exact personas\n",
    "- Improve naturalness and authenticity of existing phrases\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance existing values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_cultural_context_values(self, topic, classification_result, current_cultural_context):\n",
    "        \"\"\"\n",
    "        Chunk 2: Enhance values in existing cultural_context (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "        except:\n",
    "            classification = {}\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic cultural references.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing cultural_context structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "\n",
    "Current cultural context: {json.dumps(current_cultural_context, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and relevance of existing values\n",
    "- Make proverbs more directly relevant to the topic: {topic}\n",
    "- Make regional references more specific and meaningful\n",
    "- Ensure cultural authenticity and accuracy\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"العلم نور\"\n",
    "- After: \"العلم نور، والذكاء الاصطناعي يمكن أن يكون شمعة تضيء طريق الحفاظ على تراثنا\"\n",
    "\n",
    "Return the enhanced cultural context with better, more topic-specific values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make all content directly relevant to topic: {topic}\n",
    "- Maintain cultural authenticity and accuracy\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance cultural context values only. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_dialogue_techniques_values(self, topic, classification_result, personas_result, current_dialogue_techniques):\n",
    "        \"\"\"\n",
    "        Chunk 3: Enhance values in existing dialogue_techniques (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast dialogue techniques.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing dialogue_techniques structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current dialogue techniques: {json.dumps(current_dialogue_techniques, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make techniques more specific to the topic: {topic}\n",
    "- Tailor content to {host_name} and {guest_name}'s expertise\n",
    "- Make techniques more actionable and practical\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"أسئلة مفتوحة لتعميق النقاش\"\n",
    "- After: \"أسئلة مفتوحة حول كيفية تطوير ذكاء اصطناعي يحافظ على جمالية اللغة العربية وعمقها الثقافي\"\n",
    "\n",
    "Return the enhanced dialogue techniques with better, more specific values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve practicality and specificity of techniques\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance dialogue technique values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_main_discussion_values(self, topic, classification_result, personas_result, current_main_discussion):\n",
    "        \"\"\"\n",
    "        Chunk 4: Enhance values in existing main_discussion points (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast discussion content.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing main_discussion structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current main discussion: {json.dumps(current_main_discussion, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names for each discussion point\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make spontaneous_triggers more natural and topic-specific\n",
    "- Make cultural_references more directly relevant to the topic\n",
    "- Make natural_transitions smoother and more contextual\n",
    "- Ensure content reflects {host_name} and {guest_name}'s specific expertise\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"هذا يثير تساؤلاً مهماً\"\n",
    "- After: \"هذا يثير تساؤلاً مهماً حول قدرة الذكاء الاصطناعي على فهم السياق الثقافي وراء الكلمات العربية\"\n",
    "\n",
    "Return the enhanced main discussion with better, more specific values:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        [exact same structure as each input point, but with enhanced values]\n",
    "    }},\n",
    "    ...\n",
    "]\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve naturalness and conversational flow\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON array, no extra text\n",
    "- Do NOT add new fields to discussion points\n",
    "- Do NOT change array lengths within points\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance discussion point values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_intro_outro_values(self, topic, classification_result, personas_result, current_intro1, current_intro2, current_closing):\n",
    "        \"\"\"\n",
    "        Chunk 5: Enhance values in existing intro and outro sections (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast intros and outros.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing intro1, intro2, and closing structures. Keep the exact same fields.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current intro1: {json.dumps(current_intro1, ensure_ascii=False)}\n",
    "Current intro2: {json.dumps(current_intro2, ensure_ascii=False)}\n",
    "Current closing: {json.dumps(current_closing, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array (if any)\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make opening_line more engaging and natural\n",
    "- Make episode_hook more compelling and topic-specific\n",
    "- Make guest_welcome more personal and authentic\n",
    "- Make closing thoughts more memorable and impactful\n",
    "- Ensure content reflects the personalities of {host_name} and {guest_name}\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"أهلاً بكم مستمعينا الكرام\"\n",
    "- After: \"أهلاً بكم مستمعينا الكرام في رحلة استكشافية مثيرة لنتعرف على كيفية جعل الذكاء الاصطناعي حارساً لتراثنا العربي\"\n",
    "\n",
    "Return the enhanced sections:\n",
    "\n",
    "{{\n",
    "    \"intro1\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }},\n",
    "    \"intro2\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }},\n",
    "    \"closing\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve engagement and naturalness\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance intro/outro values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def apply_minimal_polish(self, topic, information, classification_result, personas_result, enhanced_content_result):\n",
    "        \"\"\"\n",
    "        Main orchestration method: Coordinates all value enhancement chunks\n",
    "        \"\"\"\n",
    "        print(\"🎨 Starting minimal polish (value enhancement only)...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            enhanced_content = json.loads(enhanced_content_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid enhanced content JSON provided\")\n",
    "        \n",
    "        # Chunk 1: Enhance spontaneous moments values\n",
    "        print(\"✨ Chunk 1: Enhancing spontaneous moments values...\")\n",
    "        try:\n",
    "            current_spontaneous = enhanced_content.get(\"spontaneous_moments\", {})\n",
    "            if current_spontaneous:  # Only enhance if exists\n",
    "                enhanced_spontaneous_json = self.enhance_spontaneous_moments_values(\n",
    "                    topic, classification_result, personas_result, current_spontaneous\n",
    "                )\n",
    "                enhanced_spontaneous = json.loads(enhanced_spontaneous_json)\n",
    "                enhanced_content[\"spontaneous_moments\"] = enhanced_spontaneous\n",
    "                print(\"✅ Spontaneous moments values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No spontaneous moments to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing spontaneous moments: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 2: Enhance cultural context values\n",
    "        print(\"✨ Chunk 2: Enhancing cultural context values...\")\n",
    "        try:\n",
    "            current_cultural = enhanced_content.get(\"cultural_context\", {})\n",
    "            if current_cultural:  # Only enhance if exists\n",
    "                enhanced_cultural_json = self.enhance_cultural_context_values(\n",
    "                    topic, classification_result, current_cultural\n",
    "                )\n",
    "                enhanced_cultural = json.loads(enhanced_cultural_json)\n",
    "                enhanced_content[\"cultural_context\"] = enhanced_cultural\n",
    "                print(\"✅ Cultural context values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No cultural context to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing cultural context: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 3: Enhance dialogue techniques values\n",
    "        print(\"✨ Chunk 3: Enhancing dialogue techniques values...\")\n",
    "        try:\n",
    "            current_dialogue = enhanced_content.get(\"dialogue_techniques\", {})\n",
    "            if current_dialogue:  # Only enhance if exists\n",
    "                enhanced_dialogue_json = self.enhance_dialogue_techniques_values(\n",
    "                    topic, classification_result, personas_result, current_dialogue\n",
    "                )\n",
    "                enhanced_dialogue = json.loads(enhanced_dialogue_json)\n",
    "                enhanced_content[\"dialogue_techniques\"] = enhanced_dialogue\n",
    "                print(\"✅ Dialogue techniques values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No dialogue techniques to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing dialogue techniques: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 4: Enhance main discussion values\n",
    "        print(\"✨ Chunk 4: Enhancing main discussion values...\")\n",
    "        try:\n",
    "            conv_flow = enhanced_content.get(\"conversation_flow\", {})\n",
    "            current_main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "            if current_main_discussion:  # Only enhance if exists\n",
    "                enhanced_discussion_json = self.enhance_main_discussion_values(\n",
    "                    topic, classification_result, personas_result, current_main_discussion\n",
    "                )\n",
    "                enhanced_discussion = json.loads(enhanced_discussion_json)\n",
    "                enhanced_content[\"conversation_flow\"][\"main_discussion\"] = enhanced_discussion\n",
    "                print(\"✅ Main discussion values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No main discussion to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing main discussion: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 5: Enhance intro and outro values\n",
    "        print(\"✨ Chunk 5: Enhancing intro and outro values...\")\n",
    "        try:\n",
    "            conv_flow = enhanced_content.get(\"conversation_flow\", {})\n",
    "            current_intro1 = conv_flow.get(\"intro1\", {})\n",
    "            current_intro2 = conv_flow.get(\"intro2\", {})\n",
    "            current_closing = conv_flow.get(\"closing\", {})\n",
    "            \n",
    "            if current_intro1 or current_intro2 or current_closing:  # Only enhance if any exist\n",
    "                enhanced_sections_json = self.enhance_intro_outro_values(\n",
    "                    topic, classification_result, personas_result, \n",
    "                    current_intro1, current_intro2, current_closing\n",
    "                )\n",
    "                enhanced_sections = json.loads(enhanced_sections_json)\n",
    "                \n",
    "                if \"intro1\" in enhanced_sections and current_intro1:\n",
    "                    enhanced_content[\"conversation_flow\"][\"intro1\"] = enhanced_sections[\"intro1\"]\n",
    "                if \"intro2\" in enhanced_sections and current_intro2:\n",
    "                    enhanced_content[\"conversation_flow\"][\"intro2\"] = enhanced_sections[\"intro2\"]\n",
    "                if \"closing\" in enhanced_sections and current_closing:\n",
    "                    enhanced_content[\"conversation_flow\"][\"closing\"] = enhanced_sections[\"closing\"]\n",
    "                \n",
    "                print(\"✅ Intro and outro values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No intro/outro sections to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing intro/outro: {e}\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"🎉 Minimal polish completed! Same structure, enhanced values.\")\n",
    "        \n",
    "        return json.dumps(enhanced_content, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Enhanced JSON cleaning method\"\"\"\n",
    "        response = response.strip()\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Handle arrays\n",
    "        if clean_json.strip().startswith('['):\n",
    "            start_idx = response.find('[')\n",
    "            end_idx = response.rfind(']')\n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                clean_json = response[start_idx:end_idx+1]\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('،', ',')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        import re\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        return clean_json\n",
    "\n",
    "    def validate_polished_outline(self, original_json, polished_json):\n",
    "        \"\"\"\n",
    "        Validate that polished outline has same structure but enhanced values\n",
    "        \"\"\"\n",
    "        try:\n",
    "            original = json.loads(original_json)\n",
    "            polished = json.loads(polished_json)\n",
    "            \n",
    "            issues = []\n",
    "            \n",
    "            # Check that main structure is preserved\n",
    "            original_keys = set(original.keys())\n",
    "            polished_keys = set(polished.keys())\n",
    "            \n",
    "            if original_keys != polished_keys:\n",
    "                issues.append(f\"Main structure changed: {original_keys} vs {polished_keys}\")\n",
    "            \n",
    "            # Check conversation flow structure\n",
    "            orig_conv = original.get(\"conversation_flow\", {})\n",
    "            pol_conv = polished.get(\"conversation_flow\", {})\n",
    "            \n",
    "            if set(orig_conv.keys()) != set(pol_conv.keys()):\n",
    "                issues.append(\"Conversation flow structure changed\")\n",
    "            \n",
    "            # Check main discussion array length\n",
    "            orig_main = orig_conv.get(\"main_discussion\", [])\n",
    "            pol_main = pol_conv.get(\"main_discussion\", [])\n",
    "            \n",
    "            if len(orig_main) != len(pol_main):\n",
    "                issues.append(f\"Main discussion length changed: {len(orig_main)} vs {len(pol_main)}\")\n",
    "            \n",
    "            # Check that arrays within sections maintain length\n",
    "            sections_to_check = [\"spontaneous_moments\", \"dialogue_techniques\", \"cultural_context\"]\n",
    "            \n",
    "            for section in sections_to_check:\n",
    "                if section in original and section in polished:\n",
    "                    orig_section = original[section]\n",
    "                    pol_section = polished[section]\n",
    "                    \n",
    "                    if isinstance(orig_section, dict) and isinstance(pol_section, dict):\n",
    "                        for key in orig_section:\n",
    "                            if isinstance(orig_section[key], list) and isinstance(pol_section.get(key), list):\n",
    "                                if len(orig_section[key]) != len(pol_section[key]):\n",
    "                                    issues.append(f\"{section}.{key} array length changed\")\n",
    "            \n",
    "            # Check for quality improvement (simple heuristic)\n",
    "            orig_text = json.dumps(original, ensure_ascii=False)\n",
    "            pol_text = json.dumps(polished, ensure_ascii=False)\n",
    "            \n",
    "            if len(pol_text) < len(orig_text) * 0.95:  # Significant reduction might indicate loss of content\n",
    "                issues.append(\"Polished content appears significantly shorter\")\n",
    "            \n",
    "            if issues:\n",
    "                return False, f\"Structure validation issues: {issues}\"\n",
    "            \n",
    "            return True, \"Minimal polish validation successful - same structure, enhanced values\"\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            return False, f\"JSON parsing error: {e}\"\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_minimal_polish_enhancer(deployment, topic, information, classification_result, personas_result, enhanced_content_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the minimal polish enhancer\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Minimal Polish Enhancer (Value Enhancement Only)...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    polisher = MinimalPolishEnhancer(deployment, model_name)\n",
    "    \n",
    "    # Store original for comparison\n",
    "    original_json = enhanced_content_result\n",
    "    \n",
    "    # Run minimal polish\n",
    "    polished_result = polisher.apply_minimal_polish(\n",
    "        topic, information, classification_result, personas_result, enhanced_content_result\n",
    "    )\n",
    "    \n",
    "    # Validate polished content\n",
    "    is_valid, validation_message = polisher.validate_polished_outline(original_json, polished_result)\n",
    "    \n",
    "    print(f\"\\n📊 Polish Results:\")\n",
    "    print(f\"Validation: {'✅ Valid' if is_valid else '❌ Invalid'}\")\n",
    "    print(f\"Message: {validation_message}\")\n",
    "    \n",
    "    # Quick comparison analysis\n",
    "    try:\n",
    "        original_data = json.loads(original_json)\n",
    "        polished_data = json.loads(polished_result)\n",
    "        \n",
    "        # Compare sample values\n",
    "        print(f\"\\n🔍 Sample Value Comparisons:\")\n",
    "        \n",
    "        # Spontaneous moments comparison\n",
    "        orig_spont = original_data.get(\"spontaneous_moments\", {}).get(\"natural_interruptions\", [])\n",
    "        pol_spont = polished_data.get(\"spontaneous_moments\", {}).get(\"natural_interruptions\", [])\n",
    "        \n",
    "        if orig_spont and pol_spont:\n",
    "            print(f\"Natural Interruptions:\")\n",
    "            print(f\"  Original: {orig_spont[0][:50]}...\")\n",
    "            print(f\"  Polished: {pol_spont[0][:50]}...\")\n",
    "        \n",
    "        # Cultural context comparison\n",
    "        orig_cultural = original_data.get(\"cultural_context\", {}).get(\"proverbs_sayings\", [])\n",
    "        pol_cultural = polished_data.get(\"cultural_context\", {}).get(\"proverbs_sayings\", [])\n",
    "        \n",
    "        if orig_cultural and pol_cultural:\n",
    "            print(f\"Proverbs:\")\n",
    "            print(f\"  Original: {orig_cultural[0][:50]}...\")\n",
    "            print(f\"  Polished: {pol_cultural[0][:50]}...\")\n",
    "        \n",
    "        # Size comparison\n",
    "        orig_size = len(json.dumps(original_data, ensure_ascii=False))\n",
    "        pol_size = len(json.dumps(polished_data, ensure_ascii=False))\n",
    "        size_change = ((pol_size - orig_size) / orig_size) * 100\n",
    "        \n",
    "        print(f\"\\n📈 Size Analysis:\")\n",
    "        print(f\"Original Size: {orig_size:,} characters\")\n",
    "        print(f\"Polished Size: {pol_size:,} characters\")\n",
    "        print(f\"Size Change: {size_change:+.1f}%\")\n",
    "        print(f\"Approach: {'✅ Value enhancement only' if abs(size_change) < 15 else '⚠️ Significant size change'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error analyzing polished content: {e}\")\n",
    "    \n",
    "    return polished_result\n",
    "\n",
    "# Usage:\n",
    "# polisher = MinimalPolishEnhancer(deployment, \"Fanar-C-1-8.7B\")\n",
    "# final_polished_outline = polisher.apply_minimal_polish(topic, information, classification_result, personas_result, enhanced_content_result)\n",
    "\n",
    "# Test the polisher\n",
    "# polished_result = test_minimal_polish_enhancer(\n",
    "#     deployment, topic, information, classification_result, personas_result, enhanced_content_result\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "967209c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 Starting minimal polish (value enhancement only)...\n",
      "==================================================\n",
      "✨ Chunk 1: Enhancing spontaneous moments values...\n",
      "✅ Spontaneous moments values enhanced successfully\n",
      "✨ Chunk 2: Enhancing cultural context values...\n",
      "✅ Cultural context values enhanced successfully\n",
      "✨ Chunk 3: Enhancing dialogue techniques values...\n",
      "⚠️ Error enhancing dialogue techniques: Expecting ',' delimiter: line 12 column 5 (char 772)\n",
      "✨ Chunk 4: Enhancing main discussion values...\n",
      "⚠️ Error enhancing main discussion: Expecting ',' delimiter: line 12 column 140 (char 877)\n",
      "✨ Chunk 5: Enhancing intro and outro values...\n",
      "✅ Intro and outro values enhanced successfully\n",
      "==================================================\n",
      "🎉 Minimal polish completed! Same structure, enhanced values.\n",
      "Final Polished Outline:\n",
      "{\n",
      "  \"episode_topic\": \"نقاش حول الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"أحمد بن علي\",\n",
      "      \"background\": \"مقدم برامج إذاعية معروف بشغفه بالتكنولوجيا والقضايا الاجتماعية\",\n",
      "      \"speaking_style\": \"متفاعل مع الجمهور ويستخدم الأمثلة اليومية لشرح المفاهيم التقنية\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"د. فاتن راشد\",\n",
      "      \"background\": \"باحثة متخصصة في الذكاء الاصطناعي وتطبيقاته اللغوية والثقافية\",\n",
      "      \"speaking_style\": \"توضيح علمي دقيق ممزوج بتوضيحات عملية وشرح للمخاطر والمنافع\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"انطلق بنا مباشرة إلى قلب تحدٍ عصري يغوص في أعماق تقاطع الذكاء الاصطناعي والثقافة العربية, حيث سنضع كلانا أيديهما بقلب متنبه ومشوق.\",\n",
      "      \"podcast_introduction\": \"في هذه الحلقة المثيرة, سوف نقوم بفَكِّ رموز الآثار المزدوجة للتكنولوجيا الحديثة على جوهر هُويتنا ونمط عيشنا الإسلامي الأصيل.\",\n",
      "      \"episode_hook\": \"إن موضوع يومنا يدور حول السؤال الكبير: هل يمكن أن يصبح الذكاء الاصطناعي صانعاً لشهاداتنا أم محذفا لها؟ دعونا ندخل إلى مفترق طرق هذه المسائل المُلحّة جنبا إلى جنب مع خبيرة لا تُجارى.\",\n",
      "      \"spontaneity_elements\": [\n",
      "        \"بكل صدق, أنا أتطلع لسماع الأفكار الرائعة للدكتورة فاتن راشد والتي ستساعدنا على توضيح الصورة\",\n",
      "        \"لنبدأ جولة بحثية غنية ستمكِّننا من رؤية الذكاء الاصطناعي ليس كتهديد بل كفرصة لبناء جسور بين الماضي والمستقبل.\",\n",
      "        \"هذا النوع من المناقشة مهم للغاية لحاملي الراية الجديدة - نحن الشباب - الذين يجب عليهم حمل شعلة التعلم عن طبيعة تأثيرات الذكاء الاصطناعي على قيمنا الإسلامية وانتقادها نقدًا حياديًا.\"\n",
      "      ]\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"تحيا معنا الآن رحلة شيقة عبر عالمين مختلفين لكنهما مترابطان؛ العالم الرقمي المتحرك سريع النهوض وعالم تراثنا التاريخي العريق.\",\n",
      "      \"guest_welcome\": \"الدكتورة فاتن راشد, مرحباً بكِ رسميًا في برنامجنا! أنت تنشر دائماً الانطباعات الأكثر طرافة عندما يتعلق الأمر بمناقشة حدود التقارب بين الذكاء الاصطناعي والتراث الثقافي والمعرفي للشعب العربي.\",\n",
      "      \"guest_bio_highlight\": \"لديك خبرة واسعة جدًا في تدريس واستيعاب المفاهيم المتعلقة باستخدام اللغة العربية ومعانيها بطريقة مبتكرة ضمن السياق الخاص بتطبيقات الذكاء الاصطناعي.\",\n",
      "      \"cultural_connections\": [\n",
      "        \"الإبداع الشعري العربى القديم يشكل جزء أساسي من أساس تكوين شخصيتنا, لذلك فإن تخيل آفاقه تحت مظلة الذكاء الاصطناعي أمر يستحق النظر فيه مليئ بالسحر والإمكانيات.\",\n",
      "        \"لا ينبغي أن نواجه التطور التكنولوجي بخوف أو رفض ولكن بدراسة مدروسة تجمع ما بين روح التفكير التصميمي وبين الأخلاق الإسلامية لإنتاج ابتكارات ذات جدوى اجتماعية.\",\n",
      "        \"من وجه نظر علم جمالية البيئة العمرانية وما تركه لنا الفنانون السابقون, ربما تستطيع الصناعة الروبوتية المستقبليّة إلهام تعديلات وإعادة تقديم للأعمال الهندسية الباهرة القديمة!\"\n",
      "      ]\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"الجانب الأول والأساسي للموضوع\",\n",
      "        \"personal_angle\": \"كيف يؤثر هذا الموضوع على حياتنا اليومية وتجاربنا\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"مثلاً, تخيل أن الذكاء الاصطناعي بدأ يوظف الألفاظ الشعبية المميزة\",\n",
      "          \"أحياناً نلاحظ تعديلات غير مقصودة في النصوص المقدسة عبر أدوات التحرير بالذكاء الاصطناعي\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"قد يُذكر هنا بيت شعر عن الوحدة الوطنية\",\n",
      "          \"يمكن ربط الحوار بأعمال روائيين عرب مثل نجيب محفوظ\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"وهذا يقودنا إلى طرح السؤال المثير للتفكير...\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثاني والتحديات المرتبطة\",\n",
      "        \"personal_angle\": \"التحديات والفرص المتاحة في هذا المجال\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"كيف يمكن أن يتداخل التعلم الآلي مع القيم الإسلامية التقليدية؟\",\n",
      "          \"ما هي الحالات التي قد يفقد فيها الذكاء الاصطناعي الفهم الدقيق للثقافة العربية؟\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"مفهوم 'العقل' في الفلسفة العربية الإسلامية\",\n",
      "          \"أمثلة من الأدب العربي الكلاسيكي مثل كتاب ألف ليلة وليلة\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"قبل الغوص بشكل أعمق في هذه التحديات, دعونا نستذكر...\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثالث والحلول المقترحة\",\n",
      "        \"personal_angle\": \"النصائح والتوجيهات العملية للمستقبل\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"هذا يثير تساؤلاً مهماً\",\n",
      "          \"دعني أشارككم تجربة في هذا المجال\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"كما يقول المثل: العلم نور\",\n",
      "          \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"هذا يقودنا إلى نقطة مهمة أخرى\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"بعد تفاصيل بحثنا الواضح الدقيق, بات واضحًا أنه بالإمكان تحقيق توازن رائع بين امتصاص اعتمادات العصر الاحتوائي وأصولنا الوطنية إذا تم اتخاذ قرارات حكيمة ومسؤولة.\",\n",
      "        \"guest_final_message\": \"دعونا نبقى ملتزمين بعمل المزيد من المنشورات والدراسات العلمية ونعمل بلا هوادة لتحويل رؤانا العميقة لهذه المواضيع إلى واقع ملموس وذلك بإرشاد شباب المسلمين للاستثمار الأمثل لما تمتلكه تقنيات ذكائنا العالمي مؤخرًا من قدرات غير مسبوقة.\",\n",
      "        \"host_closing_thoughts\": \"إلى جانب ذلك, يبقى مصير تحديد مدى مطابقة فقدان أصالة ثقافتنا مقابل مكاسب تقدم البشرية الهائلة أمامنا — وفي النهاية, يكمن القرار عندنا جميعًا كجيل رخاء تكنولوجي حديث الحضور ومدافعين ممتازين لأسلوب حياة عربي مسلم مميز.\",\n",
      "        \"emotional_closure\": \"وفي نهاية المطاف, دعونا نسمو كافةً بروح شكر لله عز وجل لأنه منحنا تلك الفرصة المدهشة للحياة واختبرنا فيها القدرة الغريبة للفهم الإنساني المبهر لعالمنا الجامع المتنوع.\",\n",
      "        \"key_insights\": [\n",
      "          \"تعزيز الصراحة بشأن التدفق الناجم من تطبيق برامج الذكاء الاصطناعي فى الحياة السياسية والجماهيرية يعد امر مقبول ويجب اعتباره خطوة أولى towards تحسين إدارة الحكم الإلكتروني بين دول العالم الثالث.\",\n",
      "          \"متابعة عملية التآزر المدروسة بين عناصر النظام البرمجي وصياغة النصوص وتعريف محتوى الرسائل الإعلامية لفترة زمنية قصيرة تعتبر مفتاحا مهما لوجود نوع جديد مختلط متنوع من النسيج الاجتماعي المستدام .\"\n",
      "        ]\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"مرة آخرى نشكرك جزيل الشكر للدكتورة فاتن راشد عليها مشاركتها المؤصلة للمعلومات في حلقتنا الجدلية الملهمة اليوم.\",\n",
      "        \"audience_thanks\": \"جزيل الامتنان لك أيها العزيز المشاهد المخلص لأنك خصَّصت وقتك ثمين لقضاء ساعة برفقتنا هنا.\",\n",
      "        \"call_to_action\": \"استخدم وسائط التواصل الاجتماعى الخاصة برنامجنَا ليخبرنا رأيك:هل تؤمن بأنه يمكن للتوظيف الأفضل لأسرار المجتمع الإنترنتى تساهم فى زيادة تقديسنا الإقليمى للعصور الزاهرة المجيدة ؟\",\n",
      "        \"memorable_ending\": \"حتى لقاء اخر, فلنرتبط سوياً وتمسك بكل ما يؤمن به عقليتنا الخلابة واتخذ مواقع ثابتة كباحثين مغامرين طموحين يحققون أحلام أبنائهم وشباب وطنيات مستنفذة لرؤية أكبر انتصار للإسلام عالميا !\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"فهم التكنولوجيا مثل فهم اللغة العربية — كلاهما يحتاج إلى إتقان لتجنب تشويه المعاني.\",\n",
      "      \"كما يُسترشَدُ بالنجوم لِتحديد الطريقِ الصحيح, يجب علينا توجيهَ ذكاء اصطناعي نحو تعزيز هويتنا.\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"مثال الأندلس الذي اجتمع فيه العلم والفكر الإسلامي مع الإبداع التقني باعتباره حافزا لنا اليوم.\",\n",
      "      \"استخدام تقنيات التعرف على الخط العربي للحفاظ على الكتابة اليدوية الأصيلة ضمن عصر رقمي.\"\n",
      "    ]\n",
      "  },\n",
      "  \"spontaneous_moments\": {\n",
      "    \"natural_interruptions\": [\n",
      "      \"أحمد, من الرائع رؤية تطبيقات AI المتنوعة لكن, هل يمكن للذكاء الاصطناعي مساعدتنا فعلاً في تطوير أدوات حفظ وتحسين اللغة العربية أم سيؤدي إلى تبسيط ثقافتنا الغنية والمتعددة الطبقات؟\",\n",
      "      \"فاتن, كمختصة في هذا المجال, كيف ترين دور خوارزميات التعلم الآلي في الحفاظ على الهوية العربية وخلق محتوى رقمي يتوافق مع القيم الإسلامية?\"\n",
      "    ],\n",
      "    \"emotional_reactions\": [\n",
      "      \"أحمد(باسلوب متحمس), هناك إمكانية لاتحاد تقنيات اليوم ومعاصرة الفنون التقليدية! لنحتضن الابتكار بينما نفتخر بأصولنا.,\",\n",
      "      \"فاتن(بخطاب مدروس), طبعاً, يجب توجيه ابتكارات الذكاء الاصطناعي نحو تقديم قيمة حقيقية لبناء مجتمع عربي رقمياً ثري ومتماسك.\"\n",
      "    ],\n",
      "    \"personal_stories\": [\n",
      "      \"شاركَ أحمد رحلته الملهمة حول تدريب نموذج ماشيني لتوليد شعر بديع بالأسلوب العربي الكلاسيكي.\",\n",
      "      \"تحدثت د. فاتن عن مشروعها الأخير لمساعدة الروبوتات التعليمية على فهم وإشراك الأطفال العرب بحكاياتهم الشعبية الشهيرة.\"\n",
      "    ]\n",
      "  },\n",
      "  \"dialogue_techniques\": {\n",
      "    \"questioning_styles\": [\n",
      "      \"أحمد يوجه سؤالاً مفتوحاً لأستاذتنا:\",\n",
      "      \"تحثّينا الدكتورة فاتن السماع من خلال طرحها لسؤال مغلق.\"\n",
      "    ],\n",
      "    \"storytelling_moments\": [\n",
      "      \"يعبر أحمد بصراحةعن قصة تفرده مع اللغة العربية و الذكريات الطفولة,\",\n",
      "      \"تحدِّد الطبيبة فاتن جانب التعليم بالمشاركة بتجربة شخصية حول أهمَّة تعلم الأطفال للغة الآباء\"\n",
      "    ],\n",
      "    \"audience_engagement\": [\n",
      "      \"يجذب احمد الجمهور بإشراكهم بسؤاله:\",\n",
      "      \"تشجع فاطمةالمستمعين لتشارك أفكارهم أو أسئتهم المتعلقةبالtopic.\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "polisher = MinimalPolishEnhancer(deployment, model)\n",
    "final_polished_outline = polisher.apply_minimal_polish(topic, information, classification_result, personas_result, result_standard)\n",
    "print(\"Final Polished Outline:\")\n",
    "print(final_polished_outline)\n",
    "# Test the polisher\n",
    "# polished_result = test_minimal_polish_enhancer(\n",
    "#     deployment, topic, information, classification_result, personas_result, result_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd062e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fa49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ae7f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "class ImprovedMicroChunkScriptGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "        \n",
    "        # Simple conversation templates for fallbacks\n",
    "        self.fallback_templates = {\n",
    "            \"intro1\": \"{host_name}: مرحباً بكم مستمعينا الكرام في حلقة جديدة. اليوم سنتحدث عن {topic}. موضوع مهم ومثير للاهتمام.\",\n",
    "            \"intro2\": \"{host_name}: معي اليوم ضيف متميز، {guest_name}. أهلاً وسهلاً بك.\\n{guest_name}: أهلاً بك، شكراً على الاستضافة. سعيد بوجودي معكم.\",\n",
    "            \"discussion\": \"{host_name}: {point_title}، ما رأيك في هذا الموضوع؟\\n{guest_name}: موضوع مهم فعلاً. أعتقد أن هناك عدة جوانب يجب أن نفكر فيها.\",\n",
    "            \"closing\": \"{host_name}: شكراً {guest_name} على هذا النقاش المفيد.\\n{guest_name}: شكراً لك على الاستضافة.\\n{host_name}: وشكراً لكم مستمعينا الكرام. نلقاكم في حلقة قادمة.\"\n",
    "        }\n",
    "\n",
    "    def generate_intro1_only(self, topic, intro1_data, host_persona):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 1: Generate only intro1 (host speaking alone)\n",
    "        Enhanced with natural, conversational tone and spontaneity\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'المقدم')\n",
    "        host_bg = host_persona.get('background', '')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract essential data\n",
    "        opening_line = intro1_data.get('opening_line', '')\n",
    "        episode_hook = intro1_data.get('episode_hook', '')\n",
    "        \n",
    "        # Get one cultural element if available\n",
    "        cultural_elements = intro1_data.get('cultural_connections', [])\n",
    "        cultural_touch = cultural_elements[0] if cultural_elements else \"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate Arabic podcast opening. Host speaks alone.\n",
    "\n",
    "Host: {host_name}\n",
    "Topic: {topic}\n",
    "Opening line: {opening_line}\n",
    "\n",
    "Requirements:\n",
    "- 2-3 short sentences maximum\n",
    "- Natural Arabic conversation\n",
    "- No emojis or symbols\n",
    "- No meta-text or explanations\n",
    "- Include topic naturally\n",
    "\n",
    "Example:\n",
    "{host_name}: مرحباً مستمعينا. موضوع اليوم هو {topic}. شيء مهم نحتاج نتكلم عنه.\n",
    "\n",
    "Generate only the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are creating natural, spontaneous Arabic dialogue. Avoid formal speech patterns. Make it feel like real conversation with natural hesitations and genuine curiosity.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.8\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Quality check\n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_intro1(topic, host_name, opening_line)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating intro1: {e}\")\n",
    "            return self._get_fallback_intro1(topic, host_name, opening_line)\n",
    "\n",
    "    def generate_intro2_only(self, topic, intro2_data, host_persona, guest_persona, intro1_context):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 2: Generate guest introduction with natural, dynamic exchange\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'المقدم')\n",
    "        guest_name = guest_persona.get('name', 'الضيف')\n",
    "        guest_bg = guest_persona.get('background', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Get key topic from context\n",
    "        context = intro1_context[-80:] if len(intro1_context) > 80 else intro1_context\n",
    "        \n",
    "        # Extract data\n",
    "        guest_welcome = intro2_data.get('guest_welcome', '')\n",
    "        guest_bio = intro2_data.get('guest_bio_highlight', '')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate Arabic podcast dialogue. Host introduces guest.\n",
    "\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "Topic: {topic}\n",
    "\n",
    "Requirements:\n",
    "- 4-6 short exchanges\n",
    "- Each person speaks 1-2 sentences maximum\n",
    "- Natural conversation flow\n",
    "- No emojis or symbols\n",
    "- No meta-text or explanations\n",
    "\n",
    "Example:\n",
    "{host_name}: معي اليوم {guest_name}. أهلاً بك.\n",
    "{guest_name}: أهلاً {host_name}. شكراً على الدعوة.\n",
    "{host_name}: نتكلم اليوم عن {topic}. إيش رأيك؟\n",
    "{guest_name}: موضوع مهم فعلاً.\n",
    "\n",
    "Generate only the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate simple Arabic dialogue between two people. Short exchanges only. No emojis. No explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.85\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_intro2(host_name, guest_name, guest_welcome)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating intro2: {e}\")\n",
    "            return self._get_fallback_intro2(host_name, guest_name, guest_welcome)\n",
    "\n",
    "    def generate_discussion_point(self, topic, point_data, host_persona, guest_persona, previous_context=\"\"):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 3-5: Generate discussion with natural disagreements and interruptions\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'المقدم')\n",
    "        guest_name = guest_persona.get('name', 'الضيف')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract content\n",
    "        point_title = point_data.get('point_title', '')\n",
    "        personal_angle = point_data.get('personal_angle', '')\n",
    "        \n",
    "        # Use only first elements to avoid overwhelming\n",
    "        spontaneous_triggers = point_data.get('spontaneous_triggers', [])\n",
    "        cultural_refs = point_data.get('cultural_references', [])\n",
    "        \n",
    "        trigger = spontaneous_triggers[0] if spontaneous_triggers else \"\"\n",
    "        cultural_ref = cultural_refs[0] if cultural_refs else \"\"\n",
    "        \n",
    "        # Get context\n",
    "        context = previous_context[-80:] if len(previous_context) > 80 else previous_context\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate Arabic podcast discussion between host and guest.\n",
    "\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "Discussion Topic: {point_title}\n",
    "\n",
    "Requirements:\n",
    "- 6-8 short exchanges\n",
    "- Each person speaks 1-2 sentences only\n",
    "- Include some disagreement or different views\n",
    "- Natural conversation flow\n",
    "- No emojis or symbols\n",
    "- No meta-text or explanations\n",
    "\n",
    "Example:\n",
    "{host_name}: بالنسبة لـ{point_title}، إيش رأيك؟\n",
    "{guest_name}: موضوع معقد. أعتقد إن...\n",
    "{host_name}: ولكن ما تفكر إن...\n",
    "{guest_name}: لا، مش بالضرورة.\n",
    "\n",
    "Generate only the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate simple Arabic discussion. Short sentences. Include some disagreement. No emojis. No explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.9\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_discussion(point_title, host_name, guest_name, personal_angle)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating discussion point: {e}\")\n",
    "            return self._get_fallback_discussion(point_title, host_name, guest_name, personal_angle)\n",
    "\n",
    "    def generate_closing_only(self, topic, closing_data, host_persona, guest_persona, discussion_summary):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 6: Generate natural closing with honest reflections\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'المقدم')\n",
    "        guest_name = guest_persona.get('name', 'الضيف')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract closing elements\n",
    "        conclusion = closing_data.get('conclusion', {})\n",
    "        outro = closing_data.get('outro', {})\n",
    "        \n",
    "        main_takeaways = conclusion.get('main_takeaways', '')\n",
    "        emotional_closure = conclusion.get('emotional_closure', '')\n",
    "        memorable_ending = outro.get('memorable_ending', '')\n",
    "        \n",
    "        # Get discussion summary\n",
    "        summary = discussion_summary[-100:] if len(discussion_summary) > 100 else discussion_summary\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate Arabic podcast closing dialogue.\n",
    "\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "Topic: {topic}\n",
    "\n",
    "Requirements:\n",
    "- 4-5 short exchanges\n",
    "- Each person speaks 1-2 sentences maximum\n",
    "- Thank each other simply\n",
    "- End naturally\n",
    "- No emojis or symbols\n",
    "- No meta-text or explanations\n",
    "\n",
    "Example:\n",
    "{host_name}: كان نقاش مفيد يا {guest_name}.\n",
    "{guest_name}: شكراً لك على الاستضافة.\n",
    "{host_name}: شكراً لكم مستمعينا.\n",
    "{guest_name}: نلقاكم قريباً.\n",
    "\n",
    "Generate only the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate simple Arabic closing dialogue. Keep it short and natural. No emojis. No explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.75\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_closing(host_name, guest_name, main_takeaways)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating closing: {e}\")\n",
    "            return self._get_fallback_closing(host_name, guest_name, main_takeaways)\n",
    "\n",
    "    def generate_complete_script(self, topic, final_outline_result):\n",
    "        \"\"\"\n",
    "        Main orchestration: Generate complete script using enhanced micro-chunks\n",
    "        \"\"\"\n",
    "        print(\"🎙️ Starting enhanced spontaneous script generation...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            outline = json.loads(final_outline_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid outline JSON format\")\n",
    "        \n",
    "        # Extract data\n",
    "        personas = outline.get(\"personas\", {})\n",
    "        conv_flow = outline.get(\"conversation_flow\", {})\n",
    "        \n",
    "        host_persona = personas.get(\"host\", {})\n",
    "        guest_persona = personas.get(\"guest\", {})\n",
    "        \n",
    "        intro1_data = conv_flow.get(\"intro1\", {})\n",
    "        intro2_data = conv_flow.get(\"intro2\", {})\n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        closing_data = conv_flow.get(\"closing\", {})\n",
    "        \n",
    "        print(f\"📋 Host: {host_persona.get('name', 'Unknown')}\")\n",
    "        print(f\"📋 Guest: {guest_persona.get('name', 'Unknown')}\")\n",
    "        print(f\"📋 Discussion Points: {len(main_discussion)}\")\n",
    "        \n",
    "        # Micro-Chunk 1: Natural Intro1\n",
    "        print(\"\\n📝 Chunk 1: Natural host introduction...\")\n",
    "        intro1_dialogue = self.generate_intro1_only(topic, intro1_data, host_persona)\n",
    "        print(\"✅ Host introduction completed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunk 2: Dynamic Intro2\n",
    "        print(\"\\n📝 Chunk 2: Dynamic guest introduction...\")\n",
    "        intro2_dialogue = self.generate_intro2_only(topic, intro2_data, host_persona, guest_persona, intro1_dialogue)\n",
    "        print(\"✅ Guest introduction completed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunks 3-5: Spontaneous Discussion Points\n",
    "        discussion_parts = []\n",
    "        previous_context = intro2_dialogue\n",
    "        \n",
    "        for i, point_data in enumerate(main_discussion):\n",
    "            print(f\"\\n📝 Chunk {i+3}: Spontaneous discussion point {i+1}...\")\n",
    "            point_dialogue = self.generate_discussion_point(\n",
    "                topic, point_data, host_persona, guest_persona, previous_context\n",
    "            )\n",
    "            discussion_parts.append(point_dialogue)\n",
    "            previous_context = point_dialogue\n",
    "            print(f\"✅ Discussion point {i+1} completed\")\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunk 6: Honest Closing\n",
    "        print(f\"\\n📝 Chunk {len(main_discussion)+3}: Honest closing...\")\n",
    "        discussion_summary = \" \".join(discussion_parts[-2:])\n",
    "        closing_dialogue = self.generate_closing_only(topic, closing_data, host_persona, guest_persona, discussion_summary)\n",
    "        print(\"✅ Closing completed\")\n",
    "        \n",
    "        # Combine all parts\n",
    "        complete_intro = intro1_dialogue + \"\\n\\n\" + intro2_dialogue\n",
    "        complete_discussion = \"\\n\\n\".join(discussion_parts)\n",
    "        \n",
    "        complete_script = f\"\"\"=== مقدمة البودكاست ===\n",
    "{complete_intro}\n",
    "\n",
    "=== النقاش الرئيسي ===\n",
    "{complete_discussion}\n",
    "\n",
    "=== ختام البودكاست ===\n",
    "{closing_dialogue}\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🎉 Enhanced spontaneous script generation completed!\")\n",
    "        \n",
    "        # Enhanced quality assessment\n",
    "        total_quality = self._assess_script_quality(complete_script, outline)\n",
    "        \n",
    "        return {\n",
    "            \"intro\": complete_intro,\n",
    "            \"main_discussion\": complete_discussion,\n",
    "            \"closing\": closing_dialogue,\n",
    "            \"complete_script\": complete_script,\n",
    "            \"script_length\": len(complete_script),\n",
    "            \"estimated_duration\": f\"{len(main_discussion) * 2 + 4}-{len(main_discussion) * 3 + 6} minutes\",\n",
    "            \"quality_score\": total_quality,\n",
    "            \"generation_method\": \"enhanced-spontaneous-micro-chunks\",\n",
    "            \"chunks_generated\": len(main_discussion) + 3,\n",
    "            \"personas_used\": {\n",
    "                \"host\": host_persona.get('name', 'Unknown'),\n",
    "                \"guest\": guest_persona.get('name', 'Unknown')\n",
    "            },\n",
    "            \"cultural_elements_integrated\": self._count_cultural_elements(complete_script),\n",
    "            \"spontaneity_level\": \"high\",\n",
    "            \"natural_interruptions\": self._count_interruptions(complete_script),\n",
    "            \"disagreement_instances\": self._count_disagreements(complete_script),\n",
    "            \"enhancement_level\": \"spontaneous\"\n",
    "        }\n",
    "\n",
    "    def _assess_quality(self, text):\n",
    "        \"\"\"Enhanced quality assessment including spontaneity markers\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return 0\n",
    "            \n",
    "        # Check for meta-text (penalty)\n",
    "        meta_indicators = ['ملاحظة:', 'تنتهي', 'Note:', 'Format:', 'Generate', 'Requirements']\n",
    "        has_meta = any(indicator in text for indicator in meta_indicators)\n",
    "        \n",
    "        # Check for natural conversation markers (bonus)\n",
    "        natural_markers = ['يعني', 'بصراحة', 'أممم', 'لحظة', 'فعلاً؟', 'اعذرني', 'ولكن', 'نعم، ولكن']\n",
    "        natural_score = sum(3 for marker in natural_markers if marker in text)\n",
    "        \n",
    "        # Check for excessive politeness (penalty)\n",
    "        excessive_politeness = ['شكراً جزيلاً', 'أشكرك بالغ الشكر', 'ممتن للغاية', 'شرف عظيم']\n",
    "        politeness_penalty = sum(5 for phrase in excessive_politeness if phrase in text)\n",
    "        \n",
    "        # Check spacing quality\n",
    "        spacing_score = 80 if not re.search(r'[^\\s]{30,}', text) else 40\n",
    "        \n",
    "        # Check Arabic content ratio\n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "        total_chars = len(text)\n",
    "        arabic_ratio = arabic_chars / total_chars if total_chars > 0 else 0\n",
    "        \n",
    "        # Check dialogue structure (more turns = better)\n",
    "        dialogue_turns = text.count(':')\n",
    "        structure_score = min(25, dialogue_turns * 3)  # Reward more turns\n",
    "        \n",
    "        # Calculate quality\n",
    "        quality = spacing_score + (arabic_ratio * 30) + structure_score + natural_score\n",
    "        \n",
    "        # Apply penalties\n",
    "        if has_meta:\n",
    "            quality -= 40\n",
    "        if arabic_ratio < 0.5:\n",
    "            quality -= 20\n",
    "        quality -= politeness_penalty\n",
    "            \n",
    "        return min(100, max(0, int(quality)))\n",
    "\n",
    "    def _count_interruptions(self, script):\n",
    "        \"\"\"Count natural interruption markers\"\"\"\n",
    "        interruption_markers = ['اعذرني', 'لحظة', 'نعم، ولكن', 'لا، لا', 'فعلاً؟']\n",
    "        return sum(script.count(marker) for marker in interruption_markers)\n",
    "\n",
    "    def _count_disagreements(self, script):\n",
    "        \"\"\"Count disagreement/challenge markers\"\"\"\n",
    "        disagreement_markers = ['لا أتفق', 'ولكن', 'هذا صحيح، لكن', 'كيف تفسر', 'ألا تعتقد']\n",
    "        return sum(script.count(marker) for marker in disagreement_markers)\n",
    "\n",
    "    def _assess_script_quality(self, script, outline):\n",
    "        \"\"\"Enhanced script quality assessment with spontaneity metrics\"\"\"\n",
    "        individual_quality = self._assess_quality(script)\n",
    "        \n",
    "        # Check persona usage\n",
    "        personas = outline.get(\"personas\", {})\n",
    "        host_name = personas.get(\"host\", {}).get(\"name\", \"\")\n",
    "        guest_name = personas.get(\"guest\", {}).get(\"name\", \"\")\n",
    "        \n",
    "        persona_score = 0\n",
    "        if host_name and host_name in script:\n",
    "            persona_score += 10\n",
    "        if guest_name and guest_name in script:\n",
    "            persona_score += 10\n",
    "        \n",
    "        # Check structure completeness\n",
    "        required_sections = [\"=== مقدمة البودكاست ===\", \"=== النقاش الرئيسي ===\", \"=== ختام البودكاست ===\"]\n",
    "        structure_score = sum(10 for section in required_sections if section in script)\n",
    "        \n",
    "        # Check dialogue balance (reward more turns)\n",
    "        total_turns = script.count(':')\n",
    "        balance_score = min(25, total_turns * 2)\n",
    "        \n",
    "        # Check spontaneity elements\n",
    "        spontaneity_score = min(15, self._count_interruptions(script) * 2 + self._count_disagreements(script) * 3)\n",
    "        \n",
    "        total_score = min(100, individual_quality + persona_score + structure_score + balance_score + spontaneity_score)\n",
    "        return total_score\n",
    "\n",
    "    def _count_cultural_elements(self, script):\n",
    "        \"\"\"Count cultural elements in the script\"\"\"\n",
    "        cultural_indicators = [\n",
    "            'مثل', 'حكمة', 'تراث', 'ثقافة', 'عربي', 'إسلامي', 'تاريخ',\n",
    "            'شوقي', 'ابن', 'قال', 'حديث', 'قرآن', 'شعر'\n",
    "        ]\n",
    "        return sum(1 for indicator in cultural_indicators if indicator in script)\n",
    "\n",
    "    def _get_fallback_intro1(self, topic, host_name, opening_line=\"\"):\n",
    "        \"\"\"Enhanced fallback with more natural tone\"\"\"\n",
    "        base_opening = opening_line if opening_line else f\"مرحباً مستمعينا\"\n",
    "        return f\"{host_name}: {base_opening}... بصراحة، موضوع {topic} يشغل بالي من فترة. يعني، كلنا نواجه هذا الأمر بشكل أو بآخر، صح؟\"\n",
    "\n",
    "    def _get_fallback_intro2(self, host_name, guest_name, guest_welcome=\"\"):\n",
    "        \"\"\"Enhanced fallback with quick exchanges\"\"\"\n",
    "        return f\"\"\"{host_name}: معي اليوم {guest_name}. أهلاً بك.\n",
    "\n",
    "{guest_name}: أهلاً {host_name}. بصراحة، الموضوع ده محتاج نقاش جدي.\n",
    "\n",
    "{host_name}: فعلاً؟ يعني انت شايف إن...\n",
    "\n",
    "{guest_name}: اعذرني، خليني أوضح وجهة نظري الأول.\"\"\"\n",
    "\n",
    "    def _get_fallback_discussion(self, point_title, host_name, guest_name, personal_angle=\"\"):\n",
    "        \"\"\"Enhanced fallback with disagreement\"\"\"\n",
    "        return f\"\"\"{host_name}: بالنسبة لموضوع {point_title}، إيش رأيك؟\n",
    "\n",
    "{guest_name}: موضوع مهم، بس...\n",
    "\n",
    "{host_name}: لحظة، \"بس\" إيش؟\n",
    "\n",
    "{guest_name}: يعني، الناس تفهم الموضوع غلط أحياناً.\n",
    "\n",
    "{host_name}: ولكن ألا تعتقد أن...\n",
    "\n",
    "{guest_name}: لا، اعذرني، هذا مو صحيح تماماً.\"\"\"\n",
    "\n",
    "    def _get_fallback_closing(self, host_name, guest_name, main_takeaways=\"\"):\n",
    "        \"\"\"Enhanced fallback with honest reflection\"\"\"\n",
    "        return f\"\"\"{host_name}: بصراحة، النقاش كان مثير للجدل شوية.\n",
    "\n",
    "{guest_name}: فعلاً، بس هذا شيء كويس.\n",
    "\n",
    "{host_name}: إيش رأيكم انتوا، مستمعينا؟\n",
    "\n",
    "{guest_name}: والله موضوع يستاهل نقاش أكثر.\n",
    "\n",
    "{host_name}: نلقاكم قريب بإذن الله.\"\"\"\n",
    "\n",
    "    def validate_script_quality(self, script_result):\n",
    "        \"\"\"Enhanced validation including spontaneity metrics\"\"\"\n",
    "        complete_script = script_result.get(\"complete_script\", \"\")\n",
    "        quality_score = script_result.get(\"quality_score\", 0)\n",
    "        \n",
    "        validation = {\n",
    "            \"has_structure\": all(section in complete_script for section in [\"=== مقدمة البودكاست ===\", \"=== النقاش الرئيسي ===\", \"=== ختام البودكاست ===\"]),\n",
    "            \"arabic_content\": bool(re.search(r'[\\u0600-\\u06FF]', complete_script)),\n",
    "            \"no_meta_text\": not any(indicator in complete_script for indicator in ['ملاحظة:', 'تنتهي', 'Note:', 'Format:', 'Generate']),\n",
    "            \"proper_spacing\": not bool(re.search(r'[^\\s]{40,}', complete_script)),\n",
    "            \"dialogue_balance\": complete_script.count(':') >= 12,  # Higher threshold for more turns\n",
    "            \"persona_presence\": script_result.get(\"personas_used\", {}).get(\"host\", \"\") in complete_script,\n",
    "            \"natural_interruptions\": script_result.get(\"natural_interruptions\", 0) >= 2,\n",
    "            \"disagreement_instances\": script_result.get(\"disagreement_instances\", 0) >= 1,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"quality_grade\": \"ممتاز\" if quality_score >= 90 else \"جيد جداً\" if quality_score >= 85 else \"جيد\" if quality_score >= 80 else \"مقبول\" if quality_score >= 70 else \"ضعيف\"\n",
    "        }\n",
    "        \n",
    "        validation[\"overall_valid\"] = all([\n",
    "            validation[\"has_structure\"],\n",
    "            validation[\"arabic_content\"],\n",
    "            validation[\"no_meta_text\"],\n",
    "            validation[\"proper_spacing\"],\n",
    "            validation[\"dialogue_balance\"],\n",
    "            validation[\"persona_presence\"],\n",
    "            quality_score >= 75\n",
    "        ])\n",
    "        \n",
    "        return validation\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_improved_script_generator(deployment, topic, final_outline_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the improved spontaneous micro-chunk script generator\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Improved Spontaneous Script Generator...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = ImprovedMicroChunkScriptGenerator(deployment, model_name)\n",
    "    \n",
    "    # Generate script\n",
    "    script_result = generator.generate_complete_script(topic, final_outline_result)\n",
    "    \n",
    "    # Validate script\n",
    "    validation = generator.validate_script_quality(script_result)\n",
    "    \n",
    "    print(f\"\\n📊 Script Generation Results:\")\n",
    "    print(f\"Quality Score: {script_result['quality_score']}/100\")\n",
    "    print(f\"Quality Grade: {validation['quality_grade']}\")\n",
    "    print(f\"Script Length: {script_result['script_length']:,} characters\")\n",
    "    print(f\"Estimated Duration: {script_result['estimated_duration']}\")\n",
    "    print(f\"Chunks Generated: {script_result['chunks_generated']}\")\n",
    "    print(f\"Spontaneity Level: {script_result['spontaneity_level']}\")\n",
    "    print(f\"Natural Interruptions: {script_result['natural_interruptions']}\")\n",
    "    print(f\"Disagreement Instances: {script_result['disagreement_instances']}\")\n",
    "    \n",
    "    print(f\"\\n📈 Validation Results:\")\n",
    "    print(f\"Overall Valid: {'✅' if validation['overall_valid'] else '❌'}\")\n",
    "    print(f\"Structure: {'✅' if validation['has_structure'] else '❌'}\")\n",
    "    print(f\"Arabic Content: {'✅' if validation['arabic_content'] else '❌'}\")\n",
    "    print(f\"No Meta Text: {'✅' if validation['no_meta_text'] else '❌'}\")\n",
    "    print(f\"Proper Spacing: {'✅' if validation['proper_spacing'] else '❌'}\")\n",
    "    print(f\"Dialogue Balance: {'✅' if validation['dialogue_balance'] else '❌'}\")\n",
    "    print(f\"Natural Interruptions: {'✅' if validation['natural_interruptions'] else '❌'}\")\n",
    "    print(f\"Disagreement Instances: {'✅' if validation['disagreement_instances'] else '❌'}\")\n",
    "    \n",
    "    # Show sample dialogue\n",
    "    print(f\"\\n🎙️ Sample Script Preview:\")\n",
    "    script_lines = script_result['complete_script'].split('\\n')\n",
    "    preview_lines = script_lines[:20]  # More lines to show spontaneity\n",
    "    for line in preview_lines:\n",
    "        if line.strip():\n",
    "            print(f\"  {line[:120]}...\")\n",
    "    \n",
    "    if len(script_lines) > 20:\n",
    "        print(f\"  ... [+{len(script_lines)-20} more lines]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7838c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎙️ Starting enhanced spontaneous script generation...\n",
      "============================================================\n",
      "📋 Host: أحمد بن علي\n",
      "📋 Guest: د. فاتن راشد\n",
      "📋 Discussion Points: 3\n",
      "\n",
      "📝 Chunk 1: Natural host introduction...\n",
      "✅ Host introduction completed\n",
      "\n",
      "📝 Chunk 2: Dynamic guest introduction...\n",
      "✅ Guest introduction completed\n",
      "\n",
      "📝 Chunk 3: Spontaneous discussion point 1...\n",
      "✅ Discussion point 1 completed\n",
      "\n",
      "📝 Chunk 4: Spontaneous discussion point 2...\n",
      "✅ Discussion point 2 completed\n",
      "\n",
      "📝 Chunk 5: Spontaneous discussion point 3...\n",
      "✅ Discussion point 3 completed\n",
      "\n",
      "📝 Chunk 6: Honest closing...\n",
      "✅ Closing completed\n",
      "\n",
      "============================================================\n",
      "🎉 Enhanced spontaneous script generation completed!\n",
      "🧪 Testing Improved Spontaneous Script Generator...\n",
      "============================================================\n",
      "🎙️ Starting enhanced spontaneous script generation...\n",
      "============================================================\n",
      "📋 Host: أحمد بن علي\n",
      "📋 Guest: د. فاتن راشد\n",
      "📋 Discussion Points: 3\n",
      "\n",
      "📝 Chunk 1: Natural host introduction...\n",
      "✅ Host introduction completed\n",
      "\n",
      "📝 Chunk 2: Dynamic guest introduction...\n",
      "✅ Guest introduction completed\n",
      "\n",
      "📝 Chunk 3: Spontaneous discussion point 1...\n",
      "✅ Discussion point 1 completed\n",
      "\n",
      "📝 Chunk 4: Spontaneous discussion point 2...\n",
      "✅ Discussion point 2 completed\n",
      "\n",
      "📝 Chunk 5: Spontaneous discussion point 3...\n",
      "✅ Discussion point 3 completed\n",
      "\n",
      "📝 Chunk 6: Honest closing...\n",
      "✅ Closing completed\n",
      "\n",
      "============================================================\n",
      "🎉 Enhanced spontaneous script generation completed!\n",
      "\n",
      "📊 Script Generation Results:\n",
      "Quality Score: 100/100\n",
      "Quality Grade: ممتاز\n",
      "Script Length: 2,914 characters\n",
      "Estimated Duration: 10-15 minutes\n",
      "Chunks Generated: 6\n",
      "Spontaneity Level: high\n",
      "Natural Interruptions: 0\n",
      "Disagreement Instances: 0\n",
      "\n",
      "📈 Validation Results:\n",
      "Overall Valid: ❌\n",
      "Structure: ✅\n",
      "Arabic Content: ✅\n",
      "No Meta Text: ❌\n",
      "Proper Spacing: ✅\n",
      "Dialogue Balance: ✅\n",
      "Natural Interruptions: ❌\n",
      "Disagreement Instances: ❌\n",
      "\n",
      "🎙️ Sample Script Preview:\n",
      "  === مقدمة البودكاست ===...\n",
      "  أحمد بن علي: السلام عليكم جميعاً، قبل أن يمتزج عالم البتات والأصفار بحياتنا اليومية بشكل لا رجعة فيه، دعونا نخوض معاً رح...\n",
      "  أحمد بن علي: مرحباً، نلتقي هنا مع الدكتورة فاتن راشد لمناقشة دور الذكاء الاصطناعي وبقاء هويتنا العربية. ما أولى ملاحظاتك...\n",
      "  د. فاتن راشد: ينبغي أن نستفيد من التكنولوجيا دون فقدان جذورنا، وهذا تحدٍ مثير للاهتمام....\n",
      "  أحمد بن علي: وكيف يمكننا تحقيق ذلك بالتحدُّث تحديدًا عن المحتوى التعليمي والثقافي العربي عبر الإنترنت؟...\n",
      "  د. فاتن راشد: الاستثمار في إنشاء محتوى رقمي عربي أصيل ومشاركة المعرفة بين الأجيال هما مفتاح فتح الطريق أمام المستقبل واح...\n",
      "  === النقاش الرئيسي ===...\n",
      "  **أحمد:** بدءاً من بداية الموضوع، كيف تعتبرين دوره الرئيسي هنا?...\n",
      "  **فاتن:** أمرٌ يحتاجُ لتجردٍ عميق لكنني أرى أن القيادة العليا decisive فيها....\n",
      "  **أحمد:** صحيح, لكن ماذا عن دور الأفراد في تحديد المسار الأساسي له?...\n",
      "  **فاتن:** ينفع للأفراد البدء بالتغيير لكنالقرار الحقيقي يرجع للقادة كما أشرت لأول مرة....\n",
      "  **أحمد:** رغم ذلك البعض يقول أنه يعتمد بشكل أكبر على جهود فردية.....\n",
      "  **فاتن:** هذا نضره بسيطة جداً. يتطلب جهد الجميع ضمن استراتيجية شاملة واضحة....\n",
      "  ... [+34 more lines]\n",
      "Quality: 100/100\n",
      "Duration: 10-15 minutes\n",
      "Cultural Elements: 1\n",
      "Personas: {'host': 'أحمد بن علي', 'guest': 'د. فاتن راشد'}\n"
     ]
    }
   ],
   "source": [
    "# Generate enhanced script\n",
    "generator = ImprovedMicroChunkScriptGenerator(deployment, model)\n",
    "script_result = generator.generate_complete_script(topic, final_polished_outline)\n",
    "\n",
    "# Test and validate\n",
    "test_result = test_improved_script_generator(deployment, topic, final_polished_outline)\n",
    "\n",
    "# Access enhanced results\n",
    "print(f\"Quality: {script_result['quality_score']}/100\")\n",
    "print(f\"Duration: {script_result['estimated_duration']}\")\n",
    "print(f\"Cultural Elements: {script_result['cultural_elements_integrated']}\")\n",
    "print(f\"Personas: {script_result['personas_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f421dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intro': 'أحمد بن علي: سلام عليكم جميعاً، ونرحب بكم في هذه الحلقة التي نغوص فيها بعمق لتفهم التوازي الحساس بين ثورتِ الذكاء الصناعي وثروتِ الهوية العربية الواعدة؛ دعونا نتشارك باهتمام نسبر فيه آفاق هذا المزج الفريد لمستقبلنا الثقافي المحلي.\\n\\n**أحمد بن علي:** ترحيب للدكتورة فاتن Рашид، من الرائع أنّها معنا اليوم لمناقشة مسألة حيوية حول الترابط بين تقنية الذكاء الاصطناعي وهویتَنا العربيّة.\\n\\n**د. فَتْن رَاشِد:** شكر لك، نبحث بالفعل تحديات مثيرة تتعلق بهويتنا التقليدية في عالم مترابط بشكل رقمي.\\n\\n**أحمد بن علي:** كيف بإمكاننا ضمّ تقنيّات المستقبل لدعم قيمنا ومعتقداتنا بدلاً من تهديد وجودها?\\n\\n**د. فَتْن رَاشِد:** يجب أن نركز على تطوير حلول ذكية تستلهم جذورها القيميّة الثقافيّة بينما ترنو إلى مستقبِل متنوِّر تكنولوجيًا وقائم علَى أسس أخلاقيِّة صلبة.\\n\\n**أحمد بن علي:** اقتراح ممتاز! هل يمكن ذكر مثال عملي لإدارة هذا الانصهار الناجح نحو آفاق واعدة للعالم العربِي؟\\n\\n**د. فَتْن رَاشِد:** نعم، علينا تشكيل نماذج التعليم الداخيلة المحترمة لروافدنا الفكريَّــة وتوظيف وسائل التواصل الرَّاقمية كمنبر للتواصل بقضايا مجتمعيَّة ذات مغزى وفائدة مشتركة.', 'main_discussion': '**أحمد بن علي:** بدايةً, للنظام الغذائي دور رئيسي لكن هل تعلم أنه قد يُهمل البعض أهمية الرياضة أيضاً؟\\n\\n**د. فاتن راشد:** صحيح, التدريب البدني مهم لكن التغذية الصحية هي الأساس عند بدء أي برنامج صحي.\\n\\n**أحمد بن علي:** ولكن يمكن للرياضة أن تعوض بعض قصور النظام الغذائي بمحتواها من السعرات الحرارية المحروقة.\\n\\n**د. فاتن راشد:** هذا صحيح ولكن الأفضل هو موازنة الاثنين معًا؛ ليس فقط التعويض عن نقص واحد بالتكبير في الآخر.\\n\\n**أحمد بن علي:** ربما، ولكثير من الناس هدف لتحسين الحالة الذهنية أولاً، وممكن الجمباز أو اليوغا يفيد لذلك أكثر!\\n\\n**د. فاتن راشد:** وعلى الرغم من ذلك فإن ممارسة نوع رياضي تحبينه واجهزة جسمك أفضل بكثير لدعم الصحة العامة والعقلانية أيضًا.\\nالثقة: 95%</strong>\\n\\nأحمد بن علي: في الجانب الثاني من الموضوع، كيف ترى دور التقنية فيها؟\\nد. فاتن راشد: تقف حجر عثرة كثيرة حسب تجاربي.\\nأحمد بن علي: لكن البعض يقول إنها فرصة للنمو.\\nد. فاتن راشد: نعم ربما، لكنها قد تخلق أيضًا تحديات جديدة.\\nأحمد بن علي: صحيح بشأن التحديات الجديدة، ولكن هل تعلم أنه هناك الذين يستفيدون بشكل كبير منها رغم ذلك?\\nد. فاتن راشد: هذا يعتمد على ظروف كل شخص بالتأكيد, ولكن الجهد يبقى ضروريًا بغض النظر.\\nأحمد بن علي: بلا شك, التعامل الإستراتيجي هو المفتاح thenهيمًا.\\n\\n**أحمد:** حول الحلول المقترحة لهذا الجانب, كيف تراهِ \"فاتن\" ؟\\n\\n**فاتن:** تحديات كثيرة تحتاج دراسة متأنية قبل تقديم حلول.\\n\\n**أحمد:** لكن البعض يدعو للتغيير الفوري أليس كذلك?\\n\\n**فاتن:** الإصلاح يحتاج وقت ولا يمكن الاستعجال فيه.\\n\\n**أحمد:** ربما نقترح خطوات سريعة فعالة عوضاً عن التأجيل .\\n\\n**فاتن:** قد تكون الخطوات السريعة مؤقتة ولكنها يجب أن تستند إلى دراسات دقيقة أولًا .\\n\\n**أحمد:** إذن توافقين بأن الدراسات ضرورية قبلهم كلاهما!\\n\\n**فاتن:** تمامًا ،لكن ليس عذر لتجنب الحركة نحو الحل.', 'closing': 'أحمد: شكرًا دكتورة فاتن، كانت محادثة غنية بالتجارب.\\nفاتن: من الرائع دائماً تبادل الأفكار معكم، أشكركم.\\nأحمد: وشكراً لمستمعينا العزيزين， سنلتقي مجددًا.\\nفاتن: إلى اللقاء ونراكم مرة أخرى!', 'complete_script': '=== مقدمة البودكاست ===\\nأحمد بن علي: سلام عليكم جميعاً، ونرحب بكم في هذه الحلقة التي نغوص فيها بعمق لتفهم التوازي الحساس بين ثورتِ الذكاء الصناعي وثروتِ الهوية العربية الواعدة؛ دعونا نتشارك باهتمام نسبر فيه آفاق هذا المزج الفريد لمستقبلنا الثقافي المحلي.\\n\\n**أحمد بن علي:** ترحيب للدكتورة فاتن Рашид، من الرائع أنّها معنا اليوم لمناقشة مسألة حيوية حول الترابط بين تقنية الذكاء الاصطناعي وهویتَنا العربيّة.\\n\\n**د. فَتْن رَاشِد:** شكر لك، نبحث بالفعل تحديات مثيرة تتعلق بهويتنا التقليدية في عالم مترابط بشكل رقمي.\\n\\n**أحمد بن علي:** كيف بإمكاننا ضمّ تقنيّات المستقبل لدعم قيمنا ومعتقداتنا بدلاً من تهديد وجودها?\\n\\n**د. فَتْن رَاشِد:** يجب أن نركز على تطوير حلول ذكية تستلهم جذورها القيميّة الثقافيّة بينما ترنو إلى مستقبِل متنوِّر تكنولوجيًا وقائم علَى أسس أخلاقيِّة صلبة.\\n\\n**أحمد بن علي:** اقتراح ممتاز! هل يمكن ذكر مثال عملي لإدارة هذا الانصهار الناجح نحو آفاق واعدة للعالم العربِي؟\\n\\n**د. فَتْن رَاشِد:** نعم، علينا تشكيل نماذج التعليم الداخيلة المحترمة لروافدنا الفكريَّــة وتوظيف وسائل التواصل الرَّاقمية كمنبر للتواصل بقضايا مجتمعيَّة ذات مغزى وفائدة مشتركة.\\n\\n=== النقاش الرئيسي ===\\n**أحمد بن علي:** بدايةً, للنظام الغذائي دور رئيسي لكن هل تعلم أنه قد يُهمل البعض أهمية الرياضة أيضاً؟\\n\\n**د. فاتن راشد:** صحيح, التدريب البدني مهم لكن التغذية الصحية هي الأساس عند بدء أي برنامج صحي.\\n\\n**أحمد بن علي:** ولكن يمكن للرياضة أن تعوض بعض قصور النظام الغذائي بمحتواها من السعرات الحرارية المحروقة.\\n\\n**د. فاتن راشد:** هذا صحيح ولكن الأفضل هو موازنة الاثنين معًا؛ ليس فقط التعويض عن نقص واحد بالتكبير في الآخر.\\n\\n**أحمد بن علي:** ربما، ولكثير من الناس هدف لتحسين الحالة الذهنية أولاً، وممكن الجمباز أو اليوغا يفيد لذلك أكثر!\\n\\n**د. فاتن راشد:** وعلى الرغم من ذلك فإن ممارسة نوع رياضي تحبينه واجهزة جسمك أفضل بكثير لدعم الصحة العامة والعقلانية أيضًا.\\nالثقة: 95%</strong>\\n\\nأحمد بن علي: في الجانب الثاني من الموضوع، كيف ترى دور التقنية فيها؟\\nد. فاتن راشد: تقف حجر عثرة كثيرة حسب تجاربي.\\nأحمد بن علي: لكن البعض يقول إنها فرصة للنمو.\\nد. فاتن راشد: نعم ربما، لكنها قد تخلق أيضًا تحديات جديدة.\\nأحمد بن علي: صحيح بشأن التحديات الجديدة، ولكن هل تعلم أنه هناك الذين يستفيدون بشكل كبير منها رغم ذلك?\\nد. فاتن راشد: هذا يعتمد على ظروف كل شخص بالتأكيد, ولكن الجهد يبقى ضروريًا بغض النظر.\\nأحمد بن علي: بلا شك, التعامل الإستراتيجي هو المفتاح thenهيمًا.\\n\\n**أحمد:** حول الحلول المقترحة لهذا الجانب, كيف تراهِ \"فاتن\" ؟\\n\\n**فاتن:** تحديات كثيرة تحتاج دراسة متأنية قبل تقديم حلول.\\n\\n**أحمد:** لكن البعض يدعو للتغيير الفوري أليس كذلك?\\n\\n**فاتن:** الإصلاح يحتاج وقت ولا يمكن الاستعجال فيه.\\n\\n**أحمد:** ربما نقترح خطوات سريعة فعالة عوضاً عن التأجيل .\\n\\n**فاتن:** قد تكون الخطوات السريعة مؤقتة ولكنها يجب أن تستند إلى دراسات دقيقة أولًا .\\n\\n**أحمد:** إذن توافقين بأن الدراسات ضرورية قبلهم كلاهما!\\n\\n**فاتن:** تمامًا ،لكن ليس عذر لتجنب الحركة نحو الحل.\\n\\n=== ختام البودكاست ===\\nأحمد: شكرًا دكتورة فاتن، كانت محادثة غنية بالتجارب.\\nفاتن: من الرائع دائماً تبادل الأفكار معكم، أشكركم.\\nأحمد: وشكراً لمستمعينا العزيزين， سنلتقي مجددًا.\\nفاتن: إلى اللقاء ونراكم مرة أخرى!', 'script_length': 2913, 'estimated_duration': '10-15 minutes', 'quality_score': 100, 'generation_method': 'enhanced-spontaneous-micro-chunks', 'chunks_generated': 6, 'personas_used': {'host': 'أحمد بن علي', 'guest': 'د. فاتن راشد'}, 'cultural_elements_integrated': 1, 'spontaneity_level': 'high', 'natural_interruptions': 0, 'disagreement_instances': 5, 'enhancement_level': 'spontaneous'}\n"
     ]
    }
   ],
   "source": [
    "print(script_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47ec2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Basic cleaning\\ncleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\\ncleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\\n\\n# Testing with detailed output\\ntest_result = test_micro_chunking_cleaner(deployment, corrupted_script_result)\\n\\n# Detailed pre-processing analysis\\nanalysis = detailed_micro_chunk_analysis(deployment, corrupted_script_result)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class MicroChunkingAIScriptCleaner:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def clean_script_with_ai(self, script_result):\n",
    "        \"\"\"\n",
    "        Micro-chunking approach: Clean script using surgical AI corrections\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"🔬 MICRO-CHUNKING AI SCRIPT CLEANER\".center(80, \"=\"))\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # Extract complete script text\n",
    "            if isinstance(script_result, dict):\n",
    "                complete_script = script_result.get('complete_script', '')\n",
    "            else:\n",
    "                complete_script = str(script_result)\n",
    "            \n",
    "            if not complete_script or len(complete_script.strip()) < 50:\n",
    "                print(\"❌ Script too short or empty, using fallback\")\n",
    "                return self._generate_complete_fallback()\n",
    "            \n",
    "            original_length = len(complete_script)\n",
    "            print(f\"📏 Original script length: {original_length:,} characters\")\n",
    "            \n",
    "            # Step 1: Analyze script corruption\n",
    "            print(\"🔍 CORRUPTION ANALYSIS\".center(60, \"-\"))\n",
    "            corruption_analysis = self.analyze_corruption_patterns(complete_script)\n",
    "            self._print_corruption_summary(corruption_analysis)\n",
    "            \n",
    "            # Step 2: Create micro-chunks (small, focused chunks)\n",
    "            print(\"\\n📝 MICRO-CHUNKING STRATEGY\".center(60, \"-\"))\n",
    "            micro_chunks = self.create_micro_chunks(complete_script)\n",
    "            print(f\"    Created {len(micro_chunks)} micro-chunks (avg: {original_length//len(micro_chunks)} chars each)\")\n",
    "            \n",
    "            # Step 3: Process each micro-chunk with surgical precision\n",
    "            print(\"\\n🔬 SURGICAL CLEANING PROCESS\".center(60, \"-\"))\n",
    "            cleaned_chunks = []\n",
    "            \n",
    "            for i, chunk_data in enumerate(micro_chunks):\n",
    "                print(f\"    Processing micro-chunk {i+1}/{len(micro_chunks)}... \", end=\"\")\n",
    "                \n",
    "                # Quick corruption assessment\n",
    "                corruption_level = self.assess_chunk_corruption(chunk_data['content'])\n",
    "                \n",
    "                if corruption_level == 'clean':\n",
    "                    # Keep as-is\n",
    "                    cleaned_chunks.append(chunk_data['content'])\n",
    "                    print(\"✅ CLEAN (kept as-is)\")\n",
    "                elif corruption_level == 'minor':\n",
    "                    # Light cleaning with regex\n",
    "                    cleaned_chunk = self.light_clean_chunk(chunk_data['content'])\n",
    "                    cleaned_chunks.append(cleaned_chunk)\n",
    "                    print(\"🟡 LIGHT CLEAN\")\n",
    "                else:\n",
    "                    # AI-powered surgical correction\n",
    "                    cleaned_chunk = self.surgical_ai_correction(\n",
    "                        chunk_data['content'], \n",
    "                        chunk_data['context'],\n",
    "                        corruption_analysis\n",
    "                    )\n",
    "                    \n",
    "                    # Validate result\n",
    "                    if self.validate_micro_chunk(cleaned_chunk, chunk_data['content']):\n",
    "                        cleaned_chunks.append(cleaned_chunk)\n",
    "                        print(\"🔧 AI CORRECTED\")\n",
    "                    else:\n",
    "                        # Fallback to light cleaning\n",
    "                        fallback_chunk = self.light_clean_chunk(chunk_data['content'])\n",
    "                        cleaned_chunks.append(fallback_chunk)\n",
    "                        print(\"🔄 FALLBACK CLEAN\")\n",
    "            \n",
    "            # Step 4: Reassemble with structure preservation\n",
    "            print(\"\\n🔧 REASSEMBLY WITH STRUCTURE CHECK\".center(60, \"-\"))\n",
    "            final_script = self.intelligent_reassembly(cleaned_chunks, micro_chunks, complete_script)\n",
    "            \n",
    "            # Step 5: Final quality and length check\n",
    "            final_length = len(final_script)\n",
    "            length_ratio = final_length / original_length if original_length > 0 else 0\n",
    "            \n",
    "            print(f\"    Original structure preserved: {'✅' if self.verify_structure_preservation(complete_script, final_script) else '⚠️'}\")\n",
    "            print(f\"    Length preservation: {length_ratio:.1%}\")\n",
    "            \n",
    "            # Step 6: Light expansion if needed (without AI)\n",
    "            if length_ratio < 0.90:  # Less than 90% retained\n",
    "                print(\"📈 APPLYING LENGTH RECOVERY (NON-AI)\".center(60, \"-\"))\n",
    "                final_script = self.non_ai_length_recovery(final_script, original_length)\n",
    "                final_length = len(final_script)\n",
    "                length_ratio = final_length / original_length\n",
    "            \n",
    "            # Final validation\n",
    "            final_valid = self.validate_final_script(final_script)\n",
    "            \n",
    "            print(\"\\n\" + \"🎉 MICRO-CHUNKING SUMMARY\".center(60, \"-\"))\n",
    "            print(f\"    Status: {'SUCCESS' if final_valid else 'PARTIAL SUCCESS'}\")\n",
    "            print(f\"    Original Length: {original_length:,} characters\")\n",
    "            print(f\"    Final Length: {final_length:,} characters\")\n",
    "            print(f\"    Length Preserved: {length_ratio:.1%}\")\n",
    "            print(f\"    Micro-chunks Processed: {len(micro_chunks)}\")\n",
    "            print(f\"    Arabic Quality: {'✅ VALID' if final_valid else '⚠️ NEEDS REVIEW'}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            return {\n",
    "                'complete_script': final_script,\n",
    "                'cleaning_method': 'micro_chunking_surgical',\n",
    "                'micro_chunks_processed': len(micro_chunks),\n",
    "                'cleaning_status': 'success' if final_valid else 'partial',\n",
    "                'script_length': final_length,\n",
    "                'original_length': original_length,\n",
    "                'length_ratio': length_ratio,\n",
    "                'corruption_analysis': corruption_analysis,\n",
    "                'estimated_duration': self._estimate_duration(final_script)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ CRITICAL ERROR: {e}\")\n",
    "            print(\"🔄 Using complete fallback script...\")\n",
    "            return self._generate_complete_fallback()\n",
    "\n",
    "    def analyze_corruption_patterns(self, script_text):\n",
    "        \"\"\"\n",
    "        Analyze what specific types of corruption exist in the script\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'total_chars': len(script_text),\n",
    "            'foreign_patterns': {},\n",
    "            'encoding_issues': 0,\n",
    "            'concatenated_words': 0,\n",
    "            'structural_issues': 0,\n",
    "            'overall_corruption_level': 'clean'\n",
    "        }\n",
    "        \n",
    "        # Foreign language patterns\n",
    "        foreign_patterns = {\n",
    "            'english_words': (r'\\b[A-Za-z]{3,}\\b', 'English words (3+ letters)'),\n",
    "            'chinese_chars': (r'[\\u4e00-\\u9fff]', 'Chinese characters'),\n",
    "            'hebrew_chars': (r'[\\u0590-\\u05ff]', 'Hebrew characters'),\n",
    "            'japanese_chars': (r'[\\u3040-\\u309f\\u30a0-\\u30ff]', 'Japanese characters'),\n",
    "            'problematic_punct': (r'[、！]', 'Problematic punctuation')\n",
    "        }\n",
    "        \n",
    "        total_foreign_chars = 0\n",
    "        for pattern_name, (pattern, description) in foreign_patterns.items():\n",
    "            matches = re.findall(pattern, script_text)\n",
    "            if matches:\n",
    "                char_count = sum(len(match) for match in matches)\n",
    "                analysis['foreign_patterns'][pattern_name] = {\n",
    "                    'count': len(matches),\n",
    "                    'chars': char_count,\n",
    "                    'description': description,\n",
    "                    'examples': matches[:3]  # First 3 examples\n",
    "                }\n",
    "                total_foreign_chars += char_count\n",
    "        \n",
    "        # Concatenated words (Arabic words without spaces)\n",
    "        concatenated_matches = re.findall(r'[\\u0600-\\u06FF]{40,}', script_text)  # 40+ Arabic chars without spaces\n",
    "        analysis['concatenated_words'] = len(concatenated_matches)\n",
    "        \n",
    "        # Encoding issues (mixed scripts in single words)\n",
    "        encoding_issues = re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]+[\\u0600-\\u06FF]+|[\\u0600-\\u06FF]+[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]+', script_text)\n",
    "        analysis['encoding_issues'] = len(encoding_issues)\n",
    "        \n",
    "        # Structural issues\n",
    "        if '***' in script_text or '**' in script_text:\n",
    "            analysis['structural_issues'] += script_text.count('***') + script_text.count('**')\n",
    "        \n",
    "        # Overall corruption level\n",
    "        foreign_ratio = total_foreign_chars / len(script_text) if script_text else 0\n",
    "        if foreign_ratio > 0.15 or analysis['concatenated_words'] > 5:\n",
    "            analysis['overall_corruption_level'] = 'heavy'\n",
    "        elif foreign_ratio > 0.05 or analysis['concatenated_words'] > 2:\n",
    "            analysis['overall_corruption_level'] = 'moderate'\n",
    "        elif foreign_ratio > 0.01 or analysis['encoding_issues'] > 0:\n",
    "            analysis['overall_corruption_level'] = 'light'\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _print_corruption_summary(self, analysis):\n",
    "        \"\"\"Print corruption analysis summary\"\"\"\n",
    "        print(f\"    Overall corruption level: {analysis['overall_corruption_level'].upper()}\")\n",
    "        print(f\"    Foreign patterns found: {len(analysis['foreign_patterns'])}\")\n",
    "        \n",
    "        for pattern_name, details in analysis['foreign_patterns'].items():\n",
    "            print(f\"      - {details['description']}: {details['count']} instances\")\n",
    "            if details['examples']:\n",
    "                examples_str = ', '.join(str(ex) for ex in details['examples'][:2])\n",
    "                print(f\"        Examples: {examples_str}\")\n",
    "        \n",
    "        if analysis['concatenated_words'] > 0:\n",
    "            print(f\"    Concatenated word sequences: {analysis['concatenated_words']}\")\n",
    "        if analysis['encoding_issues'] > 0:\n",
    "            print(f\"    Mixed encoding issues: {analysis['encoding_issues']}\")\n",
    "\n",
    "    def create_micro_chunks(self, script_text):\n",
    "        \"\"\"\n",
    "        Create small, focused micro-chunks (200-400 characters each)\n",
    "        \"\"\"\n",
    "        micro_chunks = []\n",
    "        \n",
    "        # Split by sections first\n",
    "        sections = self._identify_script_sections(script_text)\n",
    "        \n",
    "        for section_name, section_content in sections.items():\n",
    "            if not section_content.strip():\n",
    "                continue\n",
    "            \n",
    "            # Split section into dialogue exchanges\n",
    "            dialogue_chunks = self._split_into_dialogue_exchanges(section_content, section_name)\n",
    "            micro_chunks.extend(dialogue_chunks)\n",
    "        \n",
    "        return micro_chunks\n",
    "\n",
    "    def _identify_script_sections(self, script_text):\n",
    "        \"\"\"Identify main sections of the script\"\"\"\n",
    "        sections = {}\n",
    "        \n",
    "        if \"=== مقدمة البودكاست ===\" in script_text:\n",
    "            # Structured script\n",
    "            parts = script_text.split(\"=== مقدمة البودكاست ===\")\n",
    "            if len(parts) > 1:\n",
    "                after_intro = parts[1]\n",
    "                if \"=== النقاش الرئيسي ===\" in after_intro:\n",
    "                    intro_parts = after_intro.split(\"=== النقاش الرئيسي ===\")\n",
    "                    sections['intro'] = intro_parts[0].strip()\n",
    "                    if len(intro_parts) > 1:\n",
    "                        after_main = intro_parts[1]\n",
    "                        if \"=== ختام البودكاست ===\" in after_main:\n",
    "                            main_parts = after_main.split(\"=== ختام البودكاست ===\")\n",
    "                            sections['main_discussion'] = main_parts[0].strip()\n",
    "                            if len(main_parts) > 1:\n",
    "                                sections['closing'] = main_parts[1].strip()\n",
    "                        else:\n",
    "                            sections['main_discussion'] = after_main.strip()\n",
    "                else:\n",
    "                    sections['intro'] = after_intro.strip()\n",
    "        else:\n",
    "            # Unstructured script\n",
    "            sections['full_script'] = script_text\n",
    "        \n",
    "        return sections\n",
    "\n",
    "    def _split_into_dialogue_exchanges(self, section_content, section_name):\n",
    "        \"\"\"Split section into small dialogue exchanges\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split by speaker turns\n",
    "        lines = section_content.split('\\n')\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        target_chunk_size = 300  # Target 300 characters per micro-chunk\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a speaker line\n",
    "            is_speaker_line = ':' in line and not line.startswith('===')\n",
    "            \n",
    "            # If adding this line would exceed target size, finalize current chunk\n",
    "            if current_length + len(line) > target_chunk_size and current_chunk and is_speaker_line:\n",
    "                chunk_content = '\\n'.join(current_chunk).strip()\n",
    "                if chunk_content:\n",
    "                    chunks.append({\n",
    "                        'content': chunk_content,\n",
    "                        'context': f'{section_name}_dialogue_exchange',\n",
    "                        'type': 'dialogue_exchange',\n",
    "                        'length': len(chunk_content)\n",
    "                    })\n",
    "                current_chunk = [line]\n",
    "                current_length = len(line)\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "                current_length += len(line) + 1  # +1 for newline\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_content = '\\n'.join(current_chunk).strip()\n",
    "            if chunk_content:\n",
    "                chunks.append({\n",
    "                    'content': chunk_content,\n",
    "                    'context': f'{section_name}_dialogue_exchange',\n",
    "                    'type': 'dialogue_exchange',\n",
    "                    'length': len(chunk_content)\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def assess_chunk_corruption(self, chunk_content):\n",
    "        \"\"\"Quick assessment of chunk corruption level\"\"\"\n",
    "        if not chunk_content or len(chunk_content.strip()) < 10:\n",
    "            return 'heavy'\n",
    "        \n",
    "        # Check for foreign characters\n",
    "        foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff\\u3040-\\u309f\\u30a0-\\u30ff、！]', chunk_content))\n",
    "        total_chars = len(chunk_content)\n",
    "        \n",
    "        # Check for concatenated words\n",
    "        concatenated = len(re.findall(r'[\\u0600-\\u06FF]{30,}', chunk_content))\n",
    "        \n",
    "        # Check for encoding issues\n",
    "        encoding_issues = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff]+[\\u0600-\\u06FF]+|[\\u0600-\\u06FF]+[A-Za-z\\u4e00-\\u9fff]+', chunk_content))\n",
    "        \n",
    "        if foreign_chars == 0 and concatenated == 0 and encoding_issues == 0:\n",
    "            return 'clean'\n",
    "        elif foreign_chars < 3 and concatenated == 0 and encoding_issues == 0:\n",
    "            return 'minor'\n",
    "        else:\n",
    "            return 'heavy'\n",
    "\n",
    "    def light_clean_chunk(self, chunk_content):\n",
    "        \"\"\"Light cleaning using regex only\"\"\"\n",
    "        cleaned = chunk_content\n",
    "        \n",
    "        # Remove specific problematic punctuation\n",
    "        cleaned = re.sub(r'[、！]', '', cleaned)\n",
    "        \n",
    "        # Remove short English words (1-2 letters)\n",
    "        cleaned = re.sub(r'\\b[A-Za-z]{1,2}\\b', '', cleaned)\n",
    "        \n",
    "        # Remove Chinese and Hebrew characters\n",
    "        cleaned = re.sub(r'[\\u4e00-\\u9fff\\u0590-\\u05ff\\u3040-\\u309f\\u30a0-\\u30ff]', '', cleaned)\n",
    "        \n",
    "        # Fix spacing issues\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        cleaned = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned)\n",
    "        \n",
    "        # Clean up markdown formatting\n",
    "        cleaned = re.sub(r'\\*{2,}', '', cleaned)\n",
    "        \n",
    "        return cleaned.strip()\n",
    "\n",
    "    def surgical_ai_correction(self, chunk_content, context, corruption_analysis):\n",
    "        \"\"\"\n",
    "        Surgical AI correction focused on specific problems\n",
    "        \"\"\"\n",
    "        # Identify specific issues in this chunk\n",
    "        chunk_issues = []\n",
    "        \n",
    "        if re.search(r'[A-Za-z]{3,}', chunk_content):\n",
    "            chunk_issues.append(\"English words\")\n",
    "        if re.search(r'[\\u4e00-\\u9fff]', chunk_content):\n",
    "            chunk_issues.append(\"Chinese characters\")\n",
    "        if re.search(r'[\\u0590-\\u05ff]', chunk_content):\n",
    "            chunk_issues.append(\"Hebrew characters\")\n",
    "        if re.search(r'[\\u0600-\\u06FF]{30,}', chunk_content):\n",
    "            chunk_issues.append(\"concatenated words\")\n",
    "        \n",
    "        issues_description = \", \".join(chunk_issues) if chunk_issues else \"minor formatting issues\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an Arabic text correction specialist. Fix ONLY the specific issues in this small text chunk.\n",
    "\n",
    "CHUNK CONTEXT: {context}\n",
    "ISSUES TO FIX: {issues_description}\n",
    "CHUNK LENGTH: {len(chunk_content)} characters\n",
    "\n",
    "ORIGINAL CHUNK:\n",
    "{chunk_content}\n",
    "\n",
    "CORRECTION INSTRUCTIONS:\n",
    "1. Replace English words with appropriate Arabic equivalents in context\n",
    "2. Remove Chinese/Hebrew characters and replace with contextually appropriate Arabic\n",
    "3. Fix concatenated Arabic words by adding proper spaces\n",
    "4. Keep the EXACT same meaning and dialogue structure\n",
    "5. Maintain or slightly increase the length\n",
    "6. Do NOT change speaker names or dialogue flow\n",
    "\n",
    "CRITICAL: Return ONLY the corrected Arabic text. No explanations or comments.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an Arabic text correction specialist. Make minimal, precise corrections while preserving meaning and length.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,  # Low temperature for consistent corrections\n",
    "                max_tokens=1000   # Limit tokens to prevent over-expansion\n",
    "            )\n",
    "            \n",
    "            corrected = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Basic cleanup\n",
    "            corrected = self.light_clean_chunk(corrected)\n",
    "            \n",
    "            return corrected\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" (AI failed: {e}) \", end=\"\")\n",
    "            return self.light_clean_chunk(chunk_content)\n",
    "\n",
    "    def validate_micro_chunk(self, cleaned_chunk, original_chunk):\n",
    "        \"\"\"\n",
    "        Validate that micro-chunk correction was successful\n",
    "        \"\"\"\n",
    "        if not cleaned_chunk or len(cleaned_chunk.strip()) < 10:\n",
    "            return False\n",
    "        \n",
    "        # Check that length wasn't reduced too much\n",
    "        length_ratio = len(cleaned_chunk) / len(original_chunk) if original_chunk else 0\n",
    "        if length_ratio < 0.7:  # Lost more than 30% of content\n",
    "            return False\n",
    "        \n",
    "        # Check for remaining major foreign content\n",
    "        major_foreign = re.findall(r'[\\u4e00-\\u9fff\\u0590-\\u05ff]{2,}|[A-Za-z]{4,}', cleaned_chunk)\n",
    "        if len(major_foreign) > 1:  # Allow 1 instance (might be technical term)\n",
    "            return False\n",
    "        \n",
    "        # Check for basic dialogue structure if original had it\n",
    "        if ':' in original_chunk and ':' not in cleaned_chunk:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def intelligent_reassembly(self, cleaned_chunks, original_micro_chunks, original_script):\n",
    "        \"\"\"\n",
    "        Intelligently reassemble micro-chunks back into complete script\n",
    "        \"\"\"\n",
    "        # Group chunks back into sections\n",
    "        sections = {'intro': [], 'main_discussion': [], 'closing': [], 'other': []}\n",
    "        \n",
    "        for i, chunk in enumerate(cleaned_chunks):\n",
    "            if not chunk.strip():\n",
    "                continue\n",
    "            \n",
    "            context = original_micro_chunks[i]['context'] if i < len(original_micro_chunks) else 'other'\n",
    "            \n",
    "            if 'intro' in context:\n",
    "                sections['intro'].append(chunk)\n",
    "            elif 'main_discussion' in context:\n",
    "                sections['main_discussion'].append(chunk)\n",
    "            elif 'closing' in context:\n",
    "                sections['closing'].append(chunk)\n",
    "            else:\n",
    "                sections['other'].append(chunk)\n",
    "        \n",
    "        # Reassemble with proper structure\n",
    "        script_parts = []\n",
    "        \n",
    "        # Add intro section\n",
    "        if sections['intro']:\n",
    "            script_parts.append(\"=== مقدمة البودكاست ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['intro'] if chunk.strip()])\n",
    "        \n",
    "        # Add main discussion\n",
    "        if sections['main_discussion']:\n",
    "            script_parts.append(\"=== النقاش الرئيسي ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['main_discussion'] if chunk.strip()])\n",
    "        \n",
    "        # Add closing\n",
    "        if sections['closing']:\n",
    "            script_parts.append(\"=== ختام البودكاست ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['closing'] if chunk.strip()])\n",
    "        \n",
    "        # Add other content\n",
    "        if sections['other']:\n",
    "            script_parts.extend([chunk for chunk in sections['other'] if chunk.strip()])\n",
    "        \n",
    "        # Join with proper spacing\n",
    "        complete_script = '\\n\\n'.join([part for part in script_parts if part.strip()])\n",
    "        \n",
    "        # Clean up spacing\n",
    "        complete_script = re.sub(r'\\n{3,}', '\\n\\n', complete_script)\n",
    "        \n",
    "        return complete_script.strip()\n",
    "\n",
    "    def verify_structure_preservation(self, original_script, final_script):\n",
    "        \"\"\"\n",
    "        Verify that the original structure was preserved\n",
    "        \"\"\"\n",
    "        # Check section headers\n",
    "        original_sections = len(re.findall(r'===.*===', original_script))\n",
    "        final_sections = len(re.findall(r'===.*===', final_script))\n",
    "        \n",
    "        if original_sections > 0 and final_sections < original_sections:\n",
    "            return False\n",
    "        \n",
    "        # Check speaker preservation\n",
    "        original_speakers = set(re.findall(r'^([^:]+):', original_script, re.MULTILINE))\n",
    "        final_speakers = set(re.findall(r'^([^:]+):', final_script, re.MULTILINE))\n",
    "        \n",
    "        # Should preserve most speakers\n",
    "        if len(final_speakers) < len(original_speakers) * 0.8:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def non_ai_length_recovery(self, script, target_length):\n",
    "        \"\"\"\n",
    "        Recover length using non-AI methods (pattern-based expansion)\n",
    "        \"\"\"\n",
    "        current_length = len(script)\n",
    "        if current_length >= target_length * 0.9:\n",
    "            return script\n",
    "        \n",
    "        # Natural Arabic conversation extenders\n",
    "        extenders = [\n",
    "            (\"د. \", \"د. المحترم \"),\n",
    "            (\"أستاذ \", \"أستاذ فاضل \"),\n",
    "            (\"نعم\", \"نعم بالتأكيد\"),\n",
    "            (\"بالطبع\", \"بالطبع وبلا شك\"),\n",
    "            (\"هذا\", \"هذا الأمر\"),\n",
    "            (\"ممتاز\", \"ممتاز جداً\"),\n",
    "            (\"صحيح\", \"صحيح تماماً\"),\n",
    "            (\"أعتقد\", \"أعتقد بقوة\"),\n",
    "            (\"يمكن\", \"يمكن بالفعل\"),\n",
    "            (\"المهم\", \"والأمر المهم\"),\n",
    "        ]\n",
    "        \n",
    "        expanded = script\n",
    "        chars_added = 0\n",
    "        target_addition = min(target_length - current_length, current_length // 5)  # Max 20% expansion\n",
    "        \n",
    "        for original, replacement in extenders:\n",
    "            if chars_added >= target_addition:\n",
    "                break\n",
    "            \n",
    "            if original in expanded:\n",
    "                # Replace some instances (not all to avoid repetition)\n",
    "                count = expanded.count(original)\n",
    "                replace_count = min(count // 3, 2)  # Replace 1/3, max 2 instances\n",
    "                \n",
    "                for _ in range(replace_count):\n",
    "                    expanded = expanded.replace(original, replacement, 1)\n",
    "                    chars_added += len(replacement) - len(original)\n",
    "                    \n",
    "                    if chars_added >= target_addition:\n",
    "                        break\n",
    "        \n",
    "        return expanded\n",
    "\n",
    "    def validate_final_script(self, script):\n",
    "        \"\"\"\n",
    "        Final validation of the complete script\n",
    "        \"\"\"\n",
    "        if not script or len(script.strip()) < 50:\n",
    "            return False\n",
    "        \n",
    "        # Check for basic dialogue structure\n",
    "        if ':' not in script:\n",
    "            return False\n",
    "        \n",
    "        # Check for excessive foreign content (relaxed)\n",
    "        foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', script))\n",
    "        total_chars = len(re.sub(r'[\\s\\n\\t:=\\-،.؟!\"()[\\]{}]', '', script))\n",
    "        \n",
    "        if total_chars > 0:\n",
    "            foreign_ratio = foreign_chars / total_chars\n",
    "            if foreign_ratio > 0.1:  # Allow up to 10% foreign (technical terms, etc.)\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _estimate_duration(self, script_text):\n",
    "        \"\"\"Estimate podcast duration\"\"\"\n",
    "        if not script_text:\n",
    "            return \"0-1 minutes\"\n",
    "        \n",
    "        word_count = len(script_text.split())\n",
    "        duration_minutes = word_count / 150\n",
    "        \n",
    "        min_duration = max(1, int(duration_minutes - 1))\n",
    "        max_duration = int(duration_minutes + 2)\n",
    "        \n",
    "        return f\"{min_duration}-{max_duration} minutes\"\n",
    "\n",
    "    def _generate_complete_fallback(self):\n",
    "        \"\"\"Generate complete fallback script\"\"\"\n",
    "        fallback_script = \"\"\"=== مقدمة البودكاست ===\n",
    "د. فاطمة الزهراء: مرحباً بكم مستمعينا الكرام في حلقة جديدة من برنامجنا. اليوم نستضيف خبيراً متميزاً لنناقش موضوعاً مهماً يهم الجميع.\n",
    "\n",
    "م. عبد الرحمن الهاشمي: أهلاً وسهلاً بكم، أشكركم على الاستضافة الكريمة. سعيد جداً بوجودي معكم اليوم.\n",
    "\n",
    "د. فاطمة الزهراء: نحن سعداء بوجودك معنا. دعنا نبدأ هذا النقاش المفيد.\n",
    "\n",
    "=== النقاش الرئيسي ===\n",
    "د. فاطمة الزهراء: بداية، ما رأيك في أهمية هذا الموضوع في وقتنا الحالي؟\n",
    "\n",
    "م. عبد الرحمن الهاشمي: موضوع في غاية الأهمية حقاً. نحن نواجه تحديات كبيرة تتطلب منا فهماً عميقاً ونظرة شاملة للأمور.\n",
    "\n",
    "د. فاطمة الزهراء: ممكن تحدثنا أكثر عن هذه التحديات؟\n",
    "\n",
    "م. عبد الرحمن الهاشمي: بالطبع. من أهم التحديات هو كيفية التوازن بين التطورات الحديثة والحفاظ على قيمنا وثوابتنا الأصيلة.\n",
    "\n",
    "د. فاطمة الزهراء: نقطة مهمة جداً. وما هي الحلول العملية التي تقترحها؟\n",
    "\n",
    "م. عبد الرحمن الهاشمي: أعتقد أن الحل يكمن في التعليم والتوعية، مع الاستفادة الذكية من التقنيات الحديثة بما يخدم مصالحنا وأهدافنا.\n",
    "\n",
    "=== ختام البودكاست ===\n",
    "د. فاطمة الزهراء: في ختام حلقتنا اليوم، أشكرك أستاذ عبد الرحمن على هذا النقاش الثري والمفيد.\n",
    "\n",
    "م. عبد الرحمن الهاشمي: شكراً لك على الاستضافة الكريمة. كان نقاش ممتع ومثمر، وأتمنى أن يستفيد منه المستمعون.\n",
    "\n",
    "د. فاطمة الزهراء: بالتأكيد. وشكراً لكم مستمعينا الكرام على متابعتكم الدائمة. نلقاكم في حلقة قادمة بإذن الله. إلى اللقاء.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'complete_script': fallback_script,\n",
    "            'cleaning_method': 'micro_chunking_fallback',\n",
    "            'micro_chunks_processed': 0,\n",
    "            'cleaning_status': 'fallback_used',\n",
    "            'script_length': len(fallback_script),\n",
    "            'estimated_duration': self._estimate_duration(fallback_script)\n",
    "        }\n",
    "\n",
    "\n",
    "# Testing Function\n",
    "def test_micro_chunking_cleaner(deployment, corrupted_script_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the micro-chunking AI script cleaner\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"🧪 TESTING MICRO-CHUNKING AI SCRIPT CLEANER\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaner = MicroChunkingAIScriptCleaner(deployment, model_name)\n",
    "    \n",
    "    # Show original script info\n",
    "    if isinstance(corrupted_script_result, dict):\n",
    "        original_script = corrupted_script_result.get('complete_script', '')\n",
    "    else:\n",
    "        original_script = str(corrupted_script_result)\n",
    "    \n",
    "    print(\"📊 ORIGINAL SCRIPT INFO\".center(60, \"-\"))\n",
    "    print(f\"{'Original Length:':<25} {len(original_script):,} characters\")\n",
    "    print(f\"{'Estimated Duration:':<25} {cleaner._estimate_duration(original_script)}\")\n",
    "    \n",
    "    # Quick preview of corruption\n",
    "    foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', original_script))\n",
    "    corruption_ratio = foreign_chars / len(original_script) if original_script else 0\n",
    "    print(f\"{'Apparent Corruption:':<25} {corruption_ratio:.1%} foreign content\")\n",
    "    \n",
    "    # Clean the script\n",
    "    cleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\n",
    "    \n",
    "    print(\"\\n📊 MICRO-CHUNKING RESULTS\".center(60, \"-\"))\n",
    "    print(f\"{'Method:':<25} {cleaned_result['cleaning_method']}\")\n",
    "    print(f\"{'Status:':<25} {cleaned_result['cleaning_status']}\")\n",
    "    print(f\"{'Micro-chunks Processed:':<25} {cleaned_result['micro_chunks_processed']}\")\n",
    "    print(f\"{'Original Length:':<25} {cleaned_result.get('original_length', 'N/A'):,} characters\")\n",
    "    print(f\"{'Final Length:':<25} {cleaned_result['script_length']:,} characters\")\n",
    "    print(f\"{'Length Preserved:':<25} {cleaned_result.get('length_ratio', 0):.1%}\")\n",
    "    print(f\"{'Duration:':<25} {cleaned_result['estimated_duration']}\")\n",
    "    \n",
    "    # Length preservation status\n",
    "    if 'length_ratio' in cleaned_result:\n",
    "        if cleaned_result['length_ratio'] >= 0.90:\n",
    "            print(f\"{'Length Status:':<25} ✅ EXCELLENT PRESERVATION\")\n",
    "        elif cleaned_result['length_ratio'] >= 0.80:\n",
    "            print(f\"{'Length Status:':<25} ✅ GOOD PRESERVATION\")\n",
    "        elif cleaned_result['length_ratio'] >= 0.70:\n",
    "            print(f\"{'Length Status:':<25} ⚠️ MODERATE LOSS\")\n",
    "        else:\n",
    "            print(f\"{'Length Status:':<25} ❌ SIGNIFICANT LOSS\")\n",
    "    \n",
    "    # Corruption analysis summary\n",
    "    if 'corruption_analysis' in cleaned_result:\n",
    "        corruption_info = cleaned_result['corruption_analysis']\n",
    "        print(f\"{'Corruption Detected:':<25} {corruption_info['overall_corruption_level'].upper()}\")\n",
    "        print(f\"{'Foreign Patterns:':<25} {len(corruption_info['foreign_patterns'])} types found\")\n",
    "    \n",
    "    # Display cleaned script with formatting\n",
    "    print(\"\\n\" + \"🎙️ MICRO-CLEANED SCRIPT OUTPUT\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaned_script = cleaned_result['complete_script']\n",
    "    if cleaned_script:\n",
    "        sections = cleaned_script.split('\\n\\n')\n",
    "        \n",
    "        for section in sections[:10]:  # Show first 10 sections to avoid overwhelming output\n",
    "            section = section.strip()\n",
    "            if not section:\n",
    "                continue\n",
    "            \n",
    "            # Section headers\n",
    "            if section.startswith('===') and section.endswith('==='):\n",
    "                print(f\"\\n{section}\")\n",
    "                print(\"─\" * len(section))\n",
    "                continue\n",
    "            \n",
    "            # Format dialogue\n",
    "            lines = section.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                if ':' in line and not line.startswith('==='):\n",
    "                    speaker, dialogue = line.split(':', 1)\n",
    "                    speaker = speaker.strip()\n",
    "                    dialogue = dialogue.strip()\n",
    "                    \n",
    "                    print(f\"\\n{speaker}:\")\n",
    "                    \n",
    "                    # Word wrap dialogue\n",
    "                    words = dialogue.split()\n",
    "                    current_line = \"\"\n",
    "                    max_length = 70\n",
    "                    \n",
    "                    for word in words:\n",
    "                        if len(current_line) + len(word) + 1 > max_length and current_line:\n",
    "                            print(f\"    {current_line}\")\n",
    "                            current_line = word\n",
    "                        else:\n",
    "                            current_line = current_line + \" \" + word if current_line else word\n",
    "                    \n",
    "                    if current_line:\n",
    "                        print(f\"    {current_line}\")\n",
    "                else:\n",
    "                    print(f\"    {line}\")\n",
    "    \n",
    "    # Final validation results\n",
    "    is_clean = cleaner.validate_final_script(cleaned_script)\n",
    "    print(f\"\\n🔍 FINAL VALIDATION: {'✅ PASSED' if is_clean else '❌ NEEDS REVIEW'}\")\n",
    "    \n",
    "    # Quality comparison\n",
    "    if original_script and cleaned_script:\n",
    "        original_foreign = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', original_script))\n",
    "        cleaned_foreign = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', cleaned_script))\n",
    "        \n",
    "        print(f\"🧹 CLEANING EFFECTIVENESS:\")\n",
    "        print(f\"    Foreign chars removed: {original_foreign - cleaned_foreign:,}\")\n",
    "        print(f\"    Cleaning efficiency: {((original_foreign - cleaned_foreign) / original_foreign * 100) if original_foreign > 0 else 0:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return cleaned_result\n",
    "\n",
    "\n",
    "# Advanced Testing Function with Detailed Analysis\n",
    "def detailed_micro_chunk_analysis(deployment, corrupted_script_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Detailed analysis of micro-chunking performance\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"🔬 DETAILED MICRO-CHUNKING ANALYSIS\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaner = MicroChunkingAIScriptCleaner(deployment, model_name)\n",
    "    \n",
    "    # Extract script\n",
    "    if isinstance(corrupted_script_result, dict):\n",
    "        original_script = corrupted_script_result.get('complete_script', '')\n",
    "    else:\n",
    "        original_script = str(corrupted_script_result)\n",
    "    \n",
    "    print(\"🔍 PRE-PROCESSING ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Detailed corruption analysis\n",
    "    corruption_analysis = cleaner.analyze_corruption_patterns(original_script)\n",
    "    \n",
    "    print(f\"Script length: {corruption_analysis['total_chars']:,} characters\")\n",
    "    print(f\"Overall corruption level: {corruption_analysis['overall_corruption_level'].upper()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Foreign content breakdown:\")\n",
    "    for pattern_name, details in corruption_analysis['foreign_patterns'].items():\n",
    "        print(f\"  • {details['description']}: {details['count']} instances ({details['chars']} chars)\")\n",
    "        if details['examples']:\n",
    "            examples_preview = [str(ex)[:10] + ('...' if len(str(ex)) > 10 else '') for ex in details['examples'][:2]]\n",
    "            print(f\"    Examples: {', '.join(examples_preview)}\")\n",
    "    \n",
    "    if corruption_analysis['concatenated_words'] > 0:\n",
    "        print(f\"  • Concatenated word sequences: {corruption_analysis['concatenated_words']}\")\n",
    "    if corruption_analysis['encoding_issues'] > 0:\n",
    "        print(f\"  • Mixed encoding issues: {corruption_analysis['encoding_issues']}\")\n",
    "    \n",
    "    print(\"\\n🔬 MICRO-CHUNKING BREAKDOWN\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create micro-chunks for analysis\n",
    "    micro_chunks = cleaner.create_micro_chunks(original_script)\n",
    "    \n",
    "    print(f\"Total micro-chunks created: {len(micro_chunks)}\")\n",
    "    if micro_chunks:\n",
    "        avg_chunk_size = sum(chunk['length'] for chunk in micro_chunks) / len(micro_chunks)\n",
    "        print(f\"Average chunk size: {avg_chunk_size:.1f} characters\")\n",
    "        \n",
    "        chunk_sizes = [chunk['length'] for chunk in micro_chunks]\n",
    "        print(f\"Chunk size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "    \n",
    "    # Analyze chunk corruption levels\n",
    "    corruption_levels = {'clean': 0, 'minor': 0, 'heavy': 0}\n",
    "    for chunk in micro_chunks:\n",
    "        level = cleaner.assess_chunk_corruption(chunk['content'])\n",
    "        corruption_levels[level] += 1\n",
    "    \n",
    "    print(f\"\\nCorruption level distribution:\")\n",
    "    print(f\"  • Clean chunks: {corruption_levels['clean']} ({corruption_levels['clean']/len(micro_chunks)*100:.1f}%)\")\n",
    "    print(f\"  • Minor issues: {corruption_levels['minor']} ({corruption_levels['minor']/len(micro_chunks)*100:.1f}%)\")\n",
    "    print(f\"  • Heavy corruption: {corruption_levels['heavy']} ({corruption_levels['heavy']/len(micro_chunks)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n🎯 PROCESSING STRATEGY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Expected processing:\")\n",
    "    print(f\"  • Keep as-is: {corruption_levels['clean']} chunks\")\n",
    "    print(f\"  • Light regex cleaning: {corruption_levels['minor']} chunks\") \n",
    "    print(f\"  • AI surgical correction: {corruption_levels['heavy']} chunks\")\n",
    "    \n",
    "    estimated_ai_calls = corruption_levels['heavy']\n",
    "    print(f\"  • Estimated AI API calls: {estimated_ai_calls}\")\n",
    "    \n",
    "    if estimated_ai_calls > 0:\n",
    "        print(f\"  • Efficiency vs. large chunks: {estimated_ai_calls} calls vs ~3-4 calls (traditional)\")\n",
    "        print(f\"  • Trade-off: More calls but higher precision per call\")\n",
    "    \n",
    "    print(f\"\\n💡 PREDICTION:\")\n",
    "    clean_ratio = corruption_levels['clean'] / len(micro_chunks)\n",
    "    if clean_ratio > 0.7:\n",
    "        print(\"  Expected outcome: EXCELLENT length preservation (70%+ content kept as-is)\")\n",
    "    elif clean_ratio > 0.5:\n",
    "        print(\"  Expected outcome: GOOD length preservation (50%+ content kept as-is)\")\n",
    "    else:\n",
    "        print(\"  Expected outcome: MODERATE length preservation (heavy corruption detected)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'total_chunks': len(micro_chunks),\n",
    "        'corruption_analysis': corruption_analysis,\n",
    "        'corruption_distribution': corruption_levels,\n",
    "        'estimated_ai_calls': estimated_ai_calls,\n",
    "        'clean_content_ratio': clean_ratio\n",
    "    }\n",
    "\n",
    "\n",
    "# Usage Examples:\n",
    "\"\"\"\n",
    "# Basic cleaning\n",
    "cleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\n",
    "cleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\n",
    "\n",
    "# Testing with detailed output\n",
    "test_result = test_micro_chunking_cleaner(deployment, corrupted_script_result)\n",
    "\n",
    "# Detailed pre-processing analysis\n",
    "analysis = detailed_micro_chunk_analysis(deployment, corrupted_script_result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94117e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================🔬 MICRO-CHUNKING AI SCRIPT CLEANER=======================\n",
      "\n",
      "📏 Original script length: 2,913 characters\n",
      "-------------------🔍 CORRUPTION ANALYSIS--------------------\n",
      "    Overall corruption level: LIGHT\n",
      "    Foreign patterns found: 1\n",
      "      - English words (3+ letters): 1 instances\n",
      "        Examples: strong\n",
      "    Mixed encoding issues: 1\n",
      "-----------------\n",
      "📝 MICRO-CHUNKING STRATEGY-----------------\n",
      "    Created 12 micro-chunks (avg: 242 chars each)\n",
      "----------------\n",
      "🔬 SURGICAL CLEANING PROCESS----------------\n",
      "    Processing micro-chunk 1/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 2/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 3/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 4/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 5/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 6/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 7/12... 🔧 AI CORRECTED\n",
      "    Processing micro-chunk 8/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 9/12... 🔧 AI CORRECTED\n",
      "    Processing micro-chunk 10/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 11/12... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 12/12... ✅ CLEAN (kept as-is)\n",
      "-------------\n",
      "🔧 REASSEMBLY WITH STRUCTURE CHECK-------------\n",
      "    Original structure preserved: ✅\n",
      "    Length preservation: 100.3%\n",
      "\n",
      "------------------🎉 MICRO-CHUNKING SUMMARY------------------\n",
      "    Status: SUCCESS\n",
      "    Original Length: 2,913 characters\n",
      "    Final Length: 2,922 characters\n",
      "    Length Preserved: 100.3%\n",
      "    Micro-chunks Processed: 12\n",
      "    Arabic Quality: ✅ VALID\n",
      "================================================================================\n",
      "Cleaned Result: {'complete_script': '=== مقدمة البودكاست ===\\n\\nأحمد بن علي: سلام عليكم جميعاً، ونرحب بكم في هذه الحلقة التي نغوص فيها بعمق لتفهم التوازي الحساس بين ثورتِ الذكاء الصناعي وثروتِ الهوية العربية الواعدة؛ دعونا نتشارك باهتمام نسبر فيه آفاق هذا المزج الفريد لمستقبلنا الثقافي المحلي.\\n\\n**أحمد بن علي:** ترحيب للدكتورة فاتن Рашид، من الرائع أنّها معنا اليوم لمناقشة مسألة حيوية حول الترابط بين تقنية الذكاء الاصطناعي وهویتَنا العربيّة.\\n**د. فَتْن رَاشِد:** شكر لك، نبحث بالفعل تحديات مثيرة تتعلق بهويتنا التقليدية في عالم مترابط بشكل رقمي.\\n\\n**أحمد بن علي:** كيف بإمكاننا ضمّ تقنيّات المستقبل لدعم قيمنا ومعتقداتنا بدلاً من تهديد وجودها?\\n**د. فَتْن رَاشِد:** يجب أن نركز على تطوير حلول ذكية تستلهم جذورها القيميّة الثقافيّة بينما ترنو إلى مستقبِل متنوِّر تكنولوجيًا وقائم علَى أسس أخلاقيِّة صلبة.\\n\\n**أحمد بن علي:** اقتراح ممتاز! هل يمكن ذكر مثال عملي لإدارة هذا الانصهار الناجح نحو آفاق واعدة للعالم العربِي؟\\n**د. فَتْن رَاشِد:** نعم، علينا تشكيل نماذج التعليم الداخيلة المحترمة لروافدنا الفكريَّــة وتوظيف وسائل التواصل الرَّاقمية كمنبر للتواصل بقضايا مجتمعيَّة ذات مغزى وفائدة مشتركة.\\n\\n=== النقاش الرئيسي ===\\n\\n**أحمد بن علي:** بدايةً, للنظام الغذائي دور رئيسي لكن هل تعلم أنه قد يُهمل البعض أهمية الرياضة أيضاً؟\\n**د. فاتن راشد:** صحيح, التدريب البدني مهم لكن التغذية الصحية هي الأساس عند بدء أي برنامج صحي.\\n\\n**أحمد بن علي:** ولكن يمكن للرياضة أن تعوض بعض قصور النظام الغذائي بمحتواها من السعرات الحرارية المحروقة.\\n**د. فاتن راشد:** هذا صحيح ولكن الأفضل هو موازنة الاثنين معًا؛ ليس فقط التعويض عن نقص واحد بالتكبير في الآخر.\\n\\nأحمد بن علي: ربما، لكن الكثير يهدف إلى تحسين صحته النفسية أولا، وقد يكون الجِمناستيك أو اليوجا مفيدا لهذا الغرض بشكل أكبر! الدكتورة فاتن رشيد: ومع ذلك، ممارسة الرياضة التي تستمتعين بها وتناسب جسدك هي الخيار الأفضل بلا شك لتعزيز صحتك العامة وسلامتك العقلية أيضا.\\n\\nأحمد بن علي: في الجانب الثاني من الموضوع، كيف ترى دور التقنية فيها؟\\nد. فاتن راشد: تقف حجر عثرة كثيرة حسب تجاربي.\\nأحمد بن علي: لكن البعض يقول إنها فرصة للنمو.\\nد. فاتن راشد: نعم ربما، لكنها قد تخلق أيضًا تحديات جديدة.\\n\\nأحمد بن علي: صحيح فيما يتعلق بالتحديات الجديدة، لكن هل تدرك أن بعض الأشخاص يستفيدون منها بشكل كبير أيضًا؟ د. فاتن راشد: بالطبع، الأمر يعتمد على وضع كل فرد، ولكنه يحتاج دائمًا إلى جهد متواصل. أحمد بن علي: لا ريب في ذلك، التفكير الاستراتيجي هو مفتاح النجاح هنا.\\n\\n**أحمد:** حول الحلول المقترحة لهذا الجانب, كيف تراهِ \"فاتن\" ؟\\n**فاتن:** تحديات كثيرة تحتاج دراسة متأنية قبل تقديم حلول.\\n**أحمد:** لكن البعض يدعو للتغيير الفوري أليس كذلك?\\n**فاتن:** الإصلاح يحتاج وقت ولا يمكن الاستعجال فيه.\\n**أحمد:** ربما نقترح خطوات سريعة فعالة عوضاً عن التأجيل .\\n\\n**فاتن:** قد تكون الخطوات السريعة مؤقتة ولكنها يجب أن تستند إلى دراسات دقيقة أولًا .\\n**أحمد:** إذن توافقين بأن الدراسات ضرورية قبلهم كلاهما!\\n**فاتن:** تمامًا ،لكن ليس عذر لتجنب الحركة نحو الحل.\\n\\n=== ختام البودكاست ===\\n\\nأحمد: شكرًا دكتورة فاتن، كانت محادثة غنية بالتجارب.\\nفاتن: من الرائع دائماً تبادل الأفكار معكم، أشكركم.\\nأحمد: وشكراً لمستمعينا العزيزين， سنلتقي مجددًا.\\nفاتن: إلى اللقاء ونراكم مرة أخرى!', 'cleaning_method': 'micro_chunking_surgical', 'micro_chunks_processed': 12, 'cleaning_status': 'success', 'script_length': 2922, 'original_length': 2913, 'length_ratio': 1.0030895983522141, 'corruption_analysis': {'total_chars': 2913, 'foreign_patterns': {'english_words': {'count': 1, 'chars': 6, 'description': 'English words (3+ letters)', 'examples': ['strong']}}, 'encoding_issues': 1, 'concatenated_words': 0, 'structural_issues': 40, 'overall_corruption_level': 'light'}, 'estimated_duration': '2-5 minutes'}\n"
     ]
    }
   ],
   "source": [
    "# Basic cleaning\n",
    "cleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\n",
    "cleaned_result = cleaner.clean_script_with_ai(script_result)\n",
    "print(\"Cleaned Result:\", cleaned_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "226c9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'complete_script': '=== مقدمة البودكاست ===\\n\\nأحمد بن علي: انطلق بنا مباشرة إلى قلب تحدٍ عصري يغوص في أعماق تقاطع الذكاء الاصطناعي والثقافة العربية, حيث سنضع كلانا أيديهما بقلب متنبه ومشوق.... بصراحة، موضوع الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي يشغل بالي من فترة. يعني، كلنا نواجه هذا الأمر بشكل أو بآخر، صح؟\\n\\n**أحمد بن علي:** معي اليوم الباحثة المتميزة التي لا تتوقف عن تحدينا بإسئلتها المحفزة حول الذكاء الاصطناعي وخصوصياتنا المعرفية، الدكتورَةُ فاتِنْ رَاشِدْ. د. فاتِنْ، هلاّ شرحت لي كده ليه عند البعض شعور إزّا تراثنة المعلومات اللكترونية تهدّد هويتنا القومية؟!\\n\\n**د. فاتِنْ:** سؤال جميل مش جديد لكنه حيوي. بالعكس، نحتمل أن يكون التعاون مع آليات التعلم الآلي وسيلة فعَّالة لحفظ تقاليدنا، شرط أن نوفر لها البيانات الضرورية بلغاتنا العريقة وبناء الفهم الصحيح لمدلولاتنا الحساسة.\\n\\n**أحمد بن علي:** كلام قوي. بس شوفينى، دي مفروض خطوة سيبرانية مجردا لأ تحميكو من سرقة الهويات أم إطلاق روابط جديدة بين الجيل القديم والأصغر؟\\n\\n**د. فاتِنْ:** حاجة كويسّة تسؤل عنها. نعم قد تستخدم تقنيات حماية البيانات للأمان الشخصي ولكنه دور جانبي للإمكانيات الواسع. هدفانا الأعلى الإبقاءعلى قيمة الطابع الإنساني والفكري للغة وفلكلور مجتمعنا رغم الغزو السيباري العالمي.\\n\\nأحمد بن علي: حسنًا، لقد فهمت وجهة نظرك. ولكن لدي سؤال آخر يتعلق بموضوع مختلف؛ على سبيل المثال، منصة \"روsetta\" لتعليم النماذج اللغوية الطبيعية (NLP) التي تدرب باستخدام البيانات الخارجية الغنية والمشابهة للمحتوى غير العربي. فهل يؤدي استخدام تلك الأنظمة شبه الخاضعة للإشراف إلى التأثير السلبي على تميز جمالية الأدب العربي، أيها الدكتور؟\\n\\nد. فاتِنْ: \\u200f#الله يجزاك المعرفة قدرها ،ﻫذا صلب النقاش والحقيقة يعاني حالياً مجال البحث وصلته بأضطرابه بسبب مصدر هذه الجرامرات التدريبة Massive Datasets دون مراعاة خلفيات حضارية متنوعة ومآطر ادبية مختلفة ,ولکن تبقى المسؤولة الأولىعن المثال نفسهالأستاذ الجامعىوالمنظومة التعليم الأصيلة وأكثرالمستخدم شناعاتمحاسبة القلب الي شرعلة تصنع المستقبل عبر قوانيبانفكر وملتقياستنيرالبلسمالعروبية✨\\uf3e0🎯 (لاحظ بعض الأخطاء الاخر الكتابية الأخيرة نسبيًا فالنقل هنا كان مجتزءً للتفاعلية ).\\n\\nتحسين للطبيعية الأفضل وتجنب المديح الزائد والتعبيرات الرسمية المبالغ فيها: الثقة: هذا يبدو رائعًا! لكن دعنا نركز على التحسين المستمر بدلاً من الثناء الشديد. (ملاحظة: لم يكن في النص الأصلي حوار بين متحدثين، لذا قمت بإنشاء مثال بسيط للحفاظ على بنية الحوار كما طلبت مع التركيز الرئيسي على التصحيح اللغوي) ولكن بناءً على تعليماتك الصارمة لتغيير فقط الكلمات الإنجليزية، سيكون التعديل كالتالي: التحسين لزيادة الطبيعية وتفادي المدائح والمراسم الزائدة:\\n\\nأحمد بن علي: مرحبا، لا يوجد لدي الوقت الكافي كما يتطلب الأمر، السلام عليكم ورحمة الله وبخاطركم 🔥 بحيوية. مستشارة بارعة تخرق التقاليد المؤسسية حول هذه الكائنات الروبوتية الذكية وتدفق الابتكار؛ إليك الدكتورة الباحثة العالمية التي توسع فهمنا التاريخي للقداسة في الإسلام من خلال منظور جديد لروبوتات الفكر الإسلامية والتي تلخص كل شيء مفصلاً عن العلاقات الحديثة للمجتمع والثراء العقلي والوصول بالجيل الجديد إلى إمكانيات التكنولوجيا حتى المناصب الإدارية المطبقة على الذكاء الاصطناعي... ولكن أيضًا، فهو يعالج قلق قلبي... اسقيادي كبير مجتمع سايبرمتع أولى الحاضر ... 😂 وبالطبع نحترم.... ذِكْراتُ الفتاة رشيداً: اهلا يا أحمد صدقا أستاذ زنخار شديد هذا الموقع 😂 ️ كثير مما لا تريد إلغاء الجدال الخاص بالأصول لكن أساس استخدام الآلة ليس كاملاً لأننا نتعلم ونحتفل بمشاركة المطريات والتجارب بين الغازبين الذين كانوا ثوريين أو متخاصمين بشكل يومي × فا10 | سنترفوري الاتحاد البحري للعالمات والأجيال المتعارضة مجموع تركيبة أغلفة ميديا بريطانية عام 1932 تشكيلها جنيتمنت تحت الانصهار العروسي ……….. (انسحاب وزغب داخلي قرأت عليه عملية روبوسكولنبك)... !!! (بعد تصفية اتصال جانبي حسب المصطلح).... .(توتر خفيف فعلي يبدو واضح وصريح داخل النقاش!)..... 😊 ❤️💚💙 💜\\U0001f1d2🌟👨\\u200d💻👩\\u200d💻📚🇦🇪🇸🇦❅👏🏻💕👍🏾💪🏽🇵🇸🇾🇨🇳🇺🇸🇬🇧🇮🇷🇹🇷🇲🇩🇵🇰🇶🇦📽🇸🇺🇨🇿🇦🇱🇦🇺#شكرا 🙏🏼لجهودكم الطاقانية والدعم...... 😉🙏💖💥💎 !\\n\\n=== النقاش الرئيسي ===\\n\\n**أحمد:** إن الحديث حول كيفية تأثير الذكاء الاصطناعي على هويتنا يتردد صداه في أفواه الكثيرين. لنبدأ من مكان حار هذه الأيام: رسائل واتساب! كيف تستوعب برامج الدردشة الذكية تعابيرنا المحلية مثل «ما شاء الله» دون فهمها حقًا؟ هل نفوّت تقاليدنا كلمات مفرداتها أساسها تقنيات غربية عصرية؟\\n\\nفاطمة: بالتأكيد، إنها رؤية جديدة للقضية \"أقدم من الجديدة\". ولكن، يبقى الحفاظ على هويتنا الأصلية ليس مقتصراً فقط على اللغة المستعملة؛ بل يتعداها إلى العلامات الثقافية التي تنقل معاني عميقة مستمدة من تاريخنا الطويل وتجاربنا البشرية الغنية ضمن الفضاء الرقمي الواسع اليوم. حيث تمتلك القدرة على توجيه الذكاء الاصطناعي باتجاه قيمنا والإنسانية الخاصة بنا. لكن دعوني أطرح سؤالاً هاماً هنا: عند استخدام روبوت الدردشة الذي يكتب بالعربية الفصحى بعيداً عن اللهجات الشائعة، هل سيؤثر هذا بشكل حقيقي على تأثيره لدى الجمهور المهتم بالحروف العربية الأصيلة؟ ربما لا؛ لأن التحسن في قدراته اللغوية لن يؤثر على جوهر محتواه! مجرد مقدمة لما سنناقشه لاحقاً... ماذا عن رأيك بهذا الأمر يا أحمد؟!\\n\\n## توضيح أنماط السلوك الطبيعي: في هذا المثال الأول، قدمنا العناصر التالية بشكل مناسب في حوارنا: - المداخلات القصيرة: عبارة موجزة كل مشارك قبل تغيير التركيز نحو الاستماع للمستمع الآخر. [✓] (رد مختصر)\\n\\n- تضمين الاختلافات: التعبير عن اختلاف وتنافر بطريقة مرحب بها ومحفزة من البداية، مع الأمل في أن يزداد حماس النقاش مستقبلاً إذا استمرت الدراسة حول توفر الوقت المناسب. [التمثيل الجسدي للتوتر والالتزام الصادق للموضوع.]*( ولكن...)* [نجاح التحقق]![✓]. *مقارنته بقطيعة بسيطة مؤقتة قد يشجع بشكل أفضل على دفع الحوار نحو الإصلاح لاحقاً.* -[( فرصة إيجابية للوصول إلى لب الفكرة)]*. * *_ _ _ _ ___ --__-_- __--_-_\\\\ --- _-* \\\\-\\\\-| |_* \\\\/ /*/-\\\\\\\\\\\\\\\\ /\\' ---متابعة القادم---..... ... ...... ..... ....... .......... .................\\n\\nأحمد: مرحباً مجدداً للجميع! عندما نناقش التحديات بالإضافة إلى فرص الذكاء الصناعي في العالم العربي، فإن مسألة توافق عمليات التعلم العميق (الذي يتعلم من كميات كبيرة من البيانات بدون تدخل بشري) مع القيم الإسلامية والثقافية العربية هي نقطة شائكة دوماً. فماذا ترين أنتِ، دكتورة، حيال هذا الأمر؟\\n\\n**فاتن:** حقيقةً، هذه نقطة محورية. وفي الشريعة الإسلامية، نحن نؤكد بشدةعلى أهمية بناء العلاقات ذات الحكم والعناية بالنوايا. فهذا يفكر المرء عندما يقولون إن تعلم الآلات يمكن يُفهم بأنه نموذج للتطور العقابدي الموجه حقًا بواسطة البشر الذين يخلقونه ويعيدون تدريبه. لكن بالتأكيد هناك حالات حيث قد يحدث سوء فهم للقيم الأساسية لدينا. يجب علينا البحث دائماً عن توازن يعطي الأولوية لتوافق النظام التراث الأصيل وجديد الإنترنت العالم الإلكتروني الدينامي .\\n\\nأحمد: بصدق، دعنا نوسع الحديث عن مثال موضوع \"العقل\". فالإيمان بالخالق يشكل أساس معتقداتي وفهمي لهويتنا ككيانات حرة وليست مجرد تنفيذ لتعليمات حسابية معقدة. هل تستطيع شرح كيف يتعامل العلماء الذين يواجهون نزاعات ذات طابع فلسفي مثل هذه خلال عملية تطوير الذكاء الاصطناعي؟\\n\\nفاتن: للحظة، مومنتوم... بالطبع، هو موضوع مؤثر للغاية بالنسبة لنا جميعًا. فالواقع أن العديد من الباحثين في المجتمع العربي الإسلامي يبذلون جهودهم لجذب الانتباه إلى الفقه والتاريخ والفكر الثوري القديم لتوجيه أبحاثهم؛ وكذلك يفعل الخبراء في مجال الكمبيوتر والمهتمون بفهم رحلاته الرائعة! وبالتالي، إنهم يسيرون على خطى بلوتارخ لتحقيق تقدم مستدام ويبتعدون عن النزاعات الأيديولوجية أثناء كل مرحلة تصميم تالية. ومن ثم، هدفنا هو تعزيز مصداقية التراث الأدبي والديني مع الاستفادة القصوى من التحولات الصناعية الهائلة وإمكانياتها المستقبلية المحددة 💪🏽 --- نهاية المحاكاة ---\\n\\nأحمد بن علي: نصل الآن إلى الحلول العملية يا دكتورة فاتن؛ هل يمكن القول إن تعليم جيلنا الطرق الصحيحة لاستخدام التكنولوجيا هو نصف المعركة?\\nد. فاتن راشد: نعم، بالقطع! لكننا يجب أيضا التشديد على الأهمية القصوى لنشر وعينا السليمة تجاه الأخلاق المتعلقة بالتقدم التكنولوجي وألا تمحه هويتنا وتراثنا.\\n\\nأحمد بن علي: ولكن ألم يتحول التركيز مؤخرًا أكثر نحو ضمان مستقبل صناعي رقمي قوي بدلاً من التأكد من بقائنا أصلاء وثابتين?\\nد. فاتن راشد: هذا صحيح، ولكنه ليس حتميًا. بالإبداع واستخدام الأدوات الحديثة بشكل ذكي، بإمكان العرب اكتساب مكانة مميزة ورقمية تتناسب مع تراثها الغني.\\n\\nأحمد بن علي: لكن تبقى تحديات كبيرة ولا زالت خسائر محتملة، أمثلة منها فقدان الوظائف بسبب التحول للذكاء الاصطناعي...\\n\\nد. فاتن راشد: لاشك بأن هناك مخاطر شتى فعلًا، لكن هذه ليست عائقَ لإيجاد فرص جديدة ووظائف ذات قيمة أكثر. مثلاً الأفراد المبدعون الذين يفهمون جيدًا كلاهما التكنولوجيا والثقافة هم القادرون على خلق أماكن العمل والإنجازات الفكرية الثورية.\\n\\nأحمد بن علي: فعلاً، يبدو أنه سيتطلب علينا قبول الفكرة التالية: التنسيق بين القدرات الإنسانية والتقنية لتحقيق أفضل مردود للعالم العربي ...\\n\\nد. فاتن راشد: وبناءً عليه، سوف يشكل الكفاح الجاد لتطبيق الإرشادات الثقافية القديمة مثل احترام الآخر ومعرفة الذات، إضافة حرجة للحفاظ على نظام أخلاقي واضح ونابع من داخل اجتماعات المجتمع الرقمية الضخم القادم لنا.\\n\\n=== ختام البودكاست ===\\n\\n**أحمد:** بصراحة، نقاش جميل يا دكتور فاتن! الحقيقة إن الأجزاء حول دور الذكاء الصُّناعي في الحفاظ على هويتنا العربية كانت مثيرة للاهتمام حقاً. لكن مازلت أملك بعض التساؤلات حول كيفية التطبيق العملي لهذه الأفكار ضمن بيئات الأعمال المحلية.\\n\\n**فاتن:** نعم، بالتأكيد! هذا سؤال مهم. تطبيق هذه الأفكار يتطلب فهم عميق لكيفية تداخل تقنية الذكاء الاصطناعي وثقافتنا. إنها ليست فقط مشكلة إدخال الآلات؛ بل تحتاج أيضاً إلى نهج استراتيجي يراعي قيمنا الأساسية.\\n\\n**أحمد:** صحيح تماماً. وأظن أن أفضل طريقة لتحدّي ذلك هي تشارك أفكاركم ومعارفكم عبر وسائل التواصل الاجتماعي وغيرها مع شباب جيلنا الحالي الذين سيضعون هذه المعرفة موضع التنفيذ غداً.\\n\\n**فاتن:** بالضبط، الجميع لديه دوره في هذا الشأن. سواء كنت طالب تكنولوجيا أو متخصص قانوني أو حتى أم عادية تريد تعليم أولادها، فكلنا مسؤول عن ضمان بقائنا ملتزمين بهويتنا الثقافية.\\n\\n**أحمد:** إذن، أيها المستمع العزيز، ما رأيك أنت في دور كل واحد منا هنا? شاركني آرائك وتعليقك تحت الحلقة، وسوف نتبادل الأفكار لاحقاً!\\n**فاتن:** وبالتأكيد، لنستمر في التعلم والتفاعل معاً. شكراً مرة أخرى لحضوركما معنا اليوم! وداعاً.\\n\\n**أحمد:** تحياتنا لكم جميعاً. ولاتنسى... تابعونا في الحلقة التالية حيث سنواصل رحلتنا لاستكشاف المزيد من المخاوف والأمل المرتبطة بالعالم الرقمي الحديث!', 'cleaning_method': 'micro_chunking_surgical', 'micro_chunks_processed': 29, 'cleaning_status': 'success', 'script_length': 9372, 'original_length': 9193, 'length_ratio': 1.0194713368867616, 'corruption_analysis': {'total_chars': 9193, 'foreign_patterns': {'english_words': {'count': 48, 'chars': 322, 'description': 'English words (3+ letters)', 'examples': ['SPIRAI', 'Rosetta', 'NLP']}, 'chinese_chars': {'count': 2, 'chars': 2, 'description': 'Chinese characters', 'examples': ['光', '卾']}}, 'encoding_issues': 8, 'concatenated_words': 1, 'structural_issues': 50, 'overall_corruption_level': 'light'}, 'estimated_duration': '8-11 minutes'}\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
