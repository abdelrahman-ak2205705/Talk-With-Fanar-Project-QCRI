{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e5796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# import openai\n",
    "from openai import AzureOpenAI\n",
    "# !pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3174aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def APIKeyManager(model_type, key_path):\n",
    "    \n",
    "    load_dotenv(dotenv_path=key_path, override=True)\n",
    "    if model_type=='azure':\n",
    "        client = AzureOpenAI(\n",
    "            api_version=os.environ[\"AZURE_API_VERSION\"],\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_API_KEY\"],\n",
    "        )\n",
    "        return client\n",
    "    elif model_type=='fanar':\n",
    "        client = OpenAI(\n",
    "            base_url = \"https://api.fanar.qa/v1\",\n",
    "            api_key  = os.environ[\"FANAR_API_KEY\"],\n",
    "        )\n",
    "        client.default_params = {\"model\": \"Fanar-C-1-8.7B\"}\n",
    "        return client    \n",
    "    elif model_type=='gemini':\n",
    "        pass\n",
    "    return client\n",
    "\n",
    "# Load environment variables\n",
    "model_type=\"fanar\"\n",
    "deployment = APIKeyManager(model_type, \"./azure.env\")\n",
    "model = \"Fanar-C-1-8.7B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a84582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class TopicClassifier:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def classify_topic(self, topic, information):\n",
    "        \"\"\"\n",
    "        Classify podcast topic and determine optimal approach\n",
    "        \n",
    "        Args:\n",
    "            topic: Main topic of the podcast episode\n",
    "            information: Background information about the topic\n",
    "            \n",
    "        Returns:\n",
    "            JSON with classification results\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in analyzing and classifying topics for Arabic podcast production.\n",
    "\n",
    "Task: Analyze the following topic and determine the best approach for an Arabic podcast.\n",
    "\n",
    "Topic: {topic}\n",
    "Background Information: {information}\n",
    "\n",
    "Analyze the topic and return the result in JSON format with these exact keys:\n",
    "\n",
    "{{\n",
    "    \"primary_category\": \"Main category from the available options\",\n",
    "    \"category_justification\": \"Reason for choosing this category\",\n",
    "    \"optimal_style\": \"Best discussion style from available options\",\n",
    "    \"discourse_pattern\": \"Appropriate discourse pattern\",\n",
    "    \"audience_engagement_goal\": \"Audience engagement objective\",\n",
    "    \"cultural_sensitivity_level\": \"Cultural sensitivity level\",\n",
    "    \"controversy_potential\": \"Controversy potential level\",\n",
    "    \"key_discussion_angles\": [\n",
    "        \"First main discussion angle\",\n",
    "        \"Second point of interest for Arabic audiences\"\n",
    "    ],\n",
    "    \"natural_tension_points\": [\n",
    "        \"First natural tension point in the topic\",\n",
    "        \"Second aspect that might generate healthy debate\"\n",
    "    ],\n",
    "    \"cultural_connection_opportunities\": [\n",
    "        \"First opportunity to connect with Arabic culture\",\n",
    "        \"Second relevant local or regional reference\"\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Available Categories (choose one):\n",
    "1. \"Ø§Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§\" - For technical, scientific topics and innovations\n",
    "2. \"Ø§Ù„Ø³ÙŠØ§Ø³Ø© ÙˆØ§Ù„Ø´Ø¤ÙˆÙ† Ø§Ù„Ø¹Ø§Ù…Ø©\" - For political topics, current events, public affairs\n",
    "3. \"Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\" - For social topics, relationships, values, social challenges\n",
    "4. \"Ø§Ù„Ø±ÙŠØ§Ø¶Ø© ÙˆØ§Ù„ØªØ±ÙÙŠÙ‡\" - For sports, arts, entertainment topics\n",
    "5. \"Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ©\" - For historical, heritage, cultural topics\n",
    "\n",
    "Available Styles (choose one):\n",
    "- \"Ø­ÙˆØ§Ø±ÙŠ\" - Natural friendly dialogue between host and guest\n",
    "- \"ØªØ¹Ù„ÙŠÙ…ÙŠ\" - Focus on explanation and education in entertaining way\n",
    "- \"ØªØ±ÙÙŠÙ‡ÙŠ\" - Fun and light with humorous touches\n",
    "- \"ØªØ­Ù„ÙŠÙ„ÙŠ\" - Deep, specialized analytical discussion\n",
    "\n",
    "Discourse Patterns (choose one):\n",
    "- \"Ø±Ø³Ù…ÙŠ\" - Formal and respectful language\n",
    "- \"ÙˆØ¯ÙŠ\" - Warm and familiar language\n",
    "- \"Ø¬Ø¯Ù„ÙŠ\" - Lively discussion with multiple viewpoints\n",
    "- \"Ø³Ø±Ø¯ÙŠ\" - Storytelling and narrative style\n",
    "\n",
    "Cultural Sensitivity Levels (choose one):\n",
    "- \"Ø¹Ø§Ù„ÙŠ\" - Requires extreme caution in handling\n",
    "- \"Ù…ØªÙˆØ³Ø·\" - Needs moderate cultural consideration\n",
    "- \"Ù…Ù†Ø®ÙØ¶\" - Generally acceptable topic\n",
    "\n",
    "Controversy Potential (choose one):\n",
    "- \"Ø¹Ø§Ù„ÙŠØ©\" - Inherently controversial topic\n",
    "- \"Ù…ØªÙˆØ³Ø·Ø©\" - May generate some disagreements\n",
    "- \"Ù…Ù†Ø®ÙØ¶Ø©\" - Generally acceptable topic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All JSON values must be in Modern Standard Arabic (MSA)\n",
    "- JSON keys must be in English\n",
    "- Use ONLY English commas (,) - NEVER Arabic commas (ØŒ)\n",
    "- Use ONLY standard double quotes (\") - NEVER Arabic quotes\n",
    "- Do NOT include any explanatory text before or after JSON\n",
    "- Do NOT include confidence scores like \"Ø§Ù„Ø«Ù‚Ø©: 95%\"\n",
    "- Do NOT include ```json markers\n",
    "- Return ONLY valid JSON that can be parsed by json.loads()\n",
    "- Analyze the topic deeply considering Arabic cultural context\n",
    "- Focus on what makes the topic appealing to Arabic audiences\n",
    "- Optimal episode duration is 10 minutes\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in analyzing topics for Arabic podcasts. Return ONLY valid JSON with English punctuation. No explanatory text. No confidence scores. No Arabic commas.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3  # Lower temperature for more consistent classification\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Clean the JSON response to ensure it's parseable\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before the first { and after the last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('ØŒ', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove confidence scores and meta text\n",
    "        meta_patterns = [\n",
    "            r'Ø§Ù„Ø«Ù‚Ø©:\\s*\\d+%',\n",
    "            r'Ø§Ù„Ø¯Ù‚Ø©:\\s*\\d+%',\n",
    "            r'Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø«Ù‚Ø©:\\s*\\d+%',\n",
    "            r'\\n.*Ø§Ù„Ø«Ù‚Ø©.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*'\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def classify_with_validation(self, topic, information):\n",
    "        \"\"\"\n",
    "        Classify topic with automatic validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get classification\n",
    "                classification_result = self.classify_topic(topic, information)\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(classification_result)\n",
    "                \n",
    "                # Validate required fields\n",
    "                required_fields = [\n",
    "                    \"primary_category\", \"category_justification\", \"optimal_style\",\n",
    "                    \"discourse_pattern\", \"audience_engagement_goal\", \n",
    "                    \"cultural_sensitivity_level\", \"controversy_potential\",\n",
    "                    \"key_discussion_angles\", \"natural_tension_points\",\n",
    "                    \"cultural_connection_opportunities\"\n",
    "                ]\n",
    "                \n",
    "                missing_fields = [field for field in required_fields if field not in parsed_result]\n",
    "                \n",
    "                if not missing_fields:\n",
    "                    print(f\"âœ… Classification successful on attempt {attempt + 1}\")\n",
    "                    return classification_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Attempt {attempt + 1}: Missing fields: {missing_fields}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âš ï¸ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(classification_result[:500])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"ğŸ“ Using fallback classification...\")\n",
    "        fallback_result = self._get_fallback_classification(topic)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _get_fallback_classification(self, topic):\n",
    "        \"\"\"Provide fallback classification if all attempts fail\"\"\"\n",
    "        return {\n",
    "            \"primary_category\": \"Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\",\n",
    "            \"category_justification\": \"ØªØµÙ†ÙŠÙ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø·Ø±ÙˆØ­\",\n",
    "            \"optimal_style\": \"Ø­ÙˆØ§Ø±ÙŠ\",\n",
    "            \"discourse_pattern\": \"ÙˆØ¯ÙŠ\",\n",
    "            \"audience_engagement_goal\": \"Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ÙˆØ¹ÙŠ ÙˆØ§Ù„ÙÙ‡Ù… Ø­ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\",\n",
    "            \"cultural_sensitivity_level\": \"Ù…ØªÙˆØ³Ø·\",\n",
    "            \"controversy_potential\": \"Ù…ØªÙˆØ³Ø·Ø©\",\n",
    "            \"key_discussion_angles\": [\n",
    "                \"Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹\",\n",
    "                \"Ø§Ù„ØªØ£Ø«ÙŠØ±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠ\"\n",
    "            ],\n",
    "            \"natural_tension_points\": [\n",
    "                \"ÙˆØ¬Ù‡Ø§Øª Ø§Ù„Ù†Ø¸Ø± Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø­ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\",\n",
    "                \"Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ§Ù„ÙØ±Øµ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©\"\n",
    "            ],\n",
    "            \"cultural_connection_opportunities\": [\n",
    "                \"Ø§Ù„Ø±Ø¨Ø· Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©\",\n",
    "                \"Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def analyze_classification_quality(self, parsed_result):\n",
    "        \"\"\"Analyze the quality of the classification result\"\"\"\n",
    "        analysis = {\n",
    "            \"category_appropriateness\": self._assess_category_fit(parsed_result.get(\"primary_category\", \"\")),\n",
    "            \"style_consistency\": self._assess_style_choice(parsed_result.get(\"optimal_style\", \"\")),\n",
    "            \"cultural_awareness\": len(parsed_result.get(\"cultural_connection_opportunities\", [])),\n",
    "            \"discussion_depth\": len(parsed_result.get(\"key_discussion_angles\", [])),\n",
    "            \"sensitivity_awareness\": parsed_result.get(\"cultural_sensitivity_level\", \"\") != \"\",\n",
    "            \"engagement_focus\": parsed_result.get(\"audience_engagement_goal\", \"\") != \"\"\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        score_factors = [\n",
    "            analysis[\"category_appropriateness\"] > 0,\n",
    "            analysis[\"style_consistency\"],\n",
    "            analysis[\"cultural_awareness\"] >= 2,\n",
    "            analysis[\"discussion_depth\"] >= 2,\n",
    "            analysis[\"sensitivity_awareness\"],\n",
    "            analysis[\"engagement_focus\"]\n",
    "        ]\n",
    "        \n",
    "        analysis[\"overall_score\"] = sum(score_factors) * 100 // len(score_factors)\n",
    "        analysis[\"quality_grade\"] = (\n",
    "            \"Ù…Ù…ØªØ§Ø²\" if analysis[\"overall_score\"] >= 85 else\n",
    "            \"Ø¬ÙŠØ¯\" if analysis[\"overall_score\"] >= 70 else\n",
    "            \"Ù…Ù‚Ø¨ÙˆÙ„\" if analysis[\"overall_score\"] >= 55 else\n",
    "            \"ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†\"\n",
    "        )\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _assess_category_fit(self, category):\n",
    "        \"\"\"Assess if the category choice seems appropriate\"\"\"\n",
    "        valid_categories = [\n",
    "            \"Ø§Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§\", \"Ø§Ù„Ø³ÙŠØ§Ø³Ø© ÙˆØ§Ù„Ø´Ø¤ÙˆÙ† Ø§Ù„Ø¹Ø§Ù…Ø©\", \n",
    "            \"Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\", \"Ø§Ù„Ø±ÙŠØ§Ø¶Ø© ÙˆØ§Ù„ØªØ±ÙÙŠÙ‡\", \"Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ©\"\n",
    "        ]\n",
    "        return 1 if category in valid_categories else 0\n",
    "\n",
    "    def _assess_style_choice(self, style):\n",
    "        \"\"\"Assess if the style choice is valid\"\"\"\n",
    "        valid_styles = [\"Ø­ÙˆØ§Ø±ÙŠ\", \"ØªØ¹Ù„ÙŠÙ…ÙŠ\", \"ØªØ±ÙÙŠÙ‡ÙŠ\", \"ØªØ­Ù„ÙŠÙ„ÙŠ\"]\n",
    "        return style in valid_styles\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_topic_classifier(deployment, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the topic classifier with comprehensive validation\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª Testing Topic Classifier with Enhanced Validation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    classifier = TopicClassifier(deployment, model_name)\n",
    "    \n",
    "    # Test topic\n",
    "    topic = \"Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ\"\n",
    "    \n",
    "    information = '''\n",
    "Ù…Ø¹ Ø§Ù†ØªØ´Ø§Ø± ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø³Ø±Ø¹Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ ØªØ²Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø®Ø§ÙˆÙ Ø­ÙˆÙ„ ØªØ£Ø«ÙŠØ±Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© ÙˆØ§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. \n",
    "ØªØ´ÙŠØ± Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø¥Ù„Ù‰ Ø£Ù† 78% Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ø§ ÙŠØªØ¬Ø§ÙˆØ² 3%. \n",
    "Ù…Ø¹Ø¸Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù…Ø¯Ø±Ø¨Ø© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ØºØ±Ø¨ÙŠØ©ØŒ Ù…Ù…Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Øª Ø­ÙˆÙ„ Ù‚Ø¯Ø±ØªÙ‡Ø§ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ.\n",
    "ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ØŒ ØªØ³Ø¹Ù‰ Ø¯ÙˆÙ„ Ù…Ø«Ù„ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ù„ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ø±Ø¨ÙŠØ© Ù…Ø«Ù„ \"Ø¬Ø§ÙŠØ³\" Ùˆ\"Ø§Ù„Ø­ÙˆØ±Ø§Ø¡\" Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø¯ÙŠ.\n",
    "Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø£ÙƒØ¨Ø± ÙŠÙƒÙ…Ù† ÙÙŠ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªÙ‡Ù…ÙŠØ´Ù‡Ø§ØŒ ÙˆØ¶Ù…Ø§Ù† Ø£Ù† ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‚ÙŠÙ…Ù†Ø§ ÙˆÙ…Ø¨Ø§Ø¯Ø¦Ù†Ø§.\n",
    "'''\n",
    "    \n",
    "    # Run classification with validation\n",
    "    classification_result, parsed_result = classifier.classify_with_validation(topic, information)\n",
    "    \n",
    "    print(\"ğŸ“Š Classification Results:\")\n",
    "    print(f\"Primary Category: {parsed_result.get('primary_category', 'N/A')}\")\n",
    "    print(f\"Optimal Style: {parsed_result.get('optimal_style', 'N/A')}\")\n",
    "    print(f\"Discourse Pattern: {parsed_result.get('discourse_pattern', 'N/A')}\")\n",
    "    print(f\"Cultural Sensitivity: {parsed_result.get('cultural_sensitivity_level', 'N/A')}\")\n",
    "    print(f\"Controversy Potential: {parsed_result.get('controversy_potential', 'N/A')}\")\n",
    "    \n",
    "    # Analyze quality\n",
    "    quality_analysis = classifier.analyze_classification_quality(parsed_result)\n",
    "    print(f\"\\nğŸ“ˆ Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Cultural Awareness: {quality_analysis['cultural_awareness']} connections\")\n",
    "    print(f\"Discussion Angles: {quality_analysis['discussion_depth']} angles\")\n",
    "    \n",
    "    # Show key discussion points\n",
    "    print(f\"\\nğŸ¯ Key Discussion Angles:\")\n",
    "    for i, angle in enumerate(parsed_result.get('key_discussion_angles', []), 1):\n",
    "        print(f\"  {i}. {angle}\")\n",
    "    \n",
    "    print(f\"\\nğŸŒ Cultural Connections:\")\n",
    "    for i, connection in enumerate(parsed_result.get('cultural_connection_opportunities', []), 1):\n",
    "        print(f\"  {i}. {connection}\")\n",
    "    \n",
    "    return classification_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# classifier = TopicClassifier(deployment, \"Fanar-C-1-8.7B\")\n",
    "# classification_result, parsed_result = classifier.classify_with_validation(topic, information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bde0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Result:\n",
      "{\n",
      "  \"primary_category\": \"Ø§Ù„Ø¹Ù„ÙˆÙ…_ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§\",\n",
      "  \"category_justification\": \"ÙŠØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ØªØ­Ø¯ÙŠØ§Øª ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¯ÙˆØ±Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠ.\",\n",
      "  \"optimal_style\": \"ØªØ¹Ù„ÙŠÙ…ÙŠ\",\n",
      "  \"discourse_pattern\": \"Ø¬Ø¯Ù„ÙŠ\",\n",
      "  \"audience_engagement_goal\": \"ØªÙˆØ¹ÙŠØ© Ø§Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ† Ø¨Ø£Ù‡Ù…ÙŠØ© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø´ÙƒÙ„ Ù…Ø³Ø¤ÙˆÙ„.\",\n",
      "  \"cultural_sensitivity_level\": \"Ù…ØªÙˆØ³Ø·\",\n",
      "  \"controversy_potential\": \"Ù…ØªÙˆØ³Ø·Ø©\",\n",
      "  \"key_discussion_angles\": [\"ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\", \"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø­Ù„ÙŠØ©\"],\n",
      "  \"natural_tension_points\": [\"Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ù‚ÙˆØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØºØ±Ø¨ÙŠØ© ÙˆÙ†Ù‚Øµ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\", \"Ø¯ÙˆØ± Ø§Ù„Ø­ÙƒÙˆÙ…Ø§Øª ÙˆØ§Ù„Ù…Ø·ÙˆØ±ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ ÙÙŠ ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ\"],\n",
      "  \"cultural_connection_opportunities\": [\"Ù…Ù†Ø§Ù‚Ø´Ø© Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù„ØªÙÙˆÙ‚ Ø§Ù„ÙÙƒØ±ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ\", \"Ø¥Ø¨Ø±Ø§Ø² Ù‚ØµØµ Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„Ù„Ù…Ø¨ØªÙƒØ±ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨\"]\n",
      "}\n",
      "âœ… Category: Ø§Ù„Ø¹Ù„ÙˆÙ…_ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§\n",
      "âœ… Style: ØªØ¹Ù„ÙŠÙ…ÙŠ\n"
     ]
    }
   ],
   "source": [
    "# Testing Instructions:\n",
    "\n",
    "# To test Step 1, add this to a new cell in your notebook:\n",
    "\n",
    "# Test Step 1: Topic Classification\n",
    "\n",
    "# Test with the singlehood topic\n",
    "topic = \"Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ\"\n",
    "\n",
    "information = '''\n",
    "Ù…Ø¹ Ø§Ù†ØªØ´Ø§Ø± ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø³Ø±Ø¹Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ ØªØ²Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø®Ø§ÙˆÙ Ø­ÙˆÙ„ ØªØ£Ø«ÙŠØ±Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© ÙˆØ§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. \n",
    "ØªØ´ÙŠØ± Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø¥Ù„Ù‰ Ø£Ù† 78% Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ø§ ÙŠØªØ¬Ø§ÙˆØ² 3%. \n",
    "Ù…Ø¹Ø¸Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù…Ø¯Ø±Ø¨Ø© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ØºØ±Ø¨ÙŠØ©ØŒ Ù…Ù…Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Øª Ø­ÙˆÙ„ Ù‚Ø¯Ø±ØªÙ‡Ø§ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ.\n",
    "ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ØŒ ØªØ³Ø¹Ù‰ Ø¯ÙˆÙ„ Ù…Ø«Ù„ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ù„ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ø±Ø¨ÙŠØ© Ù…Ø«Ù„ \"Ø¬Ø§ÙŠØ³\" Ùˆ\"Ø§Ù„Ø­ÙˆØ±Ø§Ø¡\" Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø¯ÙŠ.\n",
    "Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø£ÙƒØ¨Ø± ÙŠÙƒÙ…Ù† ÙÙŠ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªÙ‡Ù…ÙŠØ´Ù‡Ø§ØŒ ÙˆØ¶Ù…Ø§Ù† Ø£Ù† ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‚ÙŠÙ…Ù†Ø§ ÙˆÙ…Ø¨Ø§Ø¯Ø¦Ù†Ø§.\n",
    "'''\n",
    "\n",
    "classifier = TopicClassifier(deployment, model)\n",
    "classification_result = classifier.classify_topic(topic, information)\n",
    "print(\"Classification Result:\")\n",
    "print(classification_result)\n",
    "\n",
    "try:\n",
    "    parsed_result = json.loads(classification_result)\n",
    "    print(f\"âœ… Category: {parsed_result['primary_category']}\")\n",
    "    print(f\"âœ… Style: {parsed_result['optimal_style']}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"âŒ JSON parsing failed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9ee26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class SimplePersonaGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def generate_personas(self, topic, information, classification_result):\n",
    "        \"\"\"\n",
    "        Generate simple but effective host and guest personas\n",
    "        \n",
    "        Args:\n",
    "            topic: Main topic of the podcast episode\n",
    "            information: Background information about the topic\n",
    "            classification_result: JSON string from Step 1 classification\n",
    "            \n",
    "        Returns:\n",
    "            JSON with simple host and guest personas\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse classification to understand the requirements\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid classification JSON provided\")\n",
    "        \n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        discourse_pattern = classification.get(\"discourse_pattern\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in designing Arabic podcast personas.\n",
    "\n",
    "Task: Create simple and suitable host and guest personas for this topic.\n",
    "\n",
    "Topic: {topic}\n",
    "Information: {information}\n",
    "Category: {primary_category}\n",
    "Required Style: {optimal_style}\n",
    "Discourse Pattern: {discourse_pattern}\n",
    "Cultural Sensitivity: {cultural_sensitivity}\n",
    "\n",
    "Return the result in this exact JSON format:\n",
    "\n",
    "{{\n",
    "    \"host\": {{\n",
    "        \"name\": \"Host's Arabic name\",\n",
    "        \"age\": numeric_age,\n",
    "        \"background\": \"Brief background in one sentence\",\n",
    "        \"personality\": \"Personality description in one sentence\",\n",
    "        \"speaking_style\": \"Speaking style in one sentence\"\n",
    "    }},\n",
    "    \"guest\": {{\n",
    "        \"name\": \"Guest's Arabic name\", \n",
    "        \"age\": numeric_age,\n",
    "        \"background\": \"Brief background in one sentence\",\n",
    "        \"expertise\": \"Area of expertise in one sentence\",\n",
    "        \"personality\": \"Personality description in one sentence\",\n",
    "        \"speaking_style\": \"Speaking style in one sentence\"\n",
    "    }},\n",
    "    \"why_good_match\": \"Why this host and guest are suitable for this topic - one sentence\"\n",
    "}}\n",
    "\n",
    "Requirements:\n",
    "- Use familiar Arabic names (like Ø£Ø­Ù…Ø¯ØŒ Ù…Ø­Ù…Ø¯ØŒ ÙØ§Ø·Ù…Ø©ØŒ Ù†ÙˆØ±ØŒ Ø¹Ù„ÙŠØŒ Ù„Ù…Ù‰ØŒ Ø³Ø§Ø±Ø©ØŒ Ø®Ø§Ù„Ø¯)\n",
    "- Simple and believable personas\n",
    "- Suitable for the topic and required style: {optimal_style}\n",
    "- Host should be curious and guest should be expert or have experience\n",
    "- All JSON values must be in Modern Standard Arabic (MSA)\n",
    "- JSON keys should be in English\n",
    "- Use ONLY English commas (,) - NEVER Arabic commas (ØŒ)\n",
    "- Use ONLY standard double quotes (\") - NEVER Arabic quotes\n",
    "- Age should be realistic numbers (25-55 range)\n",
    "- Do NOT include ```json markers\n",
    "- Do NOT include confidence scores or extra text\n",
    "- Return only valid JSON\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Host personality should match {optimal_style} style\n",
    "- Guest expertise should be relevant to: {topic}\n",
    "- Consider cultural sensitivity level: {cultural_sensitivity}\n",
    "- Make personas realistic and relatable to Arabic audiences\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are an expert in designing simple and effective podcast personas. Style: {optimal_style}. Always provide JSON values in Modern Standard Arabic while keeping keys in English. No extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Clean the JSON response to ensure it's parseable\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before the first { and after the last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('ØŒ', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove confidence scores and meta text\n",
    "        meta_patterns = [\n",
    "            r'Ø§Ù„Ø«Ù‚Ø©:\\s*\\d+%',\n",
    "            r'Ø§Ù„Ø¯Ù‚Ø©:\\s*\\d+%',\n",
    "            r'Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø«Ù‚Ø©:\\s*\\d+%',\n",
    "            r'\\n.*Ø§Ù„Ø«Ù‚Ø©.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*',\n",
    "            r'Ù…Ù„Ø§Ø­Ø¸Ø©:.*',\n",
    "            r'ØªØ¹Ù„ÙŠÙ‚:.*'\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def generate_personas_with_validation(self, topic, information, classification_result):\n",
    "        \"\"\"\n",
    "        Generate personas with automatic validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get personas\n",
    "                personas_result = self.generate_personas(topic, information, classification_result)\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(personas_result)\n",
    "                \n",
    "                # Validate required fields\n",
    "                validation_result = self._validate_personas(parsed_result)\n",
    "                \n",
    "                if validation_result[\"is_valid\"]:\n",
    "                    print(f\"âœ… Persona generation successful on attempt {attempt + 1}\")\n",
    "                    return personas_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Attempt {attempt + 1}: Validation issues: {validation_result['issues']}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âš ï¸ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(personas_result[:500])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"ğŸ“ Using fallback personas...\")\n",
    "        fallback_result = self._get_fallback_personas(topic, classification_result)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _validate_personas(self, parsed_result):\n",
    "        \"\"\"Validate the generated personas\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check main structure\n",
    "        if \"host\" not in parsed_result:\n",
    "            issues.append(\"Missing host section\")\n",
    "        if \"guest\" not in parsed_result:\n",
    "            issues.append(\"Missing guest section\")\n",
    "        if \"why_good_match\" not in parsed_result:\n",
    "            issues.append(\"Missing why_good_match section\")\n",
    "        \n",
    "        # Check host fields\n",
    "        host = parsed_result.get(\"host\", {})\n",
    "        required_host_fields = [\"name\", \"age\", \"background\", \"personality\", \"speaking_style\"]\n",
    "        for field in required_host_fields:\n",
    "            if field not in host or not host[field]:\n",
    "                issues.append(f\"Missing or empty host.{field}\")\n",
    "        \n",
    "        # Check guest fields\n",
    "        guest = parsed_result.get(\"guest\", {})\n",
    "        required_guest_fields = [\"name\", \"age\", \"background\", \"expertise\", \"personality\", \"speaking_style\"]\n",
    "        for field in required_guest_fields:\n",
    "            if field not in guest or not guest[field]:\n",
    "                issues.append(f\"Missing or empty guest.{field}\")\n",
    "        \n",
    "        # Check age validity\n",
    "        if \"age\" in host:\n",
    "            try:\n",
    "                age = int(host[\"age\"])\n",
    "                if age < 20 or age > 70:\n",
    "                    issues.append(f\"Host age {age} unrealistic (should be 20-70)\")\n",
    "            except (ValueError, TypeError):\n",
    "                issues.append(\"Host age should be a number\")\n",
    "        \n",
    "        if \"age\" in guest:\n",
    "            try:\n",
    "                age = int(guest[\"age\"])\n",
    "                if age < 20 or age > 70:\n",
    "                    issues.append(f\"Guest age {age} unrealistic (should be 20-70)\")\n",
    "            except (ValueError, TypeError):\n",
    "                issues.append(\"Guest age should be a number\")\n",
    "        \n",
    "        # Check for Arabic content\n",
    "        all_text = \" \".join([\n",
    "            str(host.get(\"name\", \"\")), str(host.get(\"background\", \"\")),\n",
    "            str(guest.get(\"name\", \"\")), str(guest.get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"why_good_match\", \"\"))\n",
    "        ])\n",
    "        \n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', all_text))\n",
    "        if arabic_chars < 10:\n",
    "            issues.append(\"Insufficient Arabic content\")\n",
    "        \n",
    "        return {\n",
    "            \"is_valid\": len(issues) == 0,\n",
    "            \"issues\": issues,\n",
    "            \"score\": max(0, 100 - len(issues) * 15)\n",
    "        }\n",
    "\n",
    "    def _get_fallback_personas(self, topic, classification_result):\n",
    "        \"\"\"Provide fallback personas if generation fails\"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            optimal_style = classification.get(\"optimal_style\", \"Ø­ÙˆØ§Ø±ÙŠ\")\n",
    "        except:\n",
    "            optimal_style = \"Ø­ÙˆØ§Ø±ÙŠ\"\n",
    "        \n",
    "        # Determine appropriate personas based on style\n",
    "        if optimal_style == \"ØªØ¹Ù„ÙŠÙ…ÙŠ\":\n",
    "            host_personality = \"Ù…Ù‚Ø¯Ù… Ù…ØªØ­Ù…Ø³ Ù„Ù„ØªØ¹Ù„Ù… ÙˆÙŠØ·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ø¶Ø­Ø©\"\n",
    "            guest_personality = \"Ø®Ø¨ÙŠØ± ØµØ¨ÙˆØ± ÙŠØ´Ø±Ø­ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©\"\n",
    "        elif optimal_style == \"ØªØ­Ù„ÙŠÙ„ÙŠ\":\n",
    "            host_personality = \"Ù…Ù‚Ø¯Ù… Ù…ÙÙƒØ± ÙŠØ·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø¹Ù…ÙŠÙ‚Ø© ÙˆÙ…Ø¯Ø±ÙˆØ³Ø©\"\n",
    "            guest_personality = \"Ù…Ø­Ù„Ù„ Ø®Ø¨ÙŠØ± ÙŠÙ‚Ø¯Ù… Ø±Ø¤Ù‰ Ù…ØªØ®ØµØµØ© ÙˆÙ…Ø¹Ù…Ù‚Ø©\"\n",
    "        elif optimal_style == \"ØªØ±ÙÙŠÙ‡ÙŠ\":\n",
    "            host_personality = \"Ù…Ù‚Ø¯Ù… Ù…Ø±Ø­ ÙˆÙ…ØªÙØ§Ø¹Ù„ ÙŠØ¶ÙŠÙ Ø±ÙˆØ­ Ø§Ù„Ø¯Ø¹Ø§Ø¨Ø©\"\n",
    "            guest_personality = \"Ø¶ÙŠÙ ÙˆØ¯ÙˆØ¯ ÙˆØ·Ø±ÙŠÙ ÙŠØ´Ø§Ø±Ùƒ ØªØ¬Ø§Ø±Ø¨Ù‡ Ø¨Ù…Ø±Ø­\"\n",
    "        else:  # Ø­ÙˆØ§Ø±ÙŠ\n",
    "            host_personality = \"Ù…Ù‚Ø¯Ù… ÙˆØ¯ÙˆØ¯ ÙˆÙØ¶ÙˆÙ„ÙŠ ÙŠØ­Ø¨ Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ\"\n",
    "            guest_personality = \"Ø¶ÙŠÙ Ù…ØªÙØªØ­ ÙˆÙ…ØªØ¹Ø§ÙˆÙ† ÙŠØ´Ø§Ø±Ùƒ Ø®Ø¨Ø±Ø§ØªÙ‡ Ø¨ØµØ±Ø§Ø­Ø©\"\n",
    "        \n",
    "        return {\n",
    "            \"host\": {\n",
    "                \"name\": \"Ø£Ø­Ù…Ø¯ Ø§Ù„Ø³Ø§Ù„Ù…\",\n",
    "                \"age\": 35,\n",
    "                \"background\": \"Ù…Ù‚Ø¯Ù… Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø°Ø§Ø¹ÙŠØ© Ù…Ø¹ Ø®Ø¨Ø±Ø© ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…ØªÙ†ÙˆØ¹Ø©\",\n",
    "                \"personality\": host_personality,\n",
    "                \"speaking_style\": \"ÙŠØªØ­Ø¯Ø« Ø¨ÙˆØ¶ÙˆØ­ ÙˆÙŠØ·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© Ù„Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ø­ÙˆØ§Ø±\"\n",
    "            },\n",
    "            \"guest\": {\n",
    "                \"name\": \"Ù†ÙˆØ± Ø§Ù„Ø¹Ù„ÙŠ\",\n",
    "                \"age\": 40,\n",
    "                \"background\": \"Ø®Ø¨ÙŠØ± ÙˆÙ…Ø®ØªØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø·Ø±ÙˆØ­\",\n",
    "                \"expertise\": \"Ù„Ø¯ÙŠÙ‡ Ù…Ø¹Ø±ÙØ© Ø¹Ù…ÙŠÙ‚Ø© ÙˆØªØ¬Ø±Ø¨Ø© Ø¹Ù…Ù„ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù†Ù‚Ø§Ø´\",\n",
    "                \"personality\": guest_personality,\n",
    "                \"speaking_style\": \"ÙŠØ¹Ø¨Ø± Ø¹Ù† Ø£ÙÙƒØ§Ø±Ù‡ Ø¨ÙˆØ¶ÙˆØ­ ÙˆÙŠØ³ØªØ®Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹\"\n",
    "            },\n",
    "            \"why_good_match\": \"Ø§Ù„Ù…Ù‚Ø¯Ù… ÙŠØ¬ÙŠØ¯ Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¶ÙŠÙ Ù„Ø¯ÙŠÙ‡ Ø§Ù„Ø®Ø¨Ø±Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ÙØ¹Ø§Ù„ÙŠØ©\"\n",
    "        }\n",
    "\n",
    "    def analyze_persona_quality(self, parsed_result):\n",
    "        \"\"\"Analyze the quality of generated personas\"\"\"\n",
    "        analysis = {\n",
    "            \"name_authenticity\": self._assess_arabic_names(parsed_result),\n",
    "            \"age_realism\": self._assess_age_realism(parsed_result),\n",
    "            \"background_relevance\": self._assess_background_relevance(parsed_result),\n",
    "            \"personality_distinctiveness\": self._assess_personality_distinctiveness(parsed_result),\n",
    "            \"style_alignment\": self._assess_style_alignment(parsed_result),\n",
    "            \"cultural_appropriateness\": self._assess_cultural_appropriateness(parsed_result)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        score_factors = [\n",
    "            analysis[\"name_authenticity\"],\n",
    "            analysis[\"age_realism\"],\n",
    "            analysis[\"background_relevance\"] > 0,\n",
    "            analysis[\"personality_distinctiveness\"],\n",
    "            analysis[\"style_alignment\"] > 0,\n",
    "            analysis[\"cultural_appropriateness\"]\n",
    "        ]\n",
    "        \n",
    "        analysis[\"overall_score\"] = sum(score_factors) * 100 // len(score_factors)\n",
    "        analysis[\"quality_grade\"] = (\n",
    "            \"Ù…Ù…ØªØ§Ø²\" if analysis[\"overall_score\"] >= 85 else\n",
    "            \"Ø¬ÙŠØ¯\" if analysis[\"overall_score\"] >= 70 else\n",
    "            \"Ù…Ù‚Ø¨ÙˆÙ„\" if analysis[\"overall_score\"] >= 55 else\n",
    "            \"ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†\"\n",
    "        )\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _assess_arabic_names(self, parsed_result):\n",
    "        \"\"\"Check if names are authentic Arabic names\"\"\"\n",
    "        host_name = parsed_result.get(\"host\", {}).get(\"name\", \"\")\n",
    "        guest_name = parsed_result.get(\"guest\", {}).get(\"name\", \"\")\n",
    "        \n",
    "        common_names = [\n",
    "            \"Ø£Ø­Ù…Ø¯\", \"Ù…Ø­Ù…Ø¯\", \"Ø¹Ù„ÙŠ\", \"Ø®Ø§Ù„Ø¯\", \"Ø¹Ù…Ø±\", \"ÙŠÙˆØ³Ù\", \"Ø­Ø³Ù†\", \"ÙƒØ±ÙŠÙ…\",\n",
    "            \"ÙØ§Ø·Ù…Ø©\", \"Ø¹Ø§Ø¦Ø´Ø©\", \"Ù†ÙˆØ±\", \"Ù„Ù…Ù‰\", \"Ø³Ø§Ø±Ø©\", \"Ù…Ø±ÙŠÙ…\", \"Ø²ÙŠÙ†Ø¨\", \"Ø±Ù‚ÙŠØ©\"\n",
    "        ]\n",
    "        \n",
    "        host_authentic = any(name in host_name for name in common_names)\n",
    "        guest_authentic = any(name in guest_name for name in common_names)\n",
    "        \n",
    "        return host_authentic and guest_authentic\n",
    "\n",
    "    def _assess_age_realism(self, parsed_result):\n",
    "        \"\"\"Check if ages are realistic\"\"\"\n",
    "        try:\n",
    "            host_age = int(parsed_result.get(\"host\", {}).get(\"age\", 0))\n",
    "            guest_age = int(parsed_result.get(\"guest\", {}).get(\"age\", 0))\n",
    "            return 25 <= host_age <= 55 and 25 <= guest_age <= 65\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _assess_background_relevance(self, parsed_result):\n",
    "        \"\"\"Assess if backgrounds are relevant and detailed\"\"\"\n",
    "        host_bg = parsed_result.get(\"host\", {}).get(\"background\", \"\")\n",
    "        guest_bg = parsed_result.get(\"guest\", {}).get(\"background\", \"\")\n",
    "        guest_exp = parsed_result.get(\"guest\", {}).get(\"expertise\", \"\")\n",
    "        \n",
    "        relevance_score = 0\n",
    "        if len(host_bg) > 20:\n",
    "            relevance_score += 1\n",
    "        if len(guest_bg) > 20:\n",
    "            relevance_score += 1\n",
    "        if len(guest_exp) > 20:\n",
    "            relevance_score += 1\n",
    "        \n",
    "        return relevance_score\n",
    "\n",
    "    def _assess_personality_distinctiveness(self, parsed_result):\n",
    "        \"\"\"Check if host and guest have distinct personalities\"\"\"\n",
    "        host_personality = parsed_result.get(\"host\", {}).get(\"personality\", \"\")\n",
    "        guest_personality = parsed_result.get(\"guest\", {}).get(\"personality\", \"\")\n",
    "        \n",
    "        # Simple check: personalities should be different\n",
    "        similarity = len(set(host_personality.split()) & set(guest_personality.split()))\n",
    "        total_words = len(set(host_personality.split()) | set(guest_personality.split()))\n",
    "        \n",
    "        return similarity / total_words < 0.5 if total_words > 0 else False\n",
    "\n",
    "    def _assess_style_alignment(self, parsed_result):\n",
    "        \"\"\"Assess if personas align with the intended style\"\"\"\n",
    "        host_style = parsed_result.get(\"host\", {}).get(\"speaking_style\", \"\")\n",
    "        guest_style = parsed_result.get(\"guest\", {}).get(\"speaking_style\", \"\")\n",
    "        \n",
    "        style_indicators = {\n",
    "            \"Ø­ÙˆØ§Ø±ÙŠ\": [\"ÙˆØ¯ÙˆØ¯\", \"Ø·Ø¨ÙŠØ¹ÙŠ\", \"ØªÙØ§Ø¹Ù„\", \"Ø­ÙˆØ§Ø±\"],\n",
    "            \"ØªØ¹Ù„ÙŠÙ…ÙŠ\": [\"ØªØ¹Ù„ÙŠÙ…\", \"Ø´Ø±Ø­\", \"ØªÙˆØ¶ÙŠØ­\", \"ØªØ¨Ø³ÙŠØ·\"],\n",
    "            \"ØªØ­Ù„ÙŠÙ„ÙŠ\": [\"ØªØ­Ù„ÙŠÙ„\", \"Ø¹Ù…Ù‚\", \"ØªØ®ØµØµ\", \"Ø¯Ù‚Ø©\"],\n",
    "            \"ØªØ±ÙÙŠÙ‡ÙŠ\": [\"Ù…Ø±Ø­\", \"Ø¯Ø¹Ø§Ø¨Ø©\", \"ØªØ±ÙÙŠÙ‡\", \"Ø®ÙØ©\"]\n",
    "        }\n",
    "        \n",
    "        # This is a simplified assessment\n",
    "        return len(host_style) > 15 and len(guest_style) > 15\n",
    "\n",
    "    def _assess_cultural_appropriateness(self, parsed_result):\n",
    "        \"\"\"Check if personas are culturally appropriate\"\"\"\n",
    "        all_content = \" \".join([\n",
    "            str(parsed_result.get(\"host\", {}).get(\"name\", \"\")),\n",
    "            str(parsed_result.get(\"host\", {}).get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"guest\", {}).get(\"name\", \"\")),\n",
    "            str(parsed_result.get(\"guest\", {}).get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"why_good_match\", \"\"))\n",
    "        ])\n",
    "        \n",
    "        # Check for Arabic content\n",
    "        arabic_ratio = len(re.findall(r'[\\u0600-\\u06FF]', all_content)) / len(all_content) if all_content else 0\n",
    "        return arabic_ratio > 0.3\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_persona_generator(deployment, topic, information, classification_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the persona generator with comprehensive validation\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª Testing Persona Generator with Enhanced Validation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = SimplePersonaGenerator(deployment, model_name)\n",
    "    \n",
    "    # Run persona generation with validation\n",
    "    personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)\n",
    "    \n",
    "    print(\"ğŸ‘¥ Generated Personas:\")\n",
    "    host = parsed_result.get(\"host\", {})\n",
    "    guest = parsed_result.get(\"guest\", {})\n",
    "    \n",
    "    print(f\"\\nğŸ¤ Host: {host.get('name', 'N/A')} (Ø¹Ù…Ø± {host.get('age', 'N/A')})\")\n",
    "    print(f\"   Ø§Ù„Ø®Ù„ÙÙŠØ©: {host.get('background', 'N/A')}\")\n",
    "    print(f\"   Ø§Ù„Ø´Ø®ØµÙŠØ©: {host.get('personality', 'N/A')}\")\n",
    "    print(f\"   Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø­Ø¯ÙŠØ«: {host.get('speaking_style', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Guest: {guest.get('name', 'N/A')} (Ø¹Ù…Ø± {guest.get('age', 'N/A')})\")\n",
    "    print(f\"   Ø§Ù„Ø®Ù„ÙÙŠØ©: {guest.get('background', 'N/A')}\")\n",
    "    print(f\"   Ø§Ù„Ø®Ø¨Ø±Ø©: {guest.get('expertise', 'N/A')}\")\n",
    "    print(f\"   Ø§Ù„Ø´Ø®ØµÙŠØ©: {guest.get('personality', 'N/A')}\")\n",
    "    print(f\"   Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø­Ø¯ÙŠØ«: {guest.get('speaking_style', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¤ Why Good Match: {parsed_result.get('why_good_match', 'N/A')}\")\n",
    "    \n",
    "    # Analyze quality\n",
    "    quality_analysis = generator.analyze_persona_quality(parsed_result)\n",
    "    print(f\"\\nğŸ“ˆ Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Name Authenticity: {'âœ…' if quality_analysis['name_authenticity'] else 'âŒ'}\")\n",
    "    print(f\"Age Realism: {'âœ…' if quality_analysis['age_realism'] else 'âŒ'}\")\n",
    "    print(f\"Background Relevance: {quality_analysis['background_relevance']}/3\")\n",
    "    print(f\"Personality Distinctiveness: {'âœ…' if quality_analysis['personality_distinctiveness'] else 'âŒ'}\")\n",
    "    \n",
    "    return personas_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# generator = SimplePersonaGenerator(deployment, \"Fanar-C-1-8.7B\")\n",
    "# personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f699140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Persona generation successful on attempt 1\n",
      "Personas Result:\n",
      "{\n",
      "  \"host\": {\n",
      "    \"name\": \"Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ\",\n",
      "    \"age\": 40,\n",
      "    \"background\": \"Ù…Ù‚Ø¯Ù… Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø°Ø§Ø¹ÙŠØ© Ù…Ø¹Ø±ÙˆÙ Ø¨Ø´ØºÙÙ‡ Ø¨Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\",\n",
      "    \"personality\": \"Ø´Ø®Øµ Ù…Ø±Ø­ ÙˆÙ…ØªØ­Ù…Ø³ ÙŠØ³Ø¹Ù‰ Ù„ÙÙ‡Ù… Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§ØµØ±Ø©\",\n",
      "    \"speaking_style\": \"Ù…ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± ÙˆÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ø´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ØªÙ‚Ù†ÙŠØ©\"\n",
      "  },\n",
      "  \"guest\": {\n",
      "    \"name\": \"Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯\",\n",
      "    \"age\": 38,\n",
      "    \"background\": \"Ø¨Ø§Ø­Ø«Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ‡ Ø§Ù„Ù„ØºÙˆÙŠØ© ÙˆØ§Ù„Ø«Ù‚Ø§ÙÙŠØ©\",\n",
      "    \"expertise\": \"ØªØ¨Ø­Ø« Ø¹Ù† Ø·Ø±Ù‚ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ³ÙŠØ§Ù‚Ù‡Ø§ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\",\n",
      "    \"personality\": \"Ø¹Ø§Ù„Ù…Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ù†Ø·Ù‚ÙŠØ© Ù„ÙƒÙ† Ù„Ù‡Ø§ Ø­Ø³ Ø¥Ù†Ø³Ø§Ù†ÙŠ Ù‚ÙˆÙŠ\",\n",
      "    \"speaking_style\": \"ØªÙˆØ¶ÙŠØ­ Ø¹Ù„Ù…ÙŠ Ø¯Ù‚ÙŠÙ‚ Ù…Ù…Ø²ÙˆØ¬ Ø¨ØªÙˆØ¶ÙŠØ­Ø§Øª Ø¹Ù…Ù„ÙŠØ© ÙˆØ´Ø±Ø­ Ù„Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„Ù…Ù†Ø§ÙØ¹\"\n",
      "  },\n",
      "  \"why_good_match\": \"Ø£Ø­Ù…Ø¯ ÙˆÙØ§Ø·Ù† Ù…Ù†Ø§Ø³Ø¨Ø§Ù† Ù„Ù„Ù†Ù‚Ø§Ø´ Ù„Ø£Ù† Ø§Ù„Ø£ÙˆÙ„ Ù„Ø¯ÙŠÙ‡ ÙØ¶ÙˆÙ„ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆØ«Ù‚Ø§ÙÙŠ ÙˆÙŠÙÙ‡Ù…Ù‡Ù…Ø§ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø£Ù…Ø§ Ø§Ù„Ø«Ø§Ù†ÙŠØ© ÙÙ‡ÙŠ Ø®Ø¨ÙŠØ±Ø© ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ© ÙˆÙ…Ø¹Ø±ÙÙŠØ© Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… ÙˆØ¬Ù‡Ø© Ù†Ø¸Ø± Ø¹Ù„Ù…ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø©.\"\n",
      "}\n",
      "Host: Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ (40 years)\n",
      "Guest: Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯ (38 years)\n"
     ]
    }
   ],
   "source": [
    "generator = SimplePersonaGenerator(deployment, model)\n",
    "personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)\n",
    "print(\"Personas Result:\")\n",
    "print(personas_result)\n",
    "    # Results are guaranteed to be valid JSON with all required fields\n",
    "host = parsed_result['host']\n",
    "guest = parsed_result['guest']\n",
    "print(f\"Host: {host['name']} ({host['age']} years)\")\n",
    "print(f\"Guest: {guest['name']} ({guest['age']} years)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab52d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class FixedConversationStructureGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def generate_conversation_structure(self, topic, information, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Step 3: Generate core conversation structure with Arabic-only content\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse inputs\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided for classification or personas\")\n",
    "        \n",
    "        # Extract key info\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        discourse_pattern = classification.get(\"discourse_pattern\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "        host_background = host.get('background', '')\n",
    "        guest_background = guest.get('background', '')\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in designing conversation structures for Arabic podcasts.\n",
    "\n",
    "CRITICAL LANGUAGE REQUIREMENTS:\n",
    "- Use ONLY Arabic language (Modern Standard Arabic)\n",
    "- NO English words, phrases, or sentences\n",
    "- NO Chinese, Japanese, or any other foreign languages\n",
    "- NO foreign characters, symbols, or punctuation\n",
    "- Arabic text ONLY with standard JSON punctuation (, : \" {{ }})\n",
    "\n",
    "Task: Create a conversation structure for this Arabic podcast episode.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "\n",
    "Generate ONLY this JSON structure with Arabic content:\n",
    "\n",
    "{{\n",
    "    \"episode_topic\": \"Arabic episode topic here\",\n",
    "    \"personas\": {{\n",
    "        \"host\": {{\n",
    "            \"name\": \"{host_name}\",\n",
    "            \"background\": \"Arabic background description\",\n",
    "            \"speaking_style\": \"Arabic speaking style description\"\n",
    "        }},\n",
    "        \"guest\": {{\n",
    "            \"name\": \"{guest_name}\",\n",
    "            \"background\": \"Arabic background description\",\n",
    "            \"speaking_style\": \"Arabic speaking style description\"\n",
    "        }}\n",
    "    }},\n",
    "    \"conversation_flow\": {{\n",
    "        \"intro1\": {{\n",
    "            \"opening_line\": \"Arabic opening line for host\",\n",
    "            \"podcast_introduction\": \"Arabic podcast introduction\",\n",
    "            \"episode_hook\": \"Arabic engaging hook about topic\"\n",
    "        }},\n",
    "        \"intro2\": {{\n",
    "            \"topic_introduction\": \"Arabic topic introduction\",\n",
    "            \"guest_welcome\": \"Arabic welcome message for guest\",\n",
    "            \"guest_bio_highlight\": \"Arabic guest background highlight\"\n",
    "        }},\n",
    "        \"main_discussion\": [\n",
    "            {{\n",
    "                \"point_title\": \"Arabic first discussion point\",\n",
    "                \"personal_angle\": \"Arabic personal connection\"\n",
    "            }},\n",
    "            {{\n",
    "                \"point_title\": \"Arabic second discussion point\", \n",
    "                \"personal_angle\": \"Arabic personal angle\"\n",
    "            }},\n",
    "            {{\n",
    "                \"point_title\": \"Arabic third discussion point\",\n",
    "                \"personal_angle\": \"Arabic concluding angle\"\n",
    "            }}\n",
    "        ],\n",
    "        \"closing\": {{\n",
    "            \"conclusion\": {{\n",
    "                \"main_takeaways\": \"Arabic main takeaways\",\n",
    "                \"guest_final_message\": \"Arabic guest final message\",\n",
    "                \"host_closing_thoughts\": \"Arabic host closing thoughts\"\n",
    "            }},\n",
    "            \"outro\": {{\n",
    "                \"guest_appreciation\": \"Arabic thank guest message\",\n",
    "                \"audience_thanks\": \"Arabic thank audience message\",\n",
    "                \"call_to_action\": \"Arabic call for engagement\",\n",
    "                \"final_goodbye\": \"Arabic final goodbye\"\n",
    "            }}\n",
    "        }}\n",
    "    }},\n",
    "    \"cultural_context\": {{\n",
    "        \"proverbs_sayings\": [\n",
    "            \"Arabic proverb related to topic\",\n",
    "            \"Arabic wisdom saying\"\n",
    "        ],\n",
    "        \"regional_references\": [\n",
    "            \"Arabic local reference related to topic\",\n",
    "            \"Arabic regional experience\"\n",
    "        ]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENTS:\n",
    "- Use ONLY English commas (,) not Arabic commas (ØŒ)\n",
    "- Use ONLY standard double quotes (\") not Arabic quotes\n",
    "- Return ONLY the JSON structure above\n",
    "- NO explanatory text before or after JSON\n",
    "- NO confidence scores or meta-text\n",
    "- NO ```json markers or code blocks\n",
    "- All Arabic text must be grammatically correct MSA\n",
    "\n",
    "Replace all placeholder text with actual Arabic content specific to the topic: {topic}\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Generate Arabic podcast conversation structure. Return ONLY valid JSON with Arabic content. NO foreign languages. Use English JSON punctuation only.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3  # Lower temperature for more predictable output\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Enhanced Arabic-only JSON cleaning method\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents for JSON\n",
    "        clean_json = clean_json.replace('ØŒ', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove foreign language characters (Chinese, Japanese, etc.)\n",
    "        # Keep only Arabic Unicode ranges + basic JSON syntax\n",
    "        foreign_patterns = [\n",
    "            r'[\\u4e00-\\u9fff]',  # Chinese characters\n",
    "            r'[\\u3040-\\u309f]',  # Hiragana\n",
    "            r'[\\u30a0-\\u30ff]',  # Katakana\n",
    "            r'[\\u3000-\\u303f]',  # Japanese punctuation\n",
    "            r'[\\uff00-\\uffef]',  # Fullwidth characters\n",
    "            r'[\\u2000-\\u206f]',  # General punctuation (some problematic ones)\n",
    "        ]\n",
    "        \n",
    "        for pattern in foreign_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json)\n",
    "        \n",
    "        # Remove specific problematic characters we've seen\n",
    "        problematic_chars = ['åƒ', 'æµ®', 'èµ·', 'æ', 'æ‚¨', 'è¶³', 'äº', 'ã€', 'ï¼', 'Indeed', 'how']\n",
    "        for char in problematic_chars:\n",
    "            clean_json = clean_json.replace(char, '')\n",
    "        \n",
    "        # Remove English words mixed in Arabic text (basic detection)\n",
    "        # This is a simple approach - remove common English words found in previous outputs\n",
    "        english_words = [\n",
    "            'Translation:', 'Hello', 'everyone', 'welcome', 'back', 'Indeed',\n",
    "            'how', 'preserving', 'culture', 'and', 'identity', 'in', 'digital', 'age',\n",
    "            'NLP', 'Ä‘á»ƒ', 'tweaking', 'idees', 'AbdulRahman', 'Fatima'\n",
    "        ]\n",
    "        \n",
    "        for word in english_words:\n",
    "            clean_json = clean_json.replace(word, '')\n",
    "        \n",
    "        # Remove meta-text patterns\n",
    "        meta_patterns = [\n",
    "            r'Ø§Ù„Ø«Ù‚Ø©:\\s*\\d+%',\n",
    "            r'Ø§Ù„Ø¯Ù‚Ø©:\\s*\\d+%',\n",
    "            r'Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø«Ù‚Ø©:\\s*\\d+%',\n",
    "            r'\\n.*Ø§Ù„Ø«Ù‚Ø©.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*',\n",
    "            r'Ù…Ù„Ø§Ø­Ø¸Ø©:.*',\n",
    "            r'ØªØ¹Ù„ÙŠÙ‚:.*',\n",
    "            r'\\(Translation:.*?\\)',\n",
    "            r'\\*\\*.*?\\*\\*',  # Remove markdown bold\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Fix missing commas between properties\n",
    "        clean_json = re.sub(r'\"\\s*\\n\\s*\"', '\",\\n\"', clean_json)\n",
    "        \n",
    "        # Remove multiple spaces\n",
    "        clean_json = re.sub(r'\\s+', ' ', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def _validate_arabic_only(self, text):\n",
    "        \"\"\"Validate that text contains only Arabic and basic punctuation\"\"\"\n",
    "        if not text:\n",
    "            return False, \"Empty text\"\n",
    "        \n",
    "        # Check for foreign language characters\n",
    "        foreign_patterns = [\n",
    "            (r'[\\u4e00-\\u9fff]', \"Chinese characters detected\"),\n",
    "            (r'[\\u3040-\\u309f\\u30a0-\\u30ff]', \"Japanese characters detected\"),\n",
    "            (r'\\b[a-zA-Z]{2,}\\b', \"English words detected\"),\n",
    "            (r'[åƒæµ®èµ·ææ‚¨è¶³äºã€ï¼]', \"Specific foreign characters detected\")\n",
    "        ]\n",
    "        \n",
    "        for pattern, message in foreign_patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return False, message\n",
    "        \n",
    "        # Check for minimum Arabic content\n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "        total_chars = len(re.sub(r'[\\s\\{\\}\",:\\[\\]]', '', text))  # Exclude JSON syntax\n",
    "        \n",
    "        if total_chars > 0:\n",
    "            arabic_ratio = arabic_chars / total_chars\n",
    "            if arabic_ratio < 0.8:  # At least 80% Arabic\n",
    "                return False, f\"Insufficient Arabic content: {arabic_ratio:.2%}\"\n",
    "        \n",
    "        return True, \"Arabic validation successful\"\n",
    "\n",
    "    def generate_conversation_structure_with_validation(self, topic, information, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Generate conversation structure with Arabic-only validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get structure\n",
    "                structure_result = self.generate_conversation_structure(topic, information, classification_result, personas_result)\n",
    "                \n",
    "                # Validate Arabic-only content\n",
    "                is_arabic_valid, arabic_message = self._validate_arabic_only(structure_result)\n",
    "                if not is_arabic_valid:\n",
    "                    print(f\"âš ï¸ Attempt {attempt + 1}: Arabic validation failed: {arabic_message}\")\n",
    "                    continue\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(structure_result)\n",
    "                \n",
    "                # Validate structure completeness\n",
    "                is_structure_valid, structure_message = self.validate_conversation_structure(structure_result)\n",
    "                \n",
    "                if is_structure_valid:\n",
    "                    print(f\"âœ… Conversation structure generation successful on attempt {attempt + 1}\")\n",
    "                    return structure_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Attempt {attempt + 1}: Structure validation failed: {structure_message}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âš ï¸ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(structure_result[:300] + \"...\" if len(structure_result) > 300 else structure_result)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"ğŸ“ Using fallback conversation structure...\")\n",
    "        fallback_result = self._get_fallback_structure(topic, classification_result, personas_result)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _get_fallback_structure(self, topic, classification_result, personas_result):\n",
    "        \"\"\"Provide Arabic-only fallback conversation structure\"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "            optimal_style = classification.get(\"optimal_style\", \"Ø­ÙˆØ§Ø±ÙŠ\")\n",
    "        except:\n",
    "            optimal_style = \"Ø­ÙˆØ§Ø±ÙŠ\"\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "        \n",
    "        return {\n",
    "            \"episode_topic\": f\"Ù†Ù‚Ø§Ø´ Ø­ÙˆÙ„ {topic}\",\n",
    "            \"personas\": {\n",
    "                \"host\": {\n",
    "                    \"name\": host_name,\n",
    "                    \"background\": host.get('background', 'Ù…Ù‚Ø¯Ù… Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø°Ø§Ø¹ÙŠØ© Ù…ØªØ®ØµØµ'),\n",
    "                    \"speaking_style\": host.get('speaking_style', 'ÙŠØªØ­Ø¯Ø« Ø¨ÙˆØ¶ÙˆØ­ ÙˆÙŠØ·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…Ø¯Ø±ÙˆØ³Ø©')\n",
    "                },\n",
    "                \"guest\": {\n",
    "                    \"name\": guest_name,\n",
    "                    \"background\": guest.get('background', 'Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹'),\n",
    "                    \"speaking_style\": guest.get('speaking_style', 'ÙŠØ´Ø±Ø­ Ø¨ÙˆØ¶ÙˆØ­ ÙˆÙŠÙ‚Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©')\n",
    "                }\n",
    "            },\n",
    "            \"conversation_flow\": {\n",
    "                \"intro1\": {\n",
    "                    \"opening_line\": f\"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù…ØŒ Ù…Ø¹ÙƒÙ… {host_name} ÙÙŠ Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©\",\n",
    "                    \"podcast_introduction\": \"Ù†Ù†Ø§Ù‚Ø´ Ø§Ù„ÙŠÙˆÙ… Ù…ÙˆØ¶ÙˆØ¹Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹ ÙŠÙ‡Ù… Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙˆÙŠØ³ØªØ­Ù‚ Ø§Ù„ØªØ£Ù…Ù„\",\n",
    "                    \"episode_hook\": f\"Ù…ÙˆØ¶ÙˆØ¹ Ø­Ù„Ù‚ØªÙ†Ø§ Ø§Ù„ÙŠÙˆÙ… Ù‡Ùˆ {topic} ÙˆØ£Ø«Ø±Ù‡ Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙ†Ø§\"\n",
    "                },\n",
    "                \"intro2\": {\n",
    "                    \"topic_introduction\": f\"Ø³Ù†ØªØ­Ø¯Ø« Ø§Ù„ÙŠÙˆÙ… Ø¹Ù† {topic} ÙˆØ¬ÙˆØ§Ù†Ø¨Ù‡ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ù…Ù‡Ù…Ø©\",\n",
    "                    \"guest_welcome\": f\"Ù…Ø¹ÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø¶ÙŠÙ Ø§Ù„Ù…ØªÙ…ÙŠØ² {guest_name}ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ\",\n",
    "                    \"guest_bio_highlight\": f\"{guest_name} Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ ÙˆÙ„Ø¯ÙŠÙ‡ Ø®Ø¨Ø±Ø© ÙˆØ§Ø³Ø¹Ø©\"\n",
    "                },\n",
    "                \"main_discussion\": [\n",
    "                    {\n",
    "                        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹\",\n",
    "                        \"personal_angle\": \"ÙƒÙŠÙ ÙŠØ¤Ø«Ø± Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙ†Ø§ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙˆØªØ¬Ø§Ø±Ø¨Ù†Ø§\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©\",\n",
    "                        \"personal_angle\": \"Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ§Ù„ÙØ±Øµ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù„Ø« ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©\",\n",
    "                        \"personal_angle\": \"Ø§Ù„Ù†ØµØ§Ø¦Ø­ ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„\"\n",
    "                    }\n",
    "                ],\n",
    "                \"closing\": {\n",
    "                    \"conclusion\": {\n",
    "                        \"main_takeaways\": \"Ø§Ù„Ø®Ù„Ø§ØµØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ù†Ù‚Ø§Ø´Ù†Ø§ Ø§Ù„ÙŠÙˆÙ…\",\n",
    "                        \"guest_final_message\": \"Ø±Ø³Ø§Ù„Ø© Ø£Ø®ÙŠØ±Ø© ÙˆÙ…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ø¶ÙŠÙ Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ†\",\n",
    "                        \"host_closing_thoughts\": \"Ø£ÙÙƒØ§Ø± Ø®ØªØ§Ù…ÙŠØ© ÙˆØªØ£Ù…Ù„Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù‚Ø¯Ù… Ø­ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\"\n",
    "                    },\n",
    "                    \"outro\": {\n",
    "                        \"guest_appreciation\": f\"Ø´ÙƒØ±Ø§Ù‹ Ø¬Ø²ÙŠÙ„Ø§Ù‹ {guest_name} Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ù…ÙÙŠØ¯ ÙˆØ§Ù„Ø«Ø±ÙŠ\",\n",
    "                        \"audience_thanks\": \"Ø´ÙƒØ±Ø§Ù‹ Ù„ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù… Ø¹Ù„Ù‰ Ù…ØªØ§Ø¨Ø¹ØªÙƒÙ… ÙˆØ§Ù‡ØªÙ…Ø§Ù…ÙƒÙ…\",\n",
    "                        \"call_to_action\": \"ØªÙØ§Ø¹Ù„ÙˆØ§ Ù…Ø¹Ù†Ø§ ÙˆØ´Ø§Ø±ÙƒÙˆÙ†Ø§ Ø¢Ø±Ø§Ø¡ÙƒÙ… Ø¹Ø¨Ø± ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ\",\n",
    "                        \"final_goodbye\": \"Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡ ÙÙŠ Ø­Ù„Ù‚Ø© Ù‚Ø§Ø¯Ù…Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"cultural_context\": {\n",
    "                \"proverbs_sayings\": [\n",
    "                    \"Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…\",\n",
    "                    \"ÙÙŠ Ø§Ù„ØªØ£Ù†ÙŠ Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆÙÙŠ Ø§Ù„Ø¹Ø¬Ù„Ø© Ø§Ù„Ù†Ø¯Ø§Ù…Ø©\"\n",
    "                ],\n",
    "                \"regional_references\": [\n",
    "                    \"Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØºÙ†ÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\",\n",
    "                    \"Ø§Ù„Ø®Ø¨Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ù„Ø¥Ù‚Ù„ÙŠÙ…ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def validate_conversation_structure(self, structure_json):\n",
    "        \"\"\"Enhanced validation for conversation structure\"\"\"\n",
    "        required_keys = [\"episode_topic\", \"personas\", \"conversation_flow\", \"cultural_context\"]\n",
    "        \n",
    "        conversation_flow_required = [\"intro1\", \"intro2\", \"main_discussion\", \"closing\"]\n",
    "        intro1_required = [\"opening_line\", \"podcast_introduction\", \"episode_hook\"]\n",
    "        intro2_required = [\"topic_introduction\", \"guest_welcome\", \"guest_bio_highlight\"]\n",
    "        \n",
    "        try:\n",
    "            structure = json.loads(structure_json)\n",
    "            missing_keys = []\n",
    "            \n",
    "            # Check main structure\n",
    "            for key in required_keys:\n",
    "                if key not in structure:\n",
    "                    missing_keys.append(key)\n",
    "            \n",
    "            # Check conversation flow\n",
    "            if \"conversation_flow\" in structure:\n",
    "                conv_flow = structure[\"conversation_flow\"]\n",
    "                for key in conversation_flow_required:\n",
    "                    if key not in conv_flow:\n",
    "                        missing_keys.append(f\"conversation_flow.{key}\")\n",
    "                \n",
    "                # Check intro1\n",
    "                if \"intro1\" in conv_flow:\n",
    "                    intro1 = conv_flow[\"intro1\"]\n",
    "                    for key in intro1_required:\n",
    "                        if key not in intro1:\n",
    "                            missing_keys.append(f\"intro1.{key}\")\n",
    "                \n",
    "                # Check intro2\n",
    "                if \"intro2\" in conv_flow:\n",
    "                    intro2 = conv_flow[\"intro2\"]\n",
    "                    for key in intro2_required:\n",
    "                        if key not in intro2:\n",
    "                            missing_keys.append(f\"intro2.{key}\")\n",
    "                \n",
    "                # Check main discussion\n",
    "                if \"main_discussion\" in conv_flow:\n",
    "                    main_disc = conv_flow[\"main_discussion\"]\n",
    "                    if not isinstance(main_disc, list) or len(main_disc) < 3:\n",
    "                        missing_keys.append(\"main_discussion (need at least 3 points)\")\n",
    "                    else:\n",
    "                        for i, point in enumerate(main_disc):\n",
    "                            if \"point_title\" not in point:\n",
    "                                missing_keys.append(f\"main_discussion[{i}].point_title\")\n",
    "                            if \"personal_angle\" not in point:\n",
    "                                missing_keys.append(f\"main_discussion[{i}].personal_angle\")\n",
    "            \n",
    "            if missing_keys:\n",
    "                return False, f\"Missing required keys: {missing_keys}\"\n",
    "            \n",
    "            return True, \"Conversation structure validation successful\"\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            return False, f\"Invalid JSON format: {e}\"\n",
    "\n",
    "    def analyze_structure_quality(self, structure_json):\n",
    "        \"\"\"Enhanced quality analysis with Arabic-only validation\"\"\"\n",
    "        try:\n",
    "            structure = json.loads(structure_json)\n",
    "            \n",
    "            analysis = {\n",
    "                \"structure_completeness\": 0,\n",
    "                \"content_quality\": 0,\n",
    "                \"cultural_integration\": 0,\n",
    "                \"arabic_purity\": 0\n",
    "            }\n",
    "            \n",
    "            # Check completeness\n",
    "            conv_flow = structure.get(\"conversation_flow\", {})\n",
    "            completeness_indicators = [\n",
    "                bool(conv_flow.get(\"intro1\")),\n",
    "                bool(conv_flow.get(\"intro2\")),\n",
    "                bool(conv_flow.get(\"main_discussion\")),\n",
    "                bool(conv_flow.get(\"closing\")),\n",
    "                len(conv_flow.get(\"main_discussion\", [])) >= 3\n",
    "            ]\n",
    "            analysis[\"structure_completeness\"] = sum(completeness_indicators) * 20\n",
    "            \n",
    "            # Check content quality\n",
    "            intro1 = conv_flow.get(\"intro1\", {})\n",
    "            intro2 = conv_flow.get(\"intro2\", {})\n",
    "            quality_indicators = [\n",
    "                len(intro1.get(\"opening_line\", \"\")) > 15,\n",
    "                len(intro1.get(\"episode_hook\", \"\")) > 15,\n",
    "                len(intro2.get(\"topic_introduction\", \"\")) > 15,\n",
    "                len(intro2.get(\"guest_welcome\", \"\")) > 10\n",
    "            ]\n",
    "            analysis[\"content_quality\"] = sum(quality_indicators) * 25\n",
    "            \n",
    "            # Check cultural integration\n",
    "            cultural = structure.get(\"cultural_context\", {})\n",
    "            cultural_indicators = [\n",
    "                len(cultural.get(\"proverbs_sayings\", [])) >= 1,\n",
    "                len(cultural.get(\"regional_references\", [])) >= 1\n",
    "            ]\n",
    "            analysis[\"cultural_integration\"] = sum(cultural_indicators) * 50\n",
    "            \n",
    "            # Check Arabic purity\n",
    "            full_text = json.dumps(structure, ensure_ascii=False)\n",
    "            is_arabic_pure, _ = self._validate_arabic_only(full_text)\n",
    "            analysis[\"arabic_purity\"] = 100 if is_arabic_pure else 0\n",
    "            \n",
    "            # Calculate overall score\n",
    "            analysis[\"overall_score\"] = min(100, sum([\n",
    "                analysis[\"structure_completeness\"],\n",
    "                analysis[\"content_quality\"],\n",
    "                analysis[\"cultural_integration\"],\n",
    "                analysis[\"arabic_purity\"]\n",
    "            ]) // 4)\n",
    "            \n",
    "            analysis[\"quality_grade\"] = (\n",
    "                \"Ù…Ù…ØªØ§Ø²\" if analysis[\"overall_score\"] >= 90 else\n",
    "                \"Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹\" if analysis[\"overall_score\"] >= 80 else\n",
    "                \"Ø¬ÙŠØ¯\" if analysis[\"overall_score\"] >= 70 else\n",
    "                \"Ù…Ù‚Ø¨ÙˆÙ„\" if analysis[\"overall_score\"] >= 60 else\n",
    "                \"ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†\"\n",
    "            )\n",
    "            \n",
    "            analysis[\"ready_for_next_step\"] = analysis[\"overall_score\"] >= 75\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except:\n",
    "            return {\"error\": \"Could not analyze structure quality\"}\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_fixed_conversation_structure_generator(deployment, topic, information, classification_result, personas_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the fixed conversation structure generator with Arabic-only validation\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª Testing Fixed Conversation Structure Generator...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = FixedConversationStructureGenerator(deployment, model_name)\n",
    "    \n",
    "    # Run generation with validation\n",
    "    structure_result, parsed_result = generator.generate_conversation_structure_with_validation(\n",
    "        topic, information, classification_result, personas_result\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“‹ Generated Structure:\")\n",
    "    print(f\"Episode Topic: {parsed_result.get('episode_topic', 'N/A')}\")\n",
    "    \n",
    "    # Show conversation flow\n",
    "    conv_flow = parsed_result.get(\"conversation_flow\", {})\n",
    "    intro1 = conv_flow.get(\"intro1\", {})\n",
    "    main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "    \n",
    "    print(f\"\\nğŸ¬ Intro1 Opening: {intro1.get('opening_line', 'N/A')[:80]}...\")\n",
    "    print(f\"ğŸ“ Discussion Points: {len(main_discussion)}\")\n",
    "    for i, point in enumerate(main_discussion, 1):\n",
    "        print(f\"  {i}. {point.get('point_title', 'N/A')[:60]}...\")\n",
    "    \n",
    "    # Show cultural context\n",
    "    cultural = parsed_result.get(\"cultural_context\", {})\n",
    "    proverbs = cultural.get(\"proverbs_sayings\", [])\n",
    "    print(f\"\\nğŸ›ï¸ Cultural Proverbs: {len(proverbs)}\")\n",
    "    for proverb in proverbs:\n",
    "        print(f\"  â€¢ {proverb}\")\n",
    "    \n",
    "    # Enhanced quality analysis\n",
    "    quality_analysis = generator.analyze_structure_quality(structure_result)\n",
    "    print(f\"\\nğŸ“ˆ Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Structure Completeness: {quality_analysis['structure_completeness']}/100\")\n",
    "    print(f\"Content Quality: {quality_analysis['content_quality']}/100\")\n",
    "    print(f\"Cultural Integration: {quality_analysis['cultural_integration']}/100\")\n",
    "    print(f\"Arabic Purity: {quality_analysis['arabic_purity']}/100\")\n",
    "    print(f\"Ready for Next Step: {'âœ…' if quality_analysis['ready_for_next_step'] else 'âŒ'}\")\n",
    "    \n",
    "    # Arabic validation check\n",
    "    is_arabic_valid, arabic_message = generator._validate_arabic_only(structure_result)\n",
    "    print(f\"\\nğŸ” Arabic Validation: {'âœ…' if is_arabic_valid else 'âŒ'} {arabic_message}\")\n",
    "    \n",
    "    return structure_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# generator = FixedConversationStructureGenerator(deployment, \"Fanar-C-1-8.7B\")\n",
    "# structure_result, parsed_result = generator.generate_conversation_structure_with_validation(topic, information, classification_result, personas_result)\n",
    "\n",
    "# Test the fixed generator\n",
    "# test_result = test_fixed_conversation_structure_generator(deployment, topic, information, classification_result, personas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09091744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Attempt 1: Arabic validation failed: English words detected\n",
      "âš ï¸ Attempt 2: Arabic validation failed: English words detected\n",
      "âš ï¸ Attempt 3: Arabic validation failed: English words detected\n",
      "ğŸ“ Using fallback conversation structure...\n",
      "Conversation Structure Result:\n",
      "{\n",
      "  \"episode_topic\": \"Ù†Ù‚Ø§Ø´ Ø­ÙˆÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ\",\n",
      "      \"background\": \"Ù…Ù‚Ø¯Ù… Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø°Ø§Ø¹ÙŠØ© Ù…Ø¹Ø±ÙˆÙ Ø¨Ø´ØºÙÙ‡ Ø¨Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\",\n",
      "      \"speaking_style\": \"Ù…ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± ÙˆÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ø´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ØªÙ‚Ù†ÙŠØ©\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯\",\n",
      "      \"background\": \"Ø¨Ø§Ø­Ø«Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ‡ Ø§Ù„Ù„ØºÙˆÙŠØ© ÙˆØ§Ù„Ø«Ù‚Ø§ÙÙŠØ©\",\n",
      "      \"speaking_style\": \"ØªÙˆØ¶ÙŠØ­ Ø¹Ù„Ù…ÙŠ Ø¯Ù‚ÙŠÙ‚ Ù…Ù…Ø²ÙˆØ¬ Ø¨ØªÙˆØ¶ÙŠØ­Ø§Øª Ø¹Ù…Ù„ÙŠØ© ÙˆØ´Ø±Ø­ Ù„Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„Ù…Ù†Ø§ÙØ¹\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù…ØŒ Ù…Ø¹ÙƒÙ… Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ ÙÙŠ Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©\",\n",
      "      \"podcast_introduction\": \"Ù†Ù†Ø§Ù‚Ø´ Ø§Ù„ÙŠÙˆÙ… Ù…ÙˆØ¶ÙˆØ¹Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹ ÙŠÙ‡Ù… Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙˆÙŠØ³ØªØ­Ù‚ Ø§Ù„ØªØ£Ù…Ù„\",\n",
      "      \"episode_hook\": \"Ù…ÙˆØ¶ÙˆØ¹ Ø­Ù„Ù‚ØªÙ†Ø§ Ø§Ù„ÙŠÙˆÙ… Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ ÙˆØ£Ø«Ø±Ù‡ Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙ†Ø§\"\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"Ø³Ù†ØªØ­Ø¯Ø« Ø§Ù„ÙŠÙˆÙ… Ø¹Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ ÙˆØ¬ÙˆØ§Ù†Ø¨Ù‡ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ù…Ù‡Ù…Ø©\",\n",
      "      \"guest_welcome\": \"Ù…Ø¹ÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø¶ÙŠÙ Ø§Ù„Ù…ØªÙ…ÙŠØ² Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ\",\n",
      "      \"guest_bio_highlight\": \"Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯ Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ ÙˆÙ„Ø¯ÙŠÙ‡ Ø®Ø¨Ø±Ø© ÙˆØ§Ø³Ø¹Ø©\"\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹\",\n",
      "        \"personal_angle\": \"ÙƒÙŠÙ ÙŠØ¤Ø«Ø± Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙ†Ø§ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙˆØªØ¬Ø§Ø±Ø¨Ù†Ø§\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©\",\n",
      "        \"personal_angle\": \"Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ§Ù„ÙØ±Øµ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù„Ø« ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©\",\n",
      "        \"personal_angle\": \"Ø§Ù„Ù†ØµØ§Ø¦Ø­ ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"Ø§Ù„Ø®Ù„Ø§ØµØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ù†Ù‚Ø§Ø´Ù†Ø§ Ø§Ù„ÙŠÙˆÙ…\",\n",
      "        \"guest_final_message\": \"Ø±Ø³Ø§Ù„Ø© Ø£Ø®ÙŠØ±Ø© ÙˆÙ…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ø¶ÙŠÙ Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ†\",\n",
      "        \"host_closing_thoughts\": \"Ø£ÙÙƒØ§Ø± Ø®ØªØ§Ù…ÙŠØ© ÙˆØªØ£Ù…Ù„Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù‚Ø¯Ù… Ø­ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\"\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"Ø´ÙƒØ±Ø§Ù‹ Ø¬Ø²ÙŠÙ„Ø§Ù‹ Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ù…ÙÙŠØ¯ ÙˆØ§Ù„Ø«Ø±ÙŠ\",\n",
      "        \"audience_thanks\": \"Ø´ÙƒØ±Ø§Ù‹ Ù„ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù… Ø¹Ù„Ù‰ Ù…ØªØ§Ø¨Ø¹ØªÙƒÙ… ÙˆØ§Ù‡ØªÙ…Ø§Ù…ÙƒÙ…\",\n",
      "        \"call_to_action\": \"ØªÙØ§Ø¹Ù„ÙˆØ§ Ù…Ø¹Ù†Ø§ ÙˆØ´Ø§Ø±ÙƒÙˆÙ†Ø§ Ø¢Ø±Ø§Ø¡ÙƒÙ… Ø¹Ø¨Ø± ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ\",\n",
      "        \"final_goodbye\": \"Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡ ÙÙŠ Ø­Ù„Ù‚Ø© Ù‚Ø§Ø¯Ù…Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…\",\n",
      "      \"ÙÙŠ Ø§Ù„ØªØ£Ù†ÙŠ Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆÙÙŠ Ø§Ù„Ø¹Ø¬Ù„Ø© Ø§Ù„Ù†Ø¯Ø§Ù…Ø©\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØºÙ†ÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\",\n",
      "      \"Ø§Ù„Ø®Ø¨Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ù„Ø¥Ù‚Ù„ÙŠÙ…ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "ğŸ§ª Testing Fixed Conversation Structure Generator...\n",
      "============================================================\n",
      "âš ï¸ Attempt 1: Arabic validation failed: English words detected\n",
      "âš ï¸ Attempt 2: Arabic validation failed: English words detected\n",
      "âš ï¸ Attempt 3: JSON parsing error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw response for debugging:\n",
      "{\n",
      "ğŸ“ Using fallback conversation structure...\n",
      "ğŸ“‹ Generated Structure:\n",
      "Episode Topic: Ù†Ù‚Ø§Ø´ Ø­ÙˆÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ\n",
      "\n",
      "ğŸ¬ Intro1 Opening: Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù…ØŒ Ù…Ø¹ÙƒÙ… Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ ÙÙŠ Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©...\n",
      "ğŸ“ Discussion Points: 3\n",
      "  1. Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹...\n",
      "  2. Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©...\n",
      "  3. Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù„Ø« ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©...\n",
      "\n",
      "ğŸ›ï¸ Cultural Proverbs: 2\n",
      "  â€¢ Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…\n",
      "  â€¢ ÙÙŠ Ø§Ù„ØªØ£Ù†ÙŠ Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆÙÙŠ Ø§Ù„Ø¹Ø¬Ù„Ø© Ø§Ù„Ù†Ø¯Ø§Ù…Ø©\n",
      "\n",
      "ğŸ“ˆ Quality Analysis:\n",
      "Overall Score: 75/100\n",
      "Quality Grade: Ø¬ÙŠØ¯\n",
      "Structure Completeness: 100/100\n",
      "Content Quality: 100/100\n",
      "Cultural Integration: 100/100\n",
      "Arabic Purity: 0/100\n",
      "Ready for Next Step: âœ…\n",
      "\n",
      "ğŸ” Arabic Validation: âŒ English words detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize with Arabic-only focus\n",
    "generator = FixedConversationStructureGenerator(deployment, model)\n",
    "\n",
    "# Generate with validation\n",
    "structure_result, parsed_result = generator.generate_conversation_structure_with_validation(\n",
    "    topic, information, classification_result, personas_result\n",
    ")\n",
    "print(\"Conversation Structure Result:\")\n",
    "print(structure_result)\n",
    "# Test with comprehensive checks\n",
    "test_result = test_fixed_conversation_structure_generator(\n",
    "    deployment, topic, information, classification_result, personas_result\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c22de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cece3b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Minimal enhancement (fastest, most concise)\\nenhancer_minimal = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"minimal\")\\nresult_minimal = enhancer_minimal.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Standard enhancement (balanced)\\nenhancer_standard = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"standard\")\\nresult_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Full enhancement (comprehensive but largest)\\nenhancer_full = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"full\")\\nresult_full = enhancer_full.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Test with specific level\\nenhanced_result = test_enhanced_dialogue_content_enhancer(\\n    deployment, topic, information, classification_result, personas_result, structure_result, \\n    model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"\\n)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "class SectionalDialogueContentEnhancer:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\", enhancement_level=\"minimal\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "        self.enhancement_level = enhancement_level  # \"minimal\", \"standard\", \"full\"\n",
    "\n",
    "    def enhance_intro_sections(self, topic, classification_result, personas_result, intro1, intro2):\n",
    "        \"\"\"\n",
    "        Chunk 1: Enhance intro1 and intro2 sections (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "        \n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"2\"\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"3\"\n",
    "        else:  # full\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"3-4\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast introductions.\n",
    "\n",
    "Task: Enhance ONLY the intro sections with natural dialogue elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current intro1: {json.dumps(intro1, ensure_ascii=False)}\n",
    "Current intro2: {json.dumps(intro2, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "\n",
    "For intro1, ADD this field:\n",
    "- \"{intro1_fields}\": [{item_count} natural spontaneous phrases that the host might use when opening, in MSA]\n",
    "\n",
    "For intro2, ADD this field:  \n",
    "- \"{intro2_fields}\": [{item_count} ways to connect this topic to Arab culture/values, in MSA]\n",
    "\n",
    "Return the enhanced sections in this exact format:\n",
    "{{\n",
    "    \"intro1\": {{\n",
    "        [keep all existing intro1 fields],\n",
    "        \"{intro1_fields}\": [new content]\n",
    "    }},\n",
    "    \"intro2\": {{\n",
    "        [keep all existing intro2 fields],\n",
    "        \"{intro2_fields}\": [new content]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing content unchanged\n",
    "- Add only the specified new fields\n",
    "- All new values in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation only (no ØŒ)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content specific to topic: {topic}\n",
    "- Match the {optimal_style} style\n",
    "- Keep arrays short and impactful\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast intros. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_main_discussion_point(self, topic, classification_result, personas_result, discussion_point, point_index):\n",
    "        \"\"\"\n",
    "        Chunk 2: Enhance individual main discussion points (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"]\"\"\"\n",
    "            field_count = \"2\"\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"],\n",
    "    \"natural_transitions\": \"transition phrase in MSA\\\"\"\"\"\n",
    "            field_count = \"3\"\n",
    "        else:  # full\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"disagreement_points\": \"disagreement description in MSA\",\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"],\n",
    "    \"natural_transitions\": \"transition phrase in MSA\",\n",
    "    \"emotional_triggers\": \"emotional description in MSA\\\"\"\"\"\n",
    "            field_count = \"5\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast discussion points.\n",
    "\n",
    "Task: Enhance ONE discussion point with rich dialogue elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Point #{point_index + 1}\n",
    "\n",
    "Current discussion point: {json.dumps(discussion_point, ensure_ascii=False)}\n",
    "\n",
    "Add EXACTLY these {field_count} fields. Keep all existing fields unchanged:\n",
    "\n",
    "{{\n",
    "    [all existing fields from discussion_point],{enhancement_fields}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing fields exactly as they are\n",
    "- Add only the {field_count} new fields shown above\n",
    "- All new content in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation ONLY (no ØŒ)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content relevant to topic: {topic}\n",
    "- Keep responses concise and actionable\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast discussion points. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_closing_sections(self, topic, classification_result, personas_result, closing_section):\n",
    "        \"\"\"\n",
    "        Chunk 3: Enhance closing (conclusion + outro) sections (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            conclusion_fields = '\"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\"'\n",
    "            outro_fields = '\"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\"'\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            conclusion_fields = '''\n",
    "        \"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\",\n",
    "        \"key_insights\": [\"insight 1 in MSA\", \"insight 2 in MSA\"]'''\n",
    "            outro_fields = '\"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\"'\n",
    "        else:  # full\n",
    "            conclusion_fields = '''\n",
    "        \"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\",\n",
    "        \"key_insights\": [2-3 key insights that should be highlighted in the wrap-up, in MSA]'''\n",
    "            outro_fields = '''\n",
    "        \"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\",\n",
    "        \"connection_building\": \"ways to build ongoing connection with the audience, in MSA\"'''\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast closings.\n",
    "\n",
    "Task: Enhance the closing section with natural wrap-up elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current closing: {json.dumps(closing_section, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "\n",
    "For conclusion subsection, ADD:\n",
    "{conclusion_fields}\n",
    "\n",
    "For outro subsection, ADD:\n",
    "{outro_fields}\n",
    "\n",
    "Return enhanced closing in this exact format:\n",
    "{{\n",
    "    \"conclusion\": {{\n",
    "        [keep all existing conclusion fields],\n",
    "        {conclusion_fields}\n",
    "    }},\n",
    "    \"outro\": {{\n",
    "        [keep all existing outro fields],\n",
    "        {outro_fields}\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing content unchanged\n",
    "- Add only the specified new fields\n",
    "- All new values in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation only (no ØŒ)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content feel conclusive and satisfying\n",
    "- Keep insights concise and actionable\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast closings. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def create_global_elements(self, topic, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Chunk 4: Create global elements (SIMPLIFIED AND REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\"\n",
    "        ],\n",
    "        \"personal_stories\": [\n",
    "            \"first personal story in MSA\",\n",
    "            \"second personal story in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\"\n",
    "        ],\n",
    "        \"storytelling_moments\": [\n",
    "            \"first storytelling moment in MSA\",\n",
    "            \"second storytelling moment in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "        else:  # full\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\",\n",
    "            \"third natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\", \n",
    "            \"third emotional reaction in MSA\"\n",
    "        ],\n",
    "        \"personal_stories\": [\n",
    "            \"first personal story in MSA\",\n",
    "            \"second personal story in MSA\"\n",
    "        ],\n",
    "        \"humorous_moments\": [\n",
    "            \"first humorous moment in MSA\",\n",
    "            \"second humorous moment in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"personality_interactions\": {\n",
    "        \"host_strengths\": \"host strengths description in MSA\",\n",
    "        \"guest_expertise\": \"guest expertise description in MSA\",\n",
    "        \"natural_chemistry\": \"chemistry description in MSA\",\n",
    "        \"tension_points\": \"tension points description in MSA\",\n",
    "        \"collaboration_moments\": \"collaboration description in MSA\"\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\",\n",
    "            \"third questioning style in MSA\"\n",
    "        ],\n",
    "        \"storytelling_moments\": [\n",
    "            \"first storytelling moment in MSA\",\n",
    "            \"second storytelling moment in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\",\n",
    "            \"third engagement technique in MSA\"\n",
    "        ],\n",
    "        \"emotional_peaks\": [\n",
    "            \"first emotional peak in MSA\",\n",
    "            \"second emotional peak in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in creating global dialogue elements for Arabic podcasts.\n",
    "\n",
    "Task: Create global sections that enhance the overall conversation flow.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "Enhancement Level: {self.enhancement_level}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Create EXACTLY this JSON structure with proper English punctuation:\n",
    "\n",
    "{structure}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Replace placeholder text with actual content in Modern Standard Arabic (MSA)\n",
    "- Use ONLY English commas (,) and standard quotes (\")\n",
    "- NO Arabic commas (ØŒ) or special punctuation\n",
    "- NO extra text before or after JSON\n",
    "- NO explanatory text\n",
    "- Make content specific to {host_name}, {guest_name}, and topic: {topic}\n",
    "- Follow the EXACT structure shown above\n",
    "- Keep content concise and actionable\n",
    "- Ensure all arrays have exactly the specified number of items\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You create global dialogue elements. Return ONLY valid JSON with English punctuation. No Arabic commas. No extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_dialogue_content(self, topic, information, classification_result, personas_result, structure_result):\n",
    "        \"\"\"\n",
    "        Main orchestration method: Coordinates all chunks with configurable enhancement levels\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”§ Starting sectional dialogue enhancement (Level: {self.enhancement_level})...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            structure = json.loads(structure_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid structure JSON provided\")\n",
    "        \n",
    "        # Extract sections\n",
    "        conv_flow = structure.get(\"conversation_flow\", {})\n",
    "        intro1 = conv_flow.get(\"intro1\", {})\n",
    "        intro2 = conv_flow.get(\"intro2\", {})\n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        closing = conv_flow.get(\"closing\", {})\n",
    "        \n",
    "        # Chunk 1: Enhance intro sections\n",
    "        print(\"ğŸ“ Chunk 1: Enhancing intro sections...\")\n",
    "        try:\n",
    "            enhanced_intros_json = self.enhance_intro_sections(\n",
    "                topic, classification_result, personas_result, intro1, intro2\n",
    "            )\n",
    "            enhanced_intros = json.loads(enhanced_intros_json)\n",
    "            \n",
    "            # Update structure\n",
    "            structure[\"conversation_flow\"][\"intro1\"].update(enhanced_intros.get(\"intro1\", {}))\n",
    "            structure[\"conversation_flow\"][\"intro2\"].update(enhanced_intros.get(\"intro2\", {}))\n",
    "            print(\"âœ… Intro sections enhanced successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error enhancing intros: {e}\")\n",
    "        \n",
    "        # Small delay between chunks\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 2: Enhance main discussion points (one by one)\n",
    "        print(\"ğŸ“ Chunk 2: Enhancing main discussion points...\")\n",
    "        enhanced_discussion_points = []\n",
    "        \n",
    "        for i, point in enumerate(main_discussion):\n",
    "            print(f\"  Enhancing discussion point {i+1}/{len(main_discussion)}...\")\n",
    "            try:\n",
    "                enhanced_point_json = self.enhance_main_discussion_point(\n",
    "                    topic, classification_result, personas_result, point, i\n",
    "                )\n",
    "                enhanced_point = json.loads(enhanced_point_json)\n",
    "                enhanced_discussion_points.append(enhanced_point)\n",
    "                print(f\"  âœ… Point {i+1} enhanced successfully\")\n",
    "                \n",
    "                # Small delay between points\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Error enhancing point {i+1}: {e}\")\n",
    "                print(f\"  ğŸ”„ Using fallback enhancement for point {i+1}...\")\n",
    "                \n",
    "                # Try fallback enhancement for this point\n",
    "                enhanced_point = self._create_fallback_discussion_point(\n",
    "                    topic, classification_result, personas_result, point, i\n",
    "                )\n",
    "                enhanced_discussion_points.append(enhanced_point)\n",
    "                print(f\"  âœ… Point {i+1} enhanced with fallback method\")\n",
    "        \n",
    "        # Update structure with enhanced discussion points\n",
    "        structure[\"conversation_flow\"][\"main_discussion\"] = enhanced_discussion_points\n",
    "        print(\"âœ… All main discussion points processed\")\n",
    "        \n",
    "        # Small delay between chunks\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 3: Enhance closing sections\n",
    "        print(\"ğŸ“ Chunk 3: Enhancing closing sections...\")\n",
    "        try:\n",
    "            enhanced_closing_json = self.enhance_closing_sections(\n",
    "                topic, classification_result, personas_result, closing\n",
    "            )\n",
    "            enhanced_closing = json.loads(enhanced_closing_json)\n",
    "            \n",
    "            # Update structure\n",
    "            structure[\"conversation_flow\"][\"closing\"].update(enhanced_closing)\n",
    "            print(\"âœ… Closing sections enhanced successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error enhancing closing: {e}\")\n",
    "        \n",
    "        # Small delay between chunks  \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 4: Create global elements\n",
    "        print(\"ğŸ“ Chunk 4: Creating global elements...\")\n",
    "        try:\n",
    "            global_elements_json = self.create_global_elements(\n",
    "                topic, classification_result, personas_result\n",
    "            )\n",
    "            global_elements = json.loads(global_elements_json)\n",
    "            \n",
    "            # Add global elements to structure\n",
    "            structure.update(global_elements)\n",
    "            print(\"âœ… Global elements created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error creating global elements: {e}\")\n",
    "            print(\"ğŸ”„ Attempting to create fallback global elements...\")\n",
    "            \n",
    "            # Create fallback global elements\n",
    "            try:\n",
    "                fallback_elements = self._create_fallback_global_elements(\n",
    "                    topic, classification_result, personas_result\n",
    "                )\n",
    "                structure.update(fallback_elements)\n",
    "                print(\"âœ… Fallback global elements created successfully\")\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"âš ï¸ Fallback also failed: {fallback_error}\")\n",
    "                print(\"ğŸ“ Using minimal default global elements...\")\n",
    "                # Add minimal default elements so validation doesn't fail\n",
    "                structure.update(self._get_minimal_global_elements())\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸ‰ Sectional dialogue enhancement completed! (Level: {self.enhancement_level})\")\n",
    "        \n",
    "        return json.dumps(structure, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Helper method to clean JSON response - Enhanced version\"\"\"\n",
    "        response = response.strip()\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic commas and punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('ØŒ', ',')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        import re\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json\n",
    "\n",
    "    def validate_enhanced_content(self, enhanced_json):\n",
    "        \"\"\"Validate the enhanced dialogue content (adapted for different levels)\"\"\"\n",
    "        try:\n",
    "            enhanced = json.loads(enhanced_json)\n",
    "            \n",
    "            missing_elements = []\n",
    "            \n",
    "            # Check global sections (varies by level)\n",
    "            if self.enhancement_level == \"minimal\":\n",
    "                required_global = [\"spontaneous_moments\", \"dialogue_techniques\"]\n",
    "            elif self.enhancement_level == \"standard\":\n",
    "                required_global = [\"spontaneous_moments\", \"dialogue_techniques\"]\n",
    "            else:  # full\n",
    "                required_global = [\"spontaneous_moments\", \"personality_interactions\", \"dialogue_techniques\"]\n",
    "                \n",
    "            for element in required_global:\n",
    "                if element not in enhanced:\n",
    "                    missing_elements.append(element)\n",
    "            \n",
    "            # Check enhanced conversation flow\n",
    "            conv_flow = enhanced.get(\"conversation_flow\", {})\n",
    "            \n",
    "            # Check intro1 enhancements\n",
    "            intro1 = conv_flow.get(\"intro1\", {})\n",
    "            if \"spontaneity_elements\" not in intro1:\n",
    "                missing_elements.append(\"intro1.spontaneity_elements\")\n",
    "            \n",
    "            # Check intro2 enhancements  \n",
    "            intro2 = conv_flow.get(\"intro2\", {})\n",
    "            if \"cultural_connections\" not in intro2:\n",
    "                missing_elements.append(\"intro2.cultural_connections\")\n",
    "            \n",
    "            # Check main discussion enhancements (varies by level)\n",
    "            main_disc = conv_flow.get(\"main_discussion\", [])\n",
    "            if self.enhancement_level == \"minimal\":\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"cultural_references\"]\n",
    "            elif self.enhancement_level == \"standard\":\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"cultural_references\", \"natural_transitions\"]\n",
    "            else:  # full\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"disagreement_points\", \"cultural_references\", \"natural_transitions\", \"emotional_triggers\"]\n",
    "            \n",
    "            for i, point in enumerate(main_disc):\n",
    "                for field in required_point_fields:\n",
    "                    if field not in point:\n",
    "                        missing_elements.append(f\"main_discussion[{i}].{field}\")\n",
    "            \n",
    "            # Check closing enhancements (varies by level)\n",
    "            closing = conv_flow.get(\"closing\", {})\n",
    "            conclusion = closing.get(\"conclusion\", {})\n",
    "            outro = closing.get(\"outro\", {})\n",
    "            \n",
    "            if \"emotional_closure\" not in conclusion:\n",
    "                missing_elements.append(\"closing.conclusion.emotional_closure\")\n",
    "            if \"memorable_ending\" not in outro:\n",
    "                missing_elements.append(\"closing.outro.memorable_ending\")\n",
    "                \n",
    "            # Additional checks for standard/full levels\n",
    "            if self.enhancement_level in [\"standard\", \"full\"]:\n",
    "                if \"key_insights\" not in conclusion:\n",
    "                    missing_elements.append(\"closing.conclusion.key_insights\")\n",
    "            if self.enhancement_level == \"full\":\n",
    "                if \"connection_building\" not in outro:\n",
    "                    missing_elements.append(\"closing.outro.connection_building\")\n",
    "            \n",
    "            if missing_elements:\n",
    "                return False, f\"Missing enhanced elements: {missing_elements}\"\n",
    "            \n",
    "            return True, f\"Sectional dialogue content enhancement validation successful (Level: {self.enhancement_level})\"\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            return False, \"Invalid JSON format in enhanced content\"\n",
    "\n",
    "    def _create_fallback_discussion_point(self, topic, classification_result, personas_result, discussion_point, point_index):\n",
    "        \"\"\"Create fallback enhancement for a single discussion point (level-aware)\"\"\"\n",
    "        enhanced_point = discussion_point.copy()\n",
    "        \n",
    "        # Add minimal enhancements based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"Ù‡Ø°Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹\",\n",
    "                    \"Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø§Ø±ÙƒÙƒÙ… ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\"\n",
    "                ],\n",
    "                \"cultural_references\": [\n",
    "                    \"ÙƒÙ…Ø§ ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù…Ø«Ù„: Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±\",\n",
    "                    \"ØªØ±Ø§Ø«Ù†Ø§ ÙŠØ¹Ù„Ù…Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡\"\n",
    "                ]\n",
    "            })\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"Ù‡Ø°Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹\",\n",
    "                    \"Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø§Ø±ÙƒÙƒÙ… ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\"\n",
    "                ],\n",
    "                \"cultural_references\": [\n",
    "                    \"ÙƒÙ…Ø§ ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù…Ø«Ù„: Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±\",\n",
    "                    \"ØªØ±Ø§Ø«Ù†Ø§ ÙŠØ¹Ù„Ù…Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡\"\n",
    "                ],\n",
    "                \"natural_transitions\": \"Ù‡Ø°Ø§ ÙŠÙ‚ÙˆØ¯Ù†Ø§ Ø¥Ù„Ù‰ Ù†Ù‚Ø·Ø© Ù…Ù‡Ù…Ø© Ø£Ø®Ø±Ù‰\"\n",
    "            })\n",
    "        else:  # full\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"Ù‡Ø°Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹\",\n",
    "                    \"Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø§Ø±ÙƒÙƒÙ… ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\"\n",
    "                ],\n",
    "                \"disagreement_points\": \"Ù‚Ø¯ ØªØ®ØªÙ„Ù ÙˆØ¬Ù‡Ø§Øª Ø§Ù„Ù†Ø¸Ø± Ø­ÙˆÙ„ Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø¶ÙŠØ©\",\n",
    "                \"cultural_references\": [\n",
    "                    \"ÙƒÙ…Ø§ ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù…Ø«Ù„: Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±\",\n",
    "                    \"ØªØ±Ø§Ø«Ù†Ø§ ÙŠØ¹Ù„Ù…Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡\"\n",
    "                ],\n",
    "                \"natural_transitions\": \"Ù‡Ø°Ø§ ÙŠÙ‚ÙˆØ¯Ù†Ø§ Ø¥Ù„Ù‰ Ù†Ù‚Ø·Ø© Ù…Ù‡Ù…Ø© Ø£Ø®Ø±Ù‰\",\n",
    "                \"emotional_triggers\": \"Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙŠÙ„Ø§Ù…Ø³ Ù‚Ù„ÙˆØ¨ ÙƒÙ„ Ù…Ù† ÙŠÙ‡ØªÙ… Ø¨Ù…Ø³ØªÙ‚Ø¨Ù„ Ø«Ù‚Ø§ÙØªÙ†Ø§\"\n",
    "            })\n",
    "        \n",
    "        return enhanced_point\n",
    "\n",
    "    def _create_fallback_global_elements(self, topic, classification_result, personas_result):\n",
    "        \"\"\"Create fallback global elements based on enhancement level\"\"\"\n",
    "        try:\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            personas = {}\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "        \n",
    "        fallback_elements = {\n",
    "            \"spontaneous_moments\": {\n",
    "                \"natural_interruptions\": [\n",
    "                    \"Ø§Ø³Ù…Ø­ÙˆØ§ Ù„ÙŠ Ø£Ù† Ø£Ø¶ÙŠÙ Ù†Ù‚Ø·Ø© Ù‡Ù†Ø§\",\n",
    "                    \"Ù‡Ø°Ø§ ÙŠØ°ÙƒØ±Ù†ÙŠ Ø¨Ù…ÙˆÙ‚Ù Ù…Ø´Ø§Ø¨Ù‡\"\n",
    "                ],\n",
    "                \"emotional_reactions\": [\n",
    "                    \"Ù‡Ø°Ø§ Ù…Ø¤Ø«Ø± ÙØ¹Ù„Ø§Ù‹\",\n",
    "                    \"Ù„Ù… Ø£ÙÙƒØ± ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø²Ø§ÙˆÙŠØ©\"\n",
    "                ]\n",
    "            },\n",
    "            \"dialogue_techniques\": {\n",
    "                \"questioning_styles\": [\n",
    "                    \"Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© Ù„ØªØ¹Ù…ÙŠÙ‚ Ø§Ù„Ù†Ù‚Ø§Ø´\",\n",
    "                    \"Ø£Ø³Ø¦Ù„Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ© Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¬Ø°ÙˆØ±\"\n",
    "                ],\n",
    "                \"audience_engagement\": [\n",
    "                    \"Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ÙŠÙÙƒØ± ÙÙŠÙ‡Ø§ Ø§Ù„Ù…Ø³ØªÙ…Ø¹\",\n",
    "                    \"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add more elements for standard/full levels\n",
    "        if self.enhancement_level in [\"standard\", \"full\"]:\n",
    "            fallback_elements[\"spontaneous_moments\"][\"personal_stories\"] = [\n",
    "                \"Ø£ØªØ°ÙƒØ± Ù…ÙˆÙ‚ÙØ§Ù‹ Ù…Ø´Ø§Ø¨Ù‡Ø§Ù‹ Ø­Ø¯Ø« Ù…Ø¹ÙŠ\",\n",
    "                \"ÙÙŠ ØªØ¬Ø±Ø¨ØªÙŠ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ¬Ø¯Øª Ø£Ù†\"\n",
    "            ]\n",
    "            fallback_elements[\"dialogue_techniques\"][\"storytelling_moments\"] = [\n",
    "                \"Ø³Ø±Ø¯ ØªØ¬Ø§Ø±Ø¨ Ø´Ø®ØµÙŠØ© Ø°Ø§Øª ØµÙ„Ø©\",\n",
    "                \"Ù‚ØµØµ Ù†Ø¬Ø§Ø­ Ù…Ù„Ù‡Ù…Ø©\"\n",
    "            ]\n",
    "        \n",
    "        if self.enhancement_level == \"full\":\n",
    "            fallback_elements[\"spontaneous_moments\"][\"humorous_moments\"] = [\n",
    "                \"Ù‡Ø°Ø§ ÙŠØ°ÙƒØ±Ù†ÙŠ Ø¨Ù†ÙƒØªØ© Ù„Ø·ÙŠÙØ©\",\n",
    "                \"Ø§Ù„Ù…ÙˆÙ‚Ù Ù„Ù‡ Ø¬Ø§Ù†Ø¨ Ø·Ø±ÙŠÙ\"\n",
    "            ]\n",
    "            fallback_elements[\"personality_interactions\"] = {\n",
    "                \"host_strengths\": f\"{host_name} Ù…Ø§Ù‡Ø± ÙÙŠ Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø­ÙˆØ§Ø±\",\n",
    "                \"guest_expertise\": f\"{guest_name} ÙŠÙ‚Ø¯Ù… Ù…Ø¹Ø±ÙØ© Ø¹Ù…ÙŠÙ‚Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ®ØµØµÙ‡\",\n",
    "                \"natural_chemistry\": \"ÙŠØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ù‚Ø¯Ù… ÙˆØ§Ù„Ø¶ÙŠÙ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…Ø±ÙŠØ­Ø©\",\n",
    "                \"tension_points\": \"Ù‚Ø¯ ÙŠØ®ØªÙ„ÙØ§Ù† ÙÙŠ Ø¨Ø¹Ø¶ ÙˆØ¬Ù‡Ø§Øª Ø§Ù„Ù†Ø¸Ø± Ù…Ù…Ø§ ÙŠØ«Ø±ÙŠ Ø§Ù„Ù†Ù‚Ø§Ø´\",\n",
    "                \"collaboration_moments\": \"ÙŠØ¨Ù†ÙŠØ§Ù† Ø¹Ù„Ù‰ Ø£ÙÙƒØ§Ø± Ø¨Ø¹Ø¶Ù‡Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¶ Ù„Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙˆÙ‰\"\n",
    "            }\n",
    "            fallback_elements[\"dialogue_techniques\"][\"emotional_peaks\"] = [\n",
    "                \"Ù„Ø­Ø¸Ø§Øª ØªØ£Ù…Ù„ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø©\",\n",
    "                \"Ù‚ØµØµ Ù…Ø¤Ø«Ø±Ø© ØªÙ„Ø§Ù…Ø³ Ø§Ù„Ù‚Ù„Ø¨\"\n",
    "            ]\n",
    "        \n",
    "        return fallback_elements\n",
    "\n",
    "    def _get_minimal_global_elements(self):\n",
    "        \"\"\"Return minimal default global elements\"\"\"\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"Ø§Ø³Ù…Ø­ÙˆØ§ Ù„ÙŠ Ø£Ù† Ø£Ø¶ÙŠÙ Ù†Ù‚Ø·Ø© Ù‡Ù†Ø§\",\n",
    "                        \"Ù‡Ø°Ø§ ÙŠØ°ÙƒØ±Ù†ÙŠ Ø¨Ù…ÙˆÙ‚Ù Ù…Ø´Ø§Ø¨Ù‡\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"Ù‡Ø°Ø§ Ù…Ø¤Ø«Ø± ÙØ¹Ù„Ø§Ù‹\",\n",
    "                        \"Ù„Ù… Ø£ÙÙƒØ± ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø²Ø§ÙˆÙŠØ©\"\n",
    "                    ]\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© Ù„ØªØ¹Ù…ÙŠÙ‚ Ø§Ù„Ù†Ù‚Ø§Ø´\",\n",
    "                        \"Ø£Ø³Ø¦Ù„Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ© Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¬Ø°ÙˆØ±\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ÙŠÙÙƒØ± ÙÙŠÙ‡Ø§ Ø§Ù„Ù…Ø³ØªÙ…Ø¹\",\n",
    "                        \"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"Ø§Ø³Ù…Ø­ÙˆØ§ Ù„ÙŠ Ø£Ù† Ø£Ø¶ÙŠÙ Ù†Ù‚Ø·Ø© Ù‡Ù†Ø§\",\n",
    "                        \"Ù‡Ø°Ø§ ÙŠØ°ÙƒØ±Ù†ÙŠ Ø¨Ù…ÙˆÙ‚Ù Ù…Ø´Ø§Ø¨Ù‡\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"Ù‡Ø°Ø§ Ù…Ø¤Ø«Ø± ÙØ¹Ù„Ø§Ù‹\",\n",
    "                        \"Ù„Ù… Ø£ÙÙƒØ± ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø²Ø§ÙˆÙŠØ©\"\n",
    "                    ],\n",
    "                    \"personal_stories\": [\n",
    "                        \"Ø£ØªØ°ÙƒØ± Ù…ÙˆÙ‚ÙØ§Ù‹ Ù…Ø´Ø§Ø¨Ù‡Ø§Ù‹ Ø­Ø¯Ø« Ù…Ø¹ÙŠ\",\n",
    "                        \"ÙÙŠ ØªØ¬Ø±Ø¨ØªÙŠ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ¬Ø¯Øª Ø£Ù†\"\n",
    "                    ]\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© Ù„ØªØ¹Ù…ÙŠÙ‚ Ø§Ù„Ù†Ù‚Ø§Ø´\",\n",
    "                        \"Ø£Ø³Ø¦Ù„Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ© Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¬Ø°ÙˆØ±\"\n",
    "                    ],\n",
    "                    \"storytelling_moments\": [\n",
    "                        \"Ø³Ø±Ø¯ ØªØ¬Ø§Ø±Ø¨ Ø´Ø®ØµÙŠØ© Ø°Ø§Øª ØµÙ„Ø©\",\n",
    "                        \"Ù‚ØµØµ Ù†Ø¬Ø§Ø­ Ù…Ù„Ù‡Ù…Ø©\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ÙŠÙÙƒØ± ÙÙŠÙ‡Ø§ Ø§Ù„Ù…Ø³ØªÙ…Ø¹\",\n",
    "                        \"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        else:  # full\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"Ø§Ø³Ù…Ø­ÙˆØ§ Ù„ÙŠ Ø£Ù† Ø£Ø¶ÙŠÙ Ù†Ù‚Ø·Ø© Ù‡Ù†Ø§\",\n",
    "                        \"Ù‡Ø°Ø§ ÙŠØ°ÙƒØ±Ù†ÙŠ Ø¨Ù…ÙˆÙ‚Ù Ù…Ø´Ø§Ø¨Ù‡\",\n",
    "                        \"Ø§Ù†ØªØ¸Ø±ØŒ Ù‡Ø°Ø§ Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"Ù‡Ø°Ø§ Ù…Ø¤Ø«Ø± ÙØ¹Ù„Ø§Ù‹\",\n",
    "                        \"Ù„Ù… Ø£ÙÙƒØ± ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø²Ø§ÙˆÙŠØ©\",\n",
    "                        \"Ø£ØªÙÙ‚ Ù…Ø¹Ùƒ ØªÙ…Ø§Ù…Ø§Ù‹\"\n",
    "                    ],\n",
    "                    \"personal_stories\": [\n",
    "                        \"Ø£ØªØ°ÙƒØ± Ù…ÙˆÙ‚ÙØ§Ù‹ Ù…Ø´Ø§Ø¨Ù‡Ø§Ù‹ Ø­Ø¯Ø« Ù…Ø¹ÙŠ\",\n",
    "                        \"ÙÙŠ ØªØ¬Ø±Ø¨ØªÙŠ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ¬Ø¯Øª Ø£Ù†\"\n",
    "                    ],\n",
    "                    \"humorous_moments\": [\n",
    "                        \"Ù‡Ø°Ø§ ÙŠØ°ÙƒØ±Ù†ÙŠ Ø¨Ù†ÙƒØªØ© Ù„Ø·ÙŠÙØ©\",\n",
    "                        \"Ø§Ù„Ù…ÙˆÙ‚Ù Ù„Ù‡ Ø¬Ø§Ù†Ø¨ Ø·Ø±ÙŠÙ\"\n",
    "                    ]\n",
    "                },\n",
    "                \"personality_interactions\": {\n",
    "                    \"host_strengths\": \"Ø§Ù„Ù…Ù‚Ø¯Ù… Ù…Ø§Ù‡Ø± ÙÙŠ Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø­ÙˆØ§Ø±\",\n",
    "                    \"guest_expertise\": \"Ø§Ù„Ø¶ÙŠÙ ÙŠÙ‚Ø¯Ù… Ù…Ø¹Ø±ÙØ© Ø¹Ù…ÙŠÙ‚Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ®ØµØµÙ‡\",\n",
    "                    \"natural_chemistry\": \"ÙŠØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ù‚Ø¯Ù… ÙˆØ§Ù„Ø¶ÙŠÙ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…Ø±ÙŠØ­Ø©\",\n",
    "                    \"tension_points\": \"Ù‚Ø¯ ÙŠØ®ØªÙ„ÙØ§Ù† ÙÙŠ Ø¨Ø¹Ø¶ ÙˆØ¬Ù‡Ø§Øª Ø§Ù„Ù†Ø¸Ø± Ù…Ù…Ø§ ÙŠØ«Ø±ÙŠ Ø§Ù„Ù†Ù‚Ø§Ø´\",\n",
    "                    \"collaboration_moments\": \"ÙŠØ¨Ù†ÙŠØ§Ù† Ø¹Ù„Ù‰ Ø£ÙÙƒØ§Ø± Ø¨Ø¹Ø¶Ù‡Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¶ Ù„Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙˆÙ‰\"\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© Ù„ØªØ¹Ù…ÙŠÙ‚ Ø§Ù„Ù†Ù‚Ø§Ø´\",\n",
    "                        \"Ø£Ø³Ø¦Ù„Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ© Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¬Ø°ÙˆØ±\",\n",
    "                        \"Ø£Ø³Ø¦Ù„Ø© Ø´Ø®ØµÙŠØ© Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠ\"\n",
    "                    ],\n",
    "                    \"storytelling_moments\": [\n",
    "                        \"Ø³Ø±Ø¯ ØªØ¬Ø§Ø±Ø¨ Ø´Ø®ØµÙŠØ© Ø°Ø§Øª ØµÙ„Ø©\",\n",
    "                        \"Ù‚ØµØµ Ù†Ø¬Ø§Ø­ Ù…Ù„Ù‡Ù…Ø©\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ÙŠÙÙƒØ± ÙÙŠÙ‡Ø§ Ø§Ù„Ù…Ø³ØªÙ…Ø¹\",\n",
    "                        \"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹\",\n",
    "                        \"Ø¯Ø¹ÙˆØ© Ø§Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ† Ù„Ù„ØªÙØ§Ø¹Ù„\"\n",
    "                    ],\n",
    "                    \"emotional_peaks\": [\n",
    "                        \"Ù„Ø­Ø¸Ø§Øª ØªØ£Ù…Ù„ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø©\",\n",
    "                        \"Ù‚ØµØµ Ù…Ø¤Ø«Ø±Ø© ØªÙ„Ø§Ù…Ø³ Ø§Ù„Ù‚Ù„Ø¨\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Enhanced Testing Function with Level Selection\n",
    "def test_enhanced_dialogue_content_enhancer(deployment, topic, information, classification_result, personas_result, structure_result, model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"):\n",
    "    \"\"\"\n",
    "    Test the enhanced dialogue content enhancer with level selection\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ§ª Testing Enhanced Dialogue Content Enhancer (Level: {enhancement_level})...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    enhancer = SectionalDialogueContentEnhancer(deployment, model_name, enhancement_level)\n",
    "    \n",
    "    # Run enhancement\n",
    "    enhanced_result = enhancer.enhance_dialogue_content(\n",
    "        topic, information, classification_result, personas_result, structure_result\n",
    "    )\n",
    "    \n",
    "    # Validate enhanced content\n",
    "    is_valid, validation_message = enhancer.validate_enhanced_content(enhanced_result)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Enhancement Results (Level: {enhancement_level}):\")\n",
    "    print(f\"Validation: {'âœ… Valid' if is_valid else 'âŒ Invalid'}\")\n",
    "    print(f\"Message: {validation_message}\")\n",
    "    \n",
    "    # Quick content analysis\n",
    "    try:\n",
    "        enhanced_data = json.loads(enhanced_result)\n",
    "        \n",
    "        # Count enhancement fields\n",
    "        conv_flow = enhanced_data.get(\"conversation_flow\", {})\n",
    "        intro1_enhancements = len([k for k in conv_flow.get(\"intro1\", {}).keys() if k not in [\"opening_line\", \"podcast_introduction\", \"episode_hook\", \"tone_guidance\"]])\n",
    "        intro2_enhancements = len([k for k in conv_flow.get(\"intro2\", {}).keys() if k not in [\"topic_introduction\", \"guest_welcome\", \"guest_bio_highlight\", \"transition_to_discussion\"]])\n",
    "        \n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        discussion_enhancements = 0\n",
    "        for point in main_discussion:\n",
    "            discussion_enhancements += len([k for k in point.keys() if k not in [\"point_title\", \"personal_angle\"]])\n",
    "        \n",
    "        closing_enhancements = 0\n",
    "        closing = conv_flow.get(\"closing\", {})\n",
    "        conclusion = closing.get(\"conclusion\", {})\n",
    "        outro = closing.get(\"outro\", {})\n",
    "        closing_enhancements += len([k for k in conclusion.keys() if k not in [\"main_takeaways\", \"guest_final_message\", \"host_closing_thoughts\"]])\n",
    "        closing_enhancements += len([k for k in outro.keys() if k not in [\"guest_appreciation\", \"audience_thanks\", \"call_to_action\", \"final_goodbye\"]])\n",
    "        \n",
    "        global_sections = len([k for k in enhanced_data.keys() if k not in [\"episode_topic\", \"personas\", \"conversation_flow\", \"cultural_context\", \"language_style\", \"technical_notes\"]])\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Enhancement Statistics:\")\n",
    "        print(f\"Intro1 Enhancements: {intro1_enhancements}\")\n",
    "        print(f\"Intro2 Enhancements: {intro2_enhancements}\")\n",
    "        print(f\"Discussion Enhancements: {discussion_enhancements}\")\n",
    "        print(f\"Closing Enhancements: {closing_enhancements}\")\n",
    "        print(f\"Global Sections Added: {global_sections}\")\n",
    "        \n",
    "        # Estimate size reduction vs original\n",
    "        total_enhancements = intro1_enhancements + intro2_enhancements + discussion_enhancements + closing_enhancements + global_sections\n",
    "        \n",
    "        if enhancement_level == \"minimal\":\n",
    "            expected_vs_full = \"~60% smaller than full enhancement\"\n",
    "        elif enhancement_level == \"standard\":\n",
    "            expected_vs_full = \"~40% smaller than full enhancement\"\n",
    "        else:\n",
    "            expected_vs_full = \"Full enhancement level\"\n",
    "        \n",
    "        print(f\"Total Enhancement Fields: {total_enhancements}\")\n",
    "        print(f\"Size vs Full: {expected_vs_full}\")\n",
    "        \n",
    "        # Show sample enhanced content\n",
    "        print(f\"\\nğŸ¯ Sample Enhanced Content:\")\n",
    "        intro1 = conv_flow.get(\"intro1\", {})\n",
    "        if \"spontaneity_elements\" in intro1:\n",
    "            spont_elements = intro1[\"spontaneity_elements\"]\n",
    "            print(f\"Intro1 Spontaneity: {len(spont_elements)} elements\")\n",
    "            for i, element in enumerate(spont_elements, 1):\n",
    "                print(f\"  {i}. {element[:60]}...\")\n",
    "        \n",
    "        global_spont = enhanced_data.get(\"spontaneous_moments\", {})\n",
    "        if \"natural_interruptions\" in global_spont:\n",
    "            interruptions = global_spont[\"natural_interruptions\"]\n",
    "            print(f\"Natural Interruptions: {len(interruptions)} items\")\n",
    "            for i, interruption in enumerate(interruptions, 1):\n",
    "                print(f\"  {i}. {interruption[:60]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error analyzing enhanced content: {e}\")\n",
    "    \n",
    "    return enhanced_result\n",
    "\n",
    "# Usage examples for different enhancement levels:\n",
    "\"\"\"\n",
    "# Minimal enhancement (fastest, most concise)\n",
    "enhancer_minimal = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"minimal\")\n",
    "result_minimal = enhancer_minimal.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Standard enhancement (balanced)\n",
    "enhancer_standard = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"standard\")\n",
    "result_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Full enhancement (comprehensive but largest)\n",
    "enhancer_full = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"full\")\n",
    "result_full = enhancer_full.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Test with specific level\n",
    "enhanced_result = test_enhanced_dialogue_content_enhancer(\n",
    "    deployment, topic, information, classification_result, personas_result, structure_result, \n",
    "    model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07febcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Starting sectional dialogue enhancement (Level: standard)...\n",
      "==================================================\n",
      "ğŸ“ Chunk 1: Enhancing intro sections...\n",
      "âœ… Intro sections enhanced successfully\n",
      "ğŸ“ Chunk 2: Enhancing main discussion points...\n",
      "  Enhancing discussion point 1/3...\n",
      "  âœ… Point 1 enhanced successfully\n",
      "  Enhancing discussion point 2/3...\n",
      "  âœ… Point 2 enhanced successfully\n",
      "  Enhancing discussion point 3/3...\n",
      "  âš ï¸ Error enhancing point 3: Expecting ',' delimiter: line 4 column 42 (char 153)\n",
      "  ğŸ”„ Using fallback enhancement for point 3...\n",
      "  âœ… Point 3 enhanced with fallback method\n",
      "âœ… All main discussion points processed\n",
      "ğŸ“ Chunk 3: Enhancing closing sections...\n",
      "âœ… Closing sections enhanced successfully\n",
      "ğŸ“ Chunk 4: Creating global elements...\n",
      "âœ… Global elements created successfully\n",
      "==================================================\n",
      "ğŸ‰ Sectional dialogue enhancement completed! (Level: standard)\n",
      "Standard Enhancement Result:\n",
      "{\n",
      "  \"episode_topic\": \"Ù†Ù‚Ø§Ø´ Ø­ÙˆÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ\",\n",
      "      \"background\": \"Ù…Ù‚Ø¯Ù… Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø°Ø§Ø¹ÙŠØ© Ù…Ø¹Ø±ÙˆÙ Ø¨Ø´ØºÙÙ‡ Ø¨Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\",\n",
      "      \"speaking_style\": \"Ù…ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± ÙˆÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ø´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ØªÙ‚Ù†ÙŠØ©\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯\",\n",
      "      \"background\": \"Ø¨Ø§Ø­Ø«Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ‡ Ø§Ù„Ù„ØºÙˆÙŠØ© ÙˆØ§Ù„Ø«Ù‚Ø§ÙÙŠØ©\",\n",
      "      \"speaking_style\": \"ØªÙˆØ¶ÙŠØ­ Ø¹Ù„Ù…ÙŠ Ø¯Ù‚ÙŠÙ‚ Ù…Ù…Ø²ÙˆØ¬ Ø¨ØªÙˆØ¶ÙŠØ­Ø§Øª Ø¹Ù…Ù„ÙŠØ© ÙˆØ´Ø±Ø­ Ù„Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„Ù…Ù†Ø§ÙØ¹\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… Ù…Ø¬Ø¯Ø¯Ø§Ù‹ Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„Ø£Ø¹Ø²Ø§Ø¡!\",\n",
      "      \"podcast_introduction\": \"Ø£ØªÙ†Ø§ÙˆÙ„ Ø§Ù„ÙŠÙˆÙ… Ù‚Ø¶ÙŠØ© Ø­ÙŠÙˆÙŠØ© ØªØªØ¯Ø§Ø®Ù„ ÙÙŠÙ‡Ø§ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø¨Ø§Ù„Ø¹Ø§Ø¯Ø§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ©\",\n",
      "      \"episode_hook\": \"Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø­Ù„Ù‚Ø©: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ØªØ·Ø±Ø­Ù‡Ø§ Ø­Ø¶Ø§Ø±ØªÙ†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ.\",\n",
      "      \"spontaneity_elements\": [\n",
      "        \"Ø£Ø¹Ù„Ù… Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø¶ÙŠØ© ØªØ«ÙŠØ± Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨ÙŠÙ† Ù…Ø¬ØªÙ…Ø¹Ù†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠ\",\n",
      "        \"Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø¨Ø¯Ø£ Ù‡Ø°Ù‡ Ø§Ù„Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ© Ù…Ø¹ Ø¶ÙŠÙØªÙ†Ø§ Ø§Ù„ÙØ§Ø¶Ù„Ø©!\",\n",
      "        \"Ø£Ø´Ø¹Ø± Ø­Ù‚Ø§ Ø¨Ø£Ù† ÙÙ‡Ù… ØªØ£Ø«ÙŠØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¶Ø±ÙˆØ±ÙŠ Ù„Ø­Ù…Ø§ÙŠØ© ØªØ±Ø§Ø«Ù†Ø§\"\n",
      "      ]\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"Ø§Ù„ÙŠÙˆÙ… Ù†Ø³ØªØ¹Ø±Ø¶ Ø±Ø­Ù„Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆÙƒÙŠÙ ÙŠØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ«Ù‚Ø§ÙØ§ØªÙ‡Ø§ Ø§Ù„ØºÙ†ÙŠØ©\",\n",
      "      \"guest_welcome\": \"Ù…Ø±Ø­Ø¨Ù‹Ø§ ÙŠØ§ Ø¯ÙƒØªÙˆØ± ÙØ§ØªÙ†! ÙŠØ³Ø¹Ø¯Ù†ÙŠ ÙˆØ¬ÙˆØ¯Ùƒ Ù…Ø¹Ù†Ø§ Ù„Ù„Ø­Ø¯ÙŠØ« Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø­Ø³Ø§Ø³\",\n",
      "      \"guest_bio_highlight\": \"ØªÙ…Ù„Ùƒ Ø®Ø¨Ø±Ø§Øª Ø¹Ù…ÙŠÙ‚Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ„Ø§Ù‚Ø­ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ø¹Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\",\n",
      "      \"cultural_connections\": [\n",
      "        \"ÙŠØªØ·Ù„Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‚Ø§Ø´ Ù…Ø¹Ø±ÙØ© Ø¹Ù…ÙŠÙ‚Ø© Ø¨ØªØ±Ø§Ø«Ù†Ø§ Ø§Ù„Ø£Ø¯Ø¨ÙŠ ÙˆØ§Ù„Ø´Ø¹Ø± Ø§Ù„Ø¬Ù…ÙŠÙ„ Ø§Ù„Ø°ÙŠ Ù†Ø¹ÙŠØ´Ù‡\",\n",
      "        \"Ù…Ù† Ø§Ù„Ù…Ù‡Ù… Ø£Ù† Ù†ØªØ°ÙƒØ± Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¬Ø°ÙˆØ±Ù†Ø§ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ø®ØªØ±Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„\",\n",
      "        \"ÙŠÙ…ÙƒÙ† Ù„Ù‡Ø°Ø§ Ø§Ù„Ø­Ø¯ÙŠØ« Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‚Ø§Ø¡ Ø§Ù„Ø¶ÙˆØ¡ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ÙÙ†ÙˆÙ† ÙƒØ§Ù„Ø®Ø· ÙˆØ§Ù„Ø¹Ù…Ø§Ø±Ø© ÙÙŠ Ù…ÙˆØ§Ø¬Ù‡Ø© ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø¬Ø¯ÙŠØ¯.\"\n",
      "      ]\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹\",\n",
      "        \"personal_angle\": \"ÙƒÙŠÙ ÙŠØ¤Ø«Ø± Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙ†Ø§ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙˆØªØ¬Ø§Ø±Ø¨Ù†Ø§\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"Ù…Ø«Ù„Ø§Ù‹, ØªØ®ÙŠÙ„ Ø£Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø¯Ø£ ÙŠÙˆØ¸Ù Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© Ø§Ù„Ù…Ù…ÙŠØ²Ø©\",\n",
      "          \"Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ Ù†Ù„Ø§Ø­Ø¸ ØªØ¹Ø¯ÙŠÙ„Ø§Øª ØºÙŠØ± Ù…Ù‚ØµÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ø³Ø© Ø¹Ø¨Ø± Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ­Ø±ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"Ù‚Ø¯ ÙŠÙØ°ÙƒØ± Ù‡Ù†Ø§ Ø¨ÙŠØª Ø´Ø¹Ø± Ø¹Ù† Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ©\",\n",
      "          \"ÙŠÙ…ÙƒÙ† Ø±Ø¨Ø· Ø§Ù„Ø­ÙˆØ§Ø± Ø¨Ø£Ø¹Ù…Ø§Ù„ Ø±ÙˆØ§Ø¦ÙŠÙŠÙ† Ø¹Ø±Ø¨ Ù…Ø«Ù„ Ù†Ø¬ÙŠØ¨ Ù…Ø­ÙÙˆØ¸\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"ÙˆÙ‡Ø°Ø§ ÙŠÙ‚ÙˆØ¯Ù†Ø§ Ø¥Ù„Ù‰ Ø·Ø±Ø­ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø«ÙŠØ± Ù„Ù„ØªÙÙƒÙŠØ±...\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©\",\n",
      "        \"personal_angle\": \"Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ§Ù„ÙØ±Øµ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØªØ¯Ø§Ø®Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©ØŸ\",\n",
      "          \"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ Ù‚Ø¯ ÙŠÙÙ‚Ø¯ ÙÙŠÙ‡Ø§ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŸ\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"Ù…ÙÙ‡ÙˆÙ… 'Ø§Ù„Ø¹Ù‚Ù„' ÙÙŠ Ø§Ù„ÙÙ„Ø³ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©\",\n",
      "          \"Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ Ù…Ø«Ù„ ÙƒØªØ§Ø¨ Ø£Ù„Ù Ù„ÙŠÙ„Ø© ÙˆÙ„ÙŠÙ„Ø©\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"Ù‚Ø¨Ù„ Ø§Ù„ØºÙˆØµ Ø¨Ø´ÙƒÙ„ Ø£Ø¹Ù…Ù‚ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª, Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø³ØªØ°ÙƒØ±...\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù„Ø« ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©\",\n",
      "        \"personal_angle\": \"Ø§Ù„Ù†ØµØ§Ø¦Ø­ ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"Ù‡Ø°Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹\",\n",
      "          \"Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø§Ø±ÙƒÙƒÙ… ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"ÙƒÙ…Ø§ ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù…Ø«Ù„: Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±\",\n",
      "          \"ØªØ±Ø§Ø«Ù†Ø§ ÙŠØ¹Ù„Ù…Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"Ù‡Ø°Ø§ ÙŠÙ‚ÙˆØ¯Ù†Ø§ Ø¥Ù„Ù‰ Ù†Ù‚Ø·Ø© Ù…Ù‡Ù…Ø© Ø£Ø®Ø±Ù‰\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"Ø§Ù„Ø®Ù„Ø§ØµØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø­ÙˆØ§Ø± ØªØ´Ø¯Ø¯ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ÙˆØ¹ÙŠ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ­ÙØ¸ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ©.\",\n",
      "        \"guest_final_message\": \"Ø¯Ø¹ÙˆÙ†Ø§ Ù†ÙˆØ§ØµÙ„ ØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ Ø¨ÙŠÙ†Ù…Ø§ Ù†Ø³Ø¹Ù‰ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ù„ØµØ§Ù„Ø­Ù†Ø§.\",\n",
      "        \"host_closing_thoughts\": \"Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ø«Ù„ Ù‡Ø°Ù‡, ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£Ù† Ù†ÙÙ‡Ù… Ø£ÙØ¶Ù„ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¯ÙˆÙ† Ø§Ù„Ø¥Ø®Ù„Ø§Ù„ Ø¨ØªÙ‚Ø§Ù„ÙŠØ¯Ù†Ø§ ÙˆÙ‚ÙŠÙ…Ù†Ø§.\",\n",
      "        \"emotional_closure\": \"Ù†Ø³Ø£Ù„ Ø§Ù„Ù„Ù‡ Ø£Ù† ÙŠÙ„Ù‡Ù…Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„ØªÙ‚Ù†ÙŠ Ø¨Ø­ÙƒÙ…Ø© ÙˆÙØ§Ø¦Ø¯Ø©.\",\n",
      "        \"key_insights\": [\n",
      "          \"Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø«Ø± Ø§Ù„Ù…ØªØ²Ø§ÙŠØ¯ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø­ÙŠØ§ØªÙ†Ø§ Ù‡Ùˆ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù†Ø­Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¨Ø´ÙƒÙ„ Ù…Ø³Ø¤ÙˆÙ„.\",\n",
      "          \"Ø¯Ù…Ø¬ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© ÙˆØ§Ù„ÙÙƒØ±Ø§Ù†ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ù…Ø¬ØªÙ…Ø¹ Ø±Ù‚Ù…ÙŠ Ø£ÙƒØ«Ø± Ø´Ù…ÙˆÙ„Ø§Ù‹.\"\n",
      "        ]\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"Ø´ÙƒØ±Ø§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯ Ù„Ù…Ø´Ø§Ø±ÙƒØªÙ‡Ø§ Ø§Ù„Ø«Ø§Ù‚Ø¨Ø©.\",\n",
      "        \"audience_thanks\": \"Ø´ÙƒØ±Ø§Ù‹ Ù„ÙƒÙ„ Ø§Ù„Ø°ÙŠÙ† Ø§Ù†Ø¶Ù…ÙˆØ§ Ø¥Ù„ÙŠÙ†Ø§ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø·Ù…ÙˆØ­Ø©.\",\n",
      "        \"call_to_action\": \"Ø´Ø§Ø±Ùƒ Ø£ÙÙƒØ§Ø±Ùƒ Ø­ÙˆÙ„ Ø¯ÙˆØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù„Ù‰ Ù…Ù†ØµØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ù„Ø¯ÙŠÙ†Ø§.\",\n",
      "        \"memorable_ending\": \"Ù†ØªÙˆØ­Ø¯ Ø¬Ù…ÙŠØ¹Ù‹Ø§ Ù„Ù†Ù†ÙŠØ± Ø·Ø±ÙŠÙ‚ Ø¹ØµØ±Ù†Ø§ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø¨Ø£Ø¶ÙˆØ§Ø¡ Ø­Ø¶Ø§Ø±ØªÙ†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØºÙ†ÙŠØ©.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…\",\n",
      "      \"ÙÙŠ Ø§Ù„ØªØ£Ù†ÙŠ Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆÙÙŠ Ø§Ù„Ø¹Ø¬Ù„Ø© Ø§Ù„Ù†Ø¯Ø§Ù…Ø©\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØºÙ†ÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\",\n",
      "      \"Ø§Ù„Ø®Ø¨Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ù„Ø¥Ù‚Ù„ÙŠÙ…ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\"\n",
      "    ]\n",
      "  },\n",
      "  \"spontaneous_moments\": {\n",
      "    \"natural_interruptions\": [\n",
      "      \"Ø£Ø­Ù…Ø¯, Ù…Ø¹ ÙƒÙ„ ØªÙ‚Ø¯Ù‘Ù… Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§, Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ´Ø¹Ø± Ø§Ù„Ø­Ø¶ÙˆØ± Ø£Ù†Ù‘ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ù‚Ø¯ ÙŠÙÙ†Ø³ÙÙŠÙ†Ø§ Ù„ØºØªÙ†Ø§ Ø§Ù„Ø£Ù…?\",\n",
      "      \"ÙØ§ØªÙ†, Ù…Ø§ Ø±Ø£ÙŠÙƒ Ø¨Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ø°ÙŠ ÙŠÙ„Ø¹Ø¨ÙÙ‡ Ø§Ù„Ø°ÙƒØ±Ù‰ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ§Ù„Ø£ÙˆØ§ØµØ± Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© ÙÙŠ Ù…Ù†Ø¹ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„Ù‡ÙˆÙŠØ©\"\n",
      "    ],\n",
      "    \"emotional_reactions\": [\n",
      "      \"Ø£Ø­Ù…Ø¯(Ø¨ØªØ¹Ø§Ø·Ù), Ù‡Ø°Ù‡ Ù…Ø®Ø§ÙˆÙ Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙ„ÙƒÙ† Ø¯Ø¹Ù†Ø§ Ù„Ø§ Ù†Ù†Ø³ÙŠ Ø§Ù„ÙØ±Øµ Ø§Ù„ØªÙŠ ÙŠÙˆÙØ±Ù‡Ø§ Ø§Ù„ØªØ·ÙˆØ±!\",\n",
      "      \"ÙØ§ØªÙ†(Ø¨Ø¥Ù„Ø­Ø§Ø­), ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ Ø¥Ù„Ù‡Ø§Ù… Ø§Ù„Ø´Ø¨Ø§Ø¨ Ù„ØªØ­ÙÙŠØ² Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ Ø¹Ø¨Ø± Ø§Ù„Ø£Ø¬ÙŠØ§Ù„ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¬Ø°ÙˆØ±Ù†Ø§\"\n",
      "    ],\n",
      "    \"personal_stories\": [\n",
      "      \"Ø£ÙˆØ¶Ø­Ù Ø£Ø­Ù…Ø¯ Ù‚ØµØªÙ‡ Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°ÙƒØ§Ø¦Ù‡ Ø§Ù„Ø®Ø§Øµ Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„ÙØµØ­Ù‰.\",\n",
      "      \"ØªØ´Ø§Ø±ÙƒØªÙ Ø¯. ÙØ§ØªÙ† ØªØ¬Ø±Ø¨ØªÙ‡Ø§ Ù…Ø¹ Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ­ÙØ¸ ÙˆÙŠØ±ÙˆÙÙŠ Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ø£Ø¯Ø¨ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\n",
      "    ]\n",
      "  },\n",
      "  \"dialogue_techniques\": {\n",
      "    \"questioning_styles\": [\n",
      "      \"Ø£Ø­Ù…Ø¯ ÙŠÙˆØ¬Ù‡ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…ÙØªÙˆØ­Ø§Ù‹ Ù„Ø£Ø³ØªØ§Ø°ØªÙ†Ø§:\",\n",
      "      \"ØªØ­Ø«Ù‘ÙŠÙ†Ø§ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ø§Ù„Ø³Ù…Ø§Ø¹ Ù…Ù† Ø®Ù„Ø§Ù„ Ø·Ø±Ø­Ù‡Ø§ Ù„Ø³Ø¤Ø§Ù„ Ù…ØºÙ„Ù‚.\"\n",
      "    ],\n",
      "    \"storytelling_moments\": [\n",
      "      \"ÙŠØ¹Ø¨Ø± Ø£Ø­Ù…Ø¯ Ø¨ØµØ±Ø§Ø­Ø©Ø¹Ù† Ù‚ØµØ© ØªÙØ±Ø¯Ù‡ Ù…Ø¹ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ùˆ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ø·ÙÙˆÙ„Ø©,\",\n",
      "      \"ØªØ­Ø¯ÙÙ‘Ø¯ Ø§Ù„Ø·Ø¨ÙŠØ¨Ø© ÙØ§ØªÙ† Ø¬Ø§Ù†Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨ØªØ¬Ø±Ø¨Ø© Ø´Ø®ØµÙŠØ© Ø­ÙˆÙ„ Ø£Ù‡Ù…ÙÙ‘Ø© ØªØ¹Ù„Ù… Ø§Ù„Ø£Ø·ÙØ§Ù„ Ù„Ù„ØºØ© Ø§Ù„Ø¢Ø¨Ø§Ø¡\"\n",
      "    ],\n",
      "    \"audience_engagement\": [\n",
      "      \"ÙŠØ¬Ø°Ø¨ Ø§Ø­Ù…Ø¯ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø¨Ø¥Ø´Ø±Ø§ÙƒÙ‡Ù… Ø¨Ø³Ø¤Ø§Ù„Ù‡:\",\n",
      "      \"ØªØ´Ø¬Ø¹ ÙØ§Ø·Ù…Ø©Ø§Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ† Ù„ØªØ´Ø§Ø±Ùƒ Ø£ÙÙƒØ§Ø±Ù‡Ù… Ø£Ùˆ Ø£Ø³Ø¦ØªÙ‡Ù… Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø©Ø¨Ø§Ù„topic.\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Standard enhancement (balanced)\n",
    "enhancer_standard = SectionalDialogueContentEnhancer(deployment, model, \"standard\")\n",
    "result_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "print(\"Standard Enhancement Result:\")\n",
    "print(result_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a852810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "class MinimalPolishEnhancer:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def enhance_spontaneous_moments_values(self, topic, classification_result, personas_result, current_spontaneous_moments):\n",
    "        \"\"\"\n",
    "        Chunk 1: Enhance values in existing spontaneous_moments (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast dialogue quality.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing spontaneous_moments structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current spontaneous moments: {json.dumps(current_spontaneous_moments, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make each phrase more natural and topic-specific\n",
    "- Connect phrases directly to the topic: {topic}\n",
    "- Make content specific to {host_name} and {guest_name}'s backgrounds\n",
    "- Ensure phrases sound more authentic and conversational\n",
    "\n",
    "Return the enhanced structure with the same fields but better values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these exact personas\n",
    "- Improve naturalness and authenticity of existing phrases\n",
    "- Use English punctuation only (no ØŒ)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance existing values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_cultural_context_values(self, topic, classification_result, current_cultural_context):\n",
    "        \"\"\"\n",
    "        Chunk 2: Enhance values in existing cultural_context (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "        except:\n",
    "            classification = {}\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic cultural references.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing cultural_context structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "\n",
    "Current cultural context: {json.dumps(current_cultural_context, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and relevance of existing values\n",
    "- Make proverbs more directly relevant to the topic: {topic}\n",
    "- Make regional references more specific and meaningful\n",
    "- Ensure cultural authenticity and accuracy\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±\"\n",
    "- After: \"Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±ØŒ ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø´Ù…Ø¹Ø© ØªØ¶ÙŠØ¡ Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ ØªØ±Ø§Ø«Ù†Ø§\"\n",
    "\n",
    "Return the enhanced cultural context with better, more topic-specific values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make all content directly relevant to topic: {topic}\n",
    "- Maintain cultural authenticity and accuracy\n",
    "- Use English punctuation only (no ØŒ)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance cultural context values only. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_dialogue_techniques_values(self, topic, classification_result, personas_result, current_dialogue_techniques):\n",
    "        \"\"\"\n",
    "        Chunk 3: Enhance values in existing dialogue_techniques (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast dialogue techniques.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing dialogue_techniques structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current dialogue techniques: {json.dumps(current_dialogue_techniques, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make techniques more specific to the topic: {topic}\n",
    "- Tailor content to {host_name} and {guest_name}'s expertise\n",
    "- Make techniques more actionable and practical\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© Ù„ØªØ¹Ù…ÙŠÙ‚ Ø§Ù„Ù†Ù‚Ø§Ø´\"\n",
    "- After: \"Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ·ÙˆÙŠØ± Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø¬Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ¹Ù…Ù‚Ù‡Ø§ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ\"\n",
    "\n",
    "Return the enhanced dialogue techniques with better, more specific values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve practicality and specificity of techniques\n",
    "- Use English punctuation only (no ØŒ)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance dialogue technique values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_main_discussion_values(self, topic, classification_result, personas_result, current_main_discussion):\n",
    "        \"\"\"\n",
    "        Chunk 4: Enhance values in existing main_discussion points (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast discussion content.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing main_discussion structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current main discussion: {json.dumps(current_main_discussion, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names for each discussion point\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make spontaneous_triggers more natural and topic-specific\n",
    "- Make cultural_references more directly relevant to the topic\n",
    "- Make natural_transitions smoother and more contextual\n",
    "- Ensure content reflects {host_name} and {guest_name}'s specific expertise\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"Ù‡Ø°Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹\"\n",
    "- After: \"Ù‡Ø°Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹ Ø­ÙˆÙ„ Ù‚Ø¯Ø±Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ ÙˆØ±Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\n",
    "\n",
    "Return the enhanced main discussion with better, more specific values:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        [exact same structure as each input point, but with enhanced values]\n",
    "    }},\n",
    "    ...\n",
    "]\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve naturalness and conversational flow\n",
    "- Use English punctuation only (no ØŒ)\n",
    "- Return only valid JSON array, no extra text\n",
    "- Do NOT add new fields to discussion points\n",
    "- Do NOT change array lengths within points\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance discussion point values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_intro_outro_values(self, topic, classification_result, personas_result, current_intro1, current_intro2, current_closing):\n",
    "        \"\"\"\n",
    "        Chunk 5: Enhance values in existing intro and outro sections (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast intros and outros.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing intro1, intro2, and closing structures. Keep the exact same fields.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current intro1: {json.dumps(current_intro1, ensure_ascii=False)}\n",
    "Current intro2: {json.dumps(current_intro2, ensure_ascii=False)}\n",
    "Current closing: {json.dumps(current_closing, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array (if any)\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make opening_line more engaging and natural\n",
    "- Make episode_hook more compelling and topic-specific\n",
    "- Make guest_welcome more personal and authentic\n",
    "- Make closing thoughts more memorable and impactful\n",
    "- Ensure content reflects the personalities of {host_name} and {guest_name}\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"Ø£Ù‡Ù„Ø§Ù‹ Ø¨ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù…\"\n",
    "- After: \"Ø£Ù‡Ù„Ø§Ù‹ Ø¨ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù… ÙÙŠ Ø±Ø­Ù„Ø© Ø§Ø³ØªÙƒØ´Ø§ÙÙŠØ© Ù…Ø«ÙŠØ±Ø© Ù„Ù†ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø¬Ø¹Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø­Ø§Ø±Ø³Ø§Ù‹ Ù„ØªØ±Ø§Ø«Ù†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠ\"\n",
    "\n",
    "Return the enhanced sections:\n",
    "\n",
    "{{\n",
    "    \"intro1\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }},\n",
    "    \"intro2\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }},\n",
    "    \"closing\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve engagement and naturalness\n",
    "- Use English punctuation only (no ØŒ)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance intro/outro values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def apply_minimal_polish(self, topic, information, classification_result, personas_result, enhanced_content_result):\n",
    "        \"\"\"\n",
    "        Main orchestration method: Coordinates all value enhancement chunks\n",
    "        \"\"\"\n",
    "        print(\"ğŸ¨ Starting minimal polish (value enhancement only)...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            enhanced_content = json.loads(enhanced_content_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid enhanced content JSON provided\")\n",
    "        \n",
    "        # Chunk 1: Enhance spontaneous moments values\n",
    "        print(\"âœ¨ Chunk 1: Enhancing spontaneous moments values...\")\n",
    "        try:\n",
    "            current_spontaneous = enhanced_content.get(\"spontaneous_moments\", {})\n",
    "            if current_spontaneous:  # Only enhance if exists\n",
    "                enhanced_spontaneous_json = self.enhance_spontaneous_moments_values(\n",
    "                    topic, classification_result, personas_result, current_spontaneous\n",
    "                )\n",
    "                enhanced_spontaneous = json.loads(enhanced_spontaneous_json)\n",
    "                enhanced_content[\"spontaneous_moments\"] = enhanced_spontaneous\n",
    "                print(\"âœ… Spontaneous moments values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"â­ï¸ No spontaneous moments to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error enhancing spontaneous moments: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 2: Enhance cultural context values\n",
    "        print(\"âœ¨ Chunk 2: Enhancing cultural context values...\")\n",
    "        try:\n",
    "            current_cultural = enhanced_content.get(\"cultural_context\", {})\n",
    "            if current_cultural:  # Only enhance if exists\n",
    "                enhanced_cultural_json = self.enhance_cultural_context_values(\n",
    "                    topic, classification_result, current_cultural\n",
    "                )\n",
    "                enhanced_cultural = json.loads(enhanced_cultural_json)\n",
    "                enhanced_content[\"cultural_context\"] = enhanced_cultural\n",
    "                print(\"âœ… Cultural context values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"â­ï¸ No cultural context to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error enhancing cultural context: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 3: Enhance dialogue techniques values\n",
    "        print(\"âœ¨ Chunk 3: Enhancing dialogue techniques values...\")\n",
    "        try:\n",
    "            current_dialogue = enhanced_content.get(\"dialogue_techniques\", {})\n",
    "            if current_dialogue:  # Only enhance if exists\n",
    "                enhanced_dialogue_json = self.enhance_dialogue_techniques_values(\n",
    "                    topic, classification_result, personas_result, current_dialogue\n",
    "                )\n",
    "                enhanced_dialogue = json.loads(enhanced_dialogue_json)\n",
    "                enhanced_content[\"dialogue_techniques\"] = enhanced_dialogue\n",
    "                print(\"âœ… Dialogue techniques values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"â­ï¸ No dialogue techniques to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error enhancing dialogue techniques: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 4: Enhance main discussion values\n",
    "        print(\"âœ¨ Chunk 4: Enhancing main discussion values...\")\n",
    "        try:\n",
    "            conv_flow = enhanced_content.get(\"conversation_flow\", {})\n",
    "            current_main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "            if current_main_discussion:  # Only enhance if exists\n",
    "                enhanced_discussion_json = self.enhance_main_discussion_values(\n",
    "                    topic, classification_result, personas_result, current_main_discussion\n",
    "                )\n",
    "                enhanced_discussion = json.loads(enhanced_discussion_json)\n",
    "                enhanced_content[\"conversation_flow\"][\"main_discussion\"] = enhanced_discussion\n",
    "                print(\"âœ… Main discussion values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"â­ï¸ No main discussion to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error enhancing main discussion: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 5: Enhance intro and outro values\n",
    "        print(\"âœ¨ Chunk 5: Enhancing intro and outro values...\")\n",
    "        try:\n",
    "            conv_flow = enhanced_content.get(\"conversation_flow\", {})\n",
    "            current_intro1 = conv_flow.get(\"intro1\", {})\n",
    "            current_intro2 = conv_flow.get(\"intro2\", {})\n",
    "            current_closing = conv_flow.get(\"closing\", {})\n",
    "            \n",
    "            if current_intro1 or current_intro2 or current_closing:  # Only enhance if any exist\n",
    "                enhanced_sections_json = self.enhance_intro_outro_values(\n",
    "                    topic, classification_result, personas_result, \n",
    "                    current_intro1, current_intro2, current_closing\n",
    "                )\n",
    "                enhanced_sections = json.loads(enhanced_sections_json)\n",
    "                \n",
    "                if \"intro1\" in enhanced_sections and current_intro1:\n",
    "                    enhanced_content[\"conversation_flow\"][\"intro1\"] = enhanced_sections[\"intro1\"]\n",
    "                if \"intro2\" in enhanced_sections and current_intro2:\n",
    "                    enhanced_content[\"conversation_flow\"][\"intro2\"] = enhanced_sections[\"intro2\"]\n",
    "                if \"closing\" in enhanced_sections and current_closing:\n",
    "                    enhanced_content[\"conversation_flow\"][\"closing\"] = enhanced_sections[\"closing\"]\n",
    "                \n",
    "                print(\"âœ… Intro and outro values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"â­ï¸ No intro/outro sections to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error enhancing intro/outro: {e}\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"ğŸ‰ Minimal polish completed! Same structure, enhanced values.\")\n",
    "        \n",
    "        return json.dumps(enhanced_content, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Enhanced JSON cleaning method\"\"\"\n",
    "        response = response.strip()\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Handle arrays\n",
    "        if clean_json.strip().startswith('['):\n",
    "            start_idx = response.find('[')\n",
    "            end_idx = response.rfind(']')\n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                clean_json = response[start_idx:end_idx+1]\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('ØŒ', ',')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        import re\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        return clean_json\n",
    "\n",
    "    def validate_polished_outline(self, original_json, polished_json):\n",
    "        \"\"\"\n",
    "        Validate that polished outline has same structure but enhanced values\n",
    "        \"\"\"\n",
    "        try:\n",
    "            original = json.loads(original_json)\n",
    "            polished = json.loads(polished_json)\n",
    "            \n",
    "            issues = []\n",
    "            \n",
    "            # Check that main structure is preserved\n",
    "            original_keys = set(original.keys())\n",
    "            polished_keys = set(polished.keys())\n",
    "            \n",
    "            if original_keys != polished_keys:\n",
    "                issues.append(f\"Main structure changed: {original_keys} vs {polished_keys}\")\n",
    "            \n",
    "            # Check conversation flow structure\n",
    "            orig_conv = original.get(\"conversation_flow\", {})\n",
    "            pol_conv = polished.get(\"conversation_flow\", {})\n",
    "            \n",
    "            if set(orig_conv.keys()) != set(pol_conv.keys()):\n",
    "                issues.append(\"Conversation flow structure changed\")\n",
    "            \n",
    "            # Check main discussion array length\n",
    "            orig_main = orig_conv.get(\"main_discussion\", [])\n",
    "            pol_main = pol_conv.get(\"main_discussion\", [])\n",
    "            \n",
    "            if len(orig_main) != len(pol_main):\n",
    "                issues.append(f\"Main discussion length changed: {len(orig_main)} vs {len(pol_main)}\")\n",
    "            \n",
    "            # Check that arrays within sections maintain length\n",
    "            sections_to_check = [\"spontaneous_moments\", \"dialogue_techniques\", \"cultural_context\"]\n",
    "            \n",
    "            for section in sections_to_check:\n",
    "                if section in original and section in polished:\n",
    "                    orig_section = original[section]\n",
    "                    pol_section = polished[section]\n",
    "                    \n",
    "                    if isinstance(orig_section, dict) and isinstance(pol_section, dict):\n",
    "                        for key in orig_section:\n",
    "                            if isinstance(orig_section[key], list) and isinstance(pol_section.get(key), list):\n",
    "                                if len(orig_section[key]) != len(pol_section[key]):\n",
    "                                    issues.append(f\"{section}.{key} array length changed\")\n",
    "            \n",
    "            # Check for quality improvement (simple heuristic)\n",
    "            orig_text = json.dumps(original, ensure_ascii=False)\n",
    "            pol_text = json.dumps(polished, ensure_ascii=False)\n",
    "            \n",
    "            if len(pol_text) < len(orig_text) * 0.95:  # Significant reduction might indicate loss of content\n",
    "                issues.append(\"Polished content appears significantly shorter\")\n",
    "            \n",
    "            if issues:\n",
    "                return False, f\"Structure validation issues: {issues}\"\n",
    "            \n",
    "            return True, \"Minimal polish validation successful - same structure, enhanced values\"\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            return False, f\"JSON parsing error: {e}\"\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_minimal_polish_enhancer(deployment, topic, information, classification_result, personas_result, enhanced_content_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the minimal polish enhancer\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª Testing Minimal Polish Enhancer (Value Enhancement Only)...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    polisher = MinimalPolishEnhancer(deployment, model_name)\n",
    "    \n",
    "    # Store original for comparison\n",
    "    original_json = enhanced_content_result\n",
    "    \n",
    "    # Run minimal polish\n",
    "    polished_result = polisher.apply_minimal_polish(\n",
    "        topic, information, classification_result, personas_result, enhanced_content_result\n",
    "    )\n",
    "    \n",
    "    # Validate polished content\n",
    "    is_valid, validation_message = polisher.validate_polished_outline(original_json, polished_result)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Polish Results:\")\n",
    "    print(f\"Validation: {'âœ… Valid' if is_valid else 'âŒ Invalid'}\")\n",
    "    print(f\"Message: {validation_message}\")\n",
    "    \n",
    "    # Quick comparison analysis\n",
    "    try:\n",
    "        original_data = json.loads(original_json)\n",
    "        polished_data = json.loads(polished_result)\n",
    "        \n",
    "        # Compare sample values\n",
    "        print(f\"\\nğŸ” Sample Value Comparisons:\")\n",
    "        \n",
    "        # Spontaneous moments comparison\n",
    "        orig_spont = original_data.get(\"spontaneous_moments\", {}).get(\"natural_interruptions\", [])\n",
    "        pol_spont = polished_data.get(\"spontaneous_moments\", {}).get(\"natural_interruptions\", [])\n",
    "        \n",
    "        if orig_spont and pol_spont:\n",
    "            print(f\"Natural Interruptions:\")\n",
    "            print(f\"  Original: {orig_spont[0][:50]}...\")\n",
    "            print(f\"  Polished: {pol_spont[0][:50]}...\")\n",
    "        \n",
    "        # Cultural context comparison\n",
    "        orig_cultural = original_data.get(\"cultural_context\", {}).get(\"proverbs_sayings\", [])\n",
    "        pol_cultural = polished_data.get(\"cultural_context\", {}).get(\"proverbs_sayings\", [])\n",
    "        \n",
    "        if orig_cultural and pol_cultural:\n",
    "            print(f\"Proverbs:\")\n",
    "            print(f\"  Original: {orig_cultural[0][:50]}...\")\n",
    "            print(f\"  Polished: {pol_cultural[0][:50]}...\")\n",
    "        \n",
    "        # Size comparison\n",
    "        orig_size = len(json.dumps(original_data, ensure_ascii=False))\n",
    "        pol_size = len(json.dumps(polished_data, ensure_ascii=False))\n",
    "        size_change = ((pol_size - orig_size) / orig_size) * 100\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Size Analysis:\")\n",
    "        print(f\"Original Size: {orig_size:,} characters\")\n",
    "        print(f\"Polished Size: {pol_size:,} characters\")\n",
    "        print(f\"Size Change: {size_change:+.1f}%\")\n",
    "        print(f\"Approach: {'âœ… Value enhancement only' if abs(size_change) < 15 else 'âš ï¸ Significant size change'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error analyzing polished content: {e}\")\n",
    "    \n",
    "    return polished_result\n",
    "\n",
    "# Usage:\n",
    "# polisher = MinimalPolishEnhancer(deployment, \"Fanar-C-1-8.7B\")\n",
    "# final_polished_outline = polisher.apply_minimal_polish(topic, information, classification_result, personas_result, enhanced_content_result)\n",
    "\n",
    "# Test the polisher\n",
    "# polished_result = test_minimal_polish_enhancer(\n",
    "#     deployment, topic, information, classification_result, personas_result, enhanced_content_result\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "967209c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ Starting minimal polish (value enhancement only)...\n",
      "==================================================\n",
      "âœ¨ Chunk 1: Enhancing spontaneous moments values...\n",
      "âœ… Spontaneous moments values enhanced successfully\n",
      "âœ¨ Chunk 2: Enhancing cultural context values...\n",
      "âœ… Cultural context values enhanced successfully\n",
      "âœ¨ Chunk 3: Enhancing dialogue techniques values...\n",
      "âš ï¸ Error enhancing dialogue techniques: Expecting ',' delimiter: line 12 column 5 (char 772)\n",
      "âœ¨ Chunk 4: Enhancing main discussion values...\n",
      "âš ï¸ Error enhancing main discussion: Expecting ',' delimiter: line 12 column 140 (char 877)\n",
      "âœ¨ Chunk 5: Enhancing intro and outro values...\n",
      "âœ… Intro and outro values enhanced successfully\n",
      "==================================================\n",
      "ğŸ‰ Minimal polish completed! Same structure, enhanced values.\n",
      "Final Polished Outline:\n",
      "{\n",
      "  \"episode_topic\": \"Ù†Ù‚Ø§Ø´ Ø­ÙˆÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ\",\n",
      "      \"background\": \"Ù…Ù‚Ø¯Ù… Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø°Ø§Ø¹ÙŠØ© Ù…Ø¹Ø±ÙˆÙ Ø¨Ø´ØºÙÙ‡ Ø¨Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\",\n",
      "      \"speaking_style\": \"Ù…ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± ÙˆÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ø´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ØªÙ‚Ù†ÙŠØ©\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯\",\n",
      "      \"background\": \"Ø¨Ø§Ø­Ø«Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ‡ Ø§Ù„Ù„ØºÙˆÙŠØ© ÙˆØ§Ù„Ø«Ù‚Ø§ÙÙŠØ©\",\n",
      "      \"speaking_style\": \"ØªÙˆØ¶ÙŠØ­ Ø¹Ù„Ù…ÙŠ Ø¯Ù‚ÙŠÙ‚ Ù…Ù…Ø²ÙˆØ¬ Ø¨ØªÙˆØ¶ÙŠØ­Ø§Øª Ø¹Ù…Ù„ÙŠØ© ÙˆØ´Ø±Ø­ Ù„Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„Ù…Ù†Ø§ÙØ¹\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"Ø§Ù†Ø·Ù„Ù‚ Ø¨Ù†Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ù‚Ù„Ø¨ ØªØ­Ø¯Ù Ø¹ØµØ±ÙŠ ÙŠØºÙˆØµ ÙÙŠ Ø£Ø¹Ù…Ø§Ù‚ ØªÙ‚Ø§Ø·Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©, Ø­ÙŠØ« Ø³Ù†Ø¶Ø¹ ÙƒÙ„Ø§Ù†Ø§ Ø£ÙŠØ¯ÙŠÙ‡Ù…Ø§ Ø¨Ù‚Ù„Ø¨ Ù…ØªÙ†Ø¨Ù‡ ÙˆÙ…Ø´ÙˆÙ‚.\",\n",
      "      \"podcast_introduction\": \"ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø«ÙŠØ±Ø©, Ø³ÙˆÙ Ù†Ù‚ÙˆÙ… Ø¨ÙÙÙƒÙÙ‘ Ø±Ù…ÙˆØ² Ø§Ù„Ø¢Ø«Ø§Ø± Ø§Ù„Ù…Ø²Ø¯ÙˆØ¬Ø© Ù„Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø¹Ù„Ù‰ Ø¬ÙˆÙ‡Ø± Ù‡ÙÙˆÙŠØªÙ†Ø§ ÙˆÙ†Ù…Ø· Ø¹ÙŠØ´Ù†Ø§ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ Ø§Ù„Ø£ØµÙŠÙ„.\",\n",
      "      \"episode_hook\": \"Ø¥Ù† Ù…ÙˆØ¶ÙˆØ¹ ÙŠÙˆÙ…Ù†Ø§ ÙŠØ¯ÙˆØ± Ø­ÙˆÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ÙƒØ¨ÙŠØ±: Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØµØ¨Ø­ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØµØ§Ù†Ø¹Ø§Ù‹ Ù„Ø´Ù‡Ø§Ø¯Ø§ØªÙ†Ø§ Ø£Ù… Ù…Ø­Ø°ÙØ§ Ù„Ù‡Ø§ØŸ Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø¯Ø®Ù„ Ø¥Ù„Ù‰ Ù…ÙØªØ±Ù‚ Ø·Ø±Ù‚ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…ÙÙ„Ø­Ù‘Ø© Ø¬Ù†Ø¨Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø®Ø¨ÙŠØ±Ø© Ù„Ø§ ØªÙØ¬Ø§Ø±Ù‰.\",\n",
      "      \"spontaneity_elements\": [\n",
      "        \"Ø¨ÙƒÙ„ ØµØ¯Ù‚, Ø£Ù†Ø§ Ø£ØªØ·Ù„Ø¹ Ù„Ø³Ù…Ø§Ø¹ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø§Ø¦Ø¹Ø© Ù„Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯ ÙˆØ§Ù„ØªÙŠ Ø³ØªØ³Ø§Ø¹Ø¯Ù†Ø§ Ø¹Ù„Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„ØµÙˆØ±Ø©\",\n",
      "        \"Ù„Ù†Ø¨Ø¯Ø£ Ø¬ÙˆÙ„Ø© Ø¨Ø­Ø«ÙŠØ© ØºÙ†ÙŠØ© Ø³ØªÙ…ÙƒÙ‘ÙÙ†Ù†Ø§ Ù…Ù† Ø±Ø¤ÙŠØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ÙŠØ³ ÙƒØªÙ‡Ø¯ÙŠØ¯ Ø¨Ù„ ÙƒÙØ±ØµØ© Ù„Ø¨Ù†Ø§Ø¡ Ø¬Ø³ÙˆØ± Ø¨ÙŠÙ† Ø§Ù„Ù…Ø§Ø¶ÙŠ ÙˆØ§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„.\",\n",
      "        \"Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ù…Ù‡Ù… Ù„Ù„ØºØ§ÙŠØ© Ù„Ø­Ø§Ù…Ù„ÙŠ Ø§Ù„Ø±Ø§ÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© - Ù†Ø­Ù† Ø§Ù„Ø´Ø¨Ø§Ø¨ - Ø§Ù„Ø°ÙŠÙ† ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ‡Ù… Ø­Ù…Ù„ Ø´Ø¹Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø¹Ù† Ø·Ø¨ÙŠØ¹Ø© ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ù†Ø§ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù†ØªÙ‚Ø§Ø¯Ù‡Ø§ Ù†Ù‚Ø¯Ù‹Ø§ Ø­ÙŠØ§Ø¯ÙŠÙ‹Ø§.\"\n",
      "      ]\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"ØªØ­ÙŠØ§ Ù…Ø¹Ù†Ø§ Ø§Ù„Ø¢Ù† Ø±Ø­Ù„Ø© Ø´ÙŠÙ‚Ø© Ø¹Ø¨Ø± Ø¹Ø§Ù„Ù…ÙŠÙ† Ù…Ø®ØªÙ„ÙÙŠÙ† Ù„ÙƒÙ†Ù‡Ù…Ø§ Ù…ØªØ±Ø§Ø¨Ø·Ø§Ù†Ø› Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ù…ØªØ­Ø±Ùƒ Ø³Ø±ÙŠØ¹ Ø§Ù„Ù†Ù‡ÙˆØ¶ ÙˆØ¹Ø§Ù„Ù… ØªØ±Ø§Ø«Ù†Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ Ø§Ù„Ø¹Ø±ÙŠÙ‚.\",\n",
      "      \"guest_welcome\": \"Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯, Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ Ø±Ø³Ù…ÙŠÙ‹Ø§ ÙÙŠ Ø¨Ø±Ù†Ø§Ù…Ø¬Ù†Ø§! Ø£Ù†Øª ØªÙ†Ø´Ø± Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ù„Ø§Ù†Ø·Ø¨Ø§Ø¹Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø·Ø±Ø§ÙØ© Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªØ¹Ù„Ù‚ Ø§Ù„Ø£Ù…Ø± Ø¨Ù…Ù†Ø§Ù‚Ø´Ø© Ø­Ø¯ÙˆØ¯ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ Ø¨ÙŠÙ† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ±Ø§Ø« Ø§Ù„Ø«Ù‚Ø§ÙÙŠ ÙˆØ§Ù„Ù…Ø¹Ø±ÙÙŠ Ù„Ù„Ø´Ø¹Ø¨ Ø§Ù„Ø¹Ø±Ø¨ÙŠ.\",\n",
      "      \"guest_bio_highlight\": \"Ù„Ø¯ÙŠÙƒ Ø®Ø¨Ø±Ø© ÙˆØ§Ø³Ø¹Ø© Ø¬Ø¯Ù‹Ø§ ÙÙŠ ØªØ¯Ø±ÙŠØ³ ÙˆØ§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆÙ…Ø¹Ø§Ù†ÙŠÙ‡Ø§ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨ØªÙƒØ±Ø© Ø¶Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø®Ø§Øµ Ø¨ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\",\n",
      "      \"cultural_connections\": [\n",
      "        \"Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ Ø§Ù„Ø´Ø¹Ø±ÙŠ Ø§Ù„Ø¹Ø±Ø¨Ù‰ Ø§Ù„Ù‚Ø¯ÙŠÙ… ÙŠØ´ÙƒÙ„ Ø¬Ø²Ø¡ Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† Ø£Ø³Ø§Ø³ ØªÙƒÙˆÙŠÙ† Ø´Ø®ØµÙŠØªÙ†Ø§, Ù„Ø°Ù„Ùƒ ÙØ¥Ù† ØªØ®ÙŠÙ„ Ø¢ÙØ§Ù‚Ù‡ ØªØ­Øª Ù…Ø¸Ù„Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø£Ù…Ø± ÙŠØ³ØªØ­Ù‚ Ø§Ù„Ù†Ø¸Ø± ÙÙŠÙ‡ Ù…Ù„ÙŠØ¦ Ø¨Ø§Ù„Ø³Ø­Ø± ÙˆØ§Ù„Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª.\",\n",
      "        \"Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ Ø£Ù† Ù†ÙˆØ§Ø¬Ù‡ Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠ Ø¨Ø®ÙˆÙ Ø£Ùˆ Ø±ÙØ¶ ÙˆÙ„ÙƒÙ† Ø¨Ø¯Ø±Ø§Ø³Ø© Ù…Ø¯Ø±ÙˆØ³Ø© ØªØ¬Ù…Ø¹ Ù…Ø§ Ø¨ÙŠÙ† Ø±ÙˆØ­ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„ØªØµÙ…ÙŠÙ…ÙŠ ÙˆØ¨ÙŠÙ† Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª Ø°Ø§Øª Ø¬Ø¯ÙˆÙ‰ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©.\",\n",
      "        \"Ù…Ù† ÙˆØ¬Ù‡ Ù†Ø¸Ø± Ø¹Ù„Ù… Ø¬Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ø±Ø§Ù†ÙŠØ© ÙˆÙ…Ø§ ØªØ±ÙƒÙ‡ Ù„Ù†Ø§ Ø§Ù„ÙÙ†Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø§Ø¨Ù‚ÙˆÙ†, Ø±Ø¨Ù…Ø§ ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ø±ÙˆØ¨ÙˆØªÙŠØ© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠÙ‘Ø© Ø¥Ù„Ù‡Ø§Ù… ØªØ¹Ø¯ÙŠÙ„Ø§Øª ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªÙ‚Ø¯ÙŠÙ… Ù„Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© Ø§Ù„Ø¨Ø§Ù‡Ø±Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©!\"\n",
      "      ]\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹\",\n",
      "        \"personal_angle\": \"ÙƒÙŠÙ ÙŠØ¤Ø«Ø± Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙ†Ø§ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙˆØªØ¬Ø§Ø±Ø¨Ù†Ø§\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"Ù…Ø«Ù„Ø§Ù‹, ØªØ®ÙŠÙ„ Ø£Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø¯Ø£ ÙŠÙˆØ¸Ù Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© Ø§Ù„Ù…Ù…ÙŠØ²Ø©\",\n",
      "          \"Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ Ù†Ù„Ø§Ø­Ø¸ ØªØ¹Ø¯ÙŠÙ„Ø§Øª ØºÙŠØ± Ù…Ù‚ØµÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ø³Ø© Ø¹Ø¨Ø± Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ­Ø±ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"Ù‚Ø¯ ÙŠÙØ°ÙƒØ± Ù‡Ù†Ø§ Ø¨ÙŠØª Ø´Ø¹Ø± Ø¹Ù† Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ©\",\n",
      "          \"ÙŠÙ…ÙƒÙ† Ø±Ø¨Ø· Ø§Ù„Ø­ÙˆØ§Ø± Ø¨Ø£Ø¹Ù…Ø§Ù„ Ø±ÙˆØ§Ø¦ÙŠÙŠÙ† Ø¹Ø±Ø¨ Ù…Ø«Ù„ Ù†Ø¬ÙŠØ¨ Ù…Ø­ÙÙˆØ¸\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"ÙˆÙ‡Ø°Ø§ ÙŠÙ‚ÙˆØ¯Ù†Ø§ Ø¥Ù„Ù‰ Ø·Ø±Ø­ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø«ÙŠØ± Ù„Ù„ØªÙÙƒÙŠØ±...\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©\",\n",
      "        \"personal_angle\": \"Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ§Ù„ÙØ±Øµ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØªØ¯Ø§Ø®Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©ØŸ\",\n",
      "          \"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ Ù‚Ø¯ ÙŠÙÙ‚Ø¯ ÙÙŠÙ‡Ø§ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŸ\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"Ù…ÙÙ‡ÙˆÙ… 'Ø§Ù„Ø¹Ù‚Ù„' ÙÙŠ Ø§Ù„ÙÙ„Ø³ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©\",\n",
      "          \"Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ Ù…Ø«Ù„ ÙƒØªØ§Ø¨ Ø£Ù„Ù Ù„ÙŠÙ„Ø© ÙˆÙ„ÙŠÙ„Ø©\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"Ù‚Ø¨Ù„ Ø§Ù„ØºÙˆØµ Ø¨Ø´ÙƒÙ„ Ø£Ø¹Ù…Ù‚ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª, Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø³ØªØ°ÙƒØ±...\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù„Ø« ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©\",\n",
      "        \"personal_angle\": \"Ø§Ù„Ù†ØµØ§Ø¦Ø­ ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"Ù‡Ø°Ø§ ÙŠØ«ÙŠØ± ØªØ³Ø§Ø¤Ù„Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹\",\n",
      "          \"Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø§Ø±ÙƒÙƒÙ… ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"ÙƒÙ…Ø§ ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù…Ø«Ù„: Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±\",\n",
      "          \"ØªØ±Ø§Ø«Ù†Ø§ ÙŠØ¹Ù„Ù…Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"Ù‡Ø°Ø§ ÙŠÙ‚ÙˆØ¯Ù†Ø§ Ø¥Ù„Ù‰ Ù†Ù‚Ø·Ø© Ù…Ù‡Ù…Ø© Ø£Ø®Ø±Ù‰\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"Ø¨Ø¹Ø¯ ØªÙØ§ØµÙŠÙ„ Ø¨Ø­Ø«Ù†Ø§ Ø§Ù„ÙˆØ§Ø¶Ø­ Ø§Ù„Ø¯Ù‚ÙŠÙ‚, Ø¨Ø§Øª ÙˆØ§Ø¶Ø­Ù‹Ø§ Ø£Ù†Ù‡ Ø¨Ø§Ù„Ø¥Ù…ÙƒØ§Ù† ØªØ­Ù‚ÙŠÙ‚ ØªÙˆØ§Ø²Ù† Ø±Ø§Ø¦Ø¹ Ø¨ÙŠÙ† Ø§Ù…ØªØµØ§Øµ Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Øª Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø§Ø­ØªÙˆØ§Ø¦ÙŠ ÙˆØ£ØµÙˆÙ„Ù†Ø§ Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ø¥Ø°Ø§ ØªÙ… Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø±Ø§Øª Ø­ÙƒÙŠÙ…Ø© ÙˆÙ…Ø³Ø¤ÙˆÙ„Ø©.\",\n",
      "        \"guest_final_message\": \"Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø¨Ù‚Ù‰ Ù…Ù„ØªØ²Ù…ÙŠÙ† Ø¨Ø¹Ù…Ù„ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª ÙˆØ§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ© ÙˆÙ†Ø¹Ù…Ù„ Ø¨Ù„Ø§ Ù‡ÙˆØ§Ø¯Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø±Ø¤Ø§Ù†Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø¥Ù„Ù‰ ÙˆØ§Ù‚Ø¹ Ù…Ù„Ù…ÙˆØ³ ÙˆØ°Ù„Ùƒ Ø¨Ø¥Ø±Ø´Ø§Ø¯ Ø´Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø£Ù…Ø«Ù„ Ù„Ù…Ø§ ØªÙ…ØªÙ„ÙƒÙ‡ ØªÙ‚Ù†ÙŠØ§Øª Ø°ÙƒØ§Ø¦Ù†Ø§ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù…Ø¤Ø®Ø±Ù‹Ø§ Ù…Ù† Ù‚Ø¯Ø±Ø§Øª ØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚Ø©.\",\n",
      "        \"host_closing_thoughts\": \"Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø°Ù„Ùƒ, ÙŠØ¨Ù‚Ù‰ Ù…ØµÙŠØ± ØªØ­Ø¯ÙŠØ¯ Ù…Ø¯Ù‰ Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙ‚Ø¯Ø§Ù† Ø£ØµØ§Ù„Ø© Ø«Ù‚Ø§ÙØªÙ†Ø§ Ù…Ù‚Ø§Ø¨Ù„ Ù…ÙƒØ§Ø³Ø¨ ØªÙ‚Ø¯Ù… Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ø§Ù„Ù‡Ø§Ø¦Ù„Ø© Ø£Ù…Ø§Ù…Ù†Ø§ â€” ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©, ÙŠÙƒÙ…Ù† Ø§Ù„Ù‚Ø±Ø§Ø± Ø¹Ù†Ø¯Ù†Ø§ Ø¬Ù…ÙŠØ¹Ù‹Ø§ ÙƒØ¬ÙŠÙ„ Ø±Ø®Ø§Ø¡ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠ Ø­Ø¯ÙŠØ« Ø§Ù„Ø­Ø¶ÙˆØ± ÙˆÙ…Ø¯Ø§ÙØ¹ÙŠÙ† Ù…Ù…ØªØ§Ø²ÙŠÙ† Ù„Ø£Ø³Ù„ÙˆØ¨ Ø­ÙŠØ§Ø© Ø¹Ø±Ø¨ÙŠ Ù…Ø³Ù„Ù… Ù…Ù…ÙŠØ².\",\n",
      "        \"emotional_closure\": \"ÙˆÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø·Ø§Ù, Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø³Ù…Ùˆ ÙƒØ§ÙØ©Ù‹ Ø¨Ø±ÙˆØ­ Ø´ÙƒØ± Ù„Ù„Ù‡ Ø¹Ø² ÙˆØ¬Ù„ Ù„Ø£Ù†Ù‡ Ù…Ù†Ø­Ù†Ø§ ØªÙ„Ùƒ Ø§Ù„ÙØ±ØµØ© Ø§Ù„Ù…Ø¯Ù‡Ø´Ø© Ù„Ù„Ø­ÙŠØ§Ø© ÙˆØ§Ø®ØªØ¨Ø±Ù†Ø§ ÙÙŠÙ‡Ø§ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„ØºØ±ÙŠØ¨Ø© Ù„Ù„ÙÙ‡Ù… Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠ Ø§Ù„Ù…Ø¨Ù‡Ø± Ù„Ø¹Ø§Ù„Ù…Ù†Ø§ Ø§Ù„Ø¬Ø§Ù…Ø¹ Ø§Ù„Ù…ØªÙ†ÙˆØ¹.\",\n",
      "        \"key_insights\": [\n",
      "          \"ØªØ¹Ø²ÙŠØ² Ø§Ù„ØµØ±Ø§Ø­Ø© Ø¨Ø´Ø£Ù† Ø§Ù„ØªØ¯ÙÙ‚ Ø§Ù„Ù†Ø§Ø¬Ù… Ù…Ù† ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙ‰ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ© ÙˆØ§Ù„Ø¬Ù…Ø§Ù‡ÙŠØ±ÙŠØ© ÙŠØ¹Ø¯ Ø§Ù…Ø± Ù…Ù‚Ø¨ÙˆÙ„ ÙˆÙŠØ¬Ø¨ Ø§Ø¹ØªØ¨Ø§Ø±Ù‡ Ø®Ø·ÙˆØ© Ø£ÙˆÙ„Ù‰ towards ØªØ­Ø³ÙŠÙ† Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø¨ÙŠÙ† Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø«Ø§Ù„Ø«.\",\n",
      "          \"Ù…ØªØ§Ø¨Ø¹Ø© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¢Ø²Ø± Ø§Ù„Ù…Ø¯Ø±ÙˆØ³Ø© Ø¨ÙŠÙ† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ ÙˆØµÙŠØ§ØºØ© Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ¹Ø±ÙŠÙ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ù„ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ© Ù‚ØµÙŠØ±Ø© ØªØ¹ØªØ¨Ø± Ù…ÙØªØ§Ø­Ø§ Ù…Ù‡Ù…Ø§ Ù„ÙˆØ¬ÙˆØ¯ Ù†ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ Ù…Ø®ØªÙ„Ø· Ù…ØªÙ†ÙˆØ¹ Ù…Ù† Ø§Ù„Ù†Ø³ÙŠØ¬ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù… .\"\n",
      "        ]\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"Ù…Ø±Ø© Ø¢Ø®Ø±Ù‰ Ù†Ø´ÙƒØ±Ùƒ Ø¬Ø²ÙŠÙ„ Ø§Ù„Ø´ÙƒØ± Ù„Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯ Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ø´Ø§Ø±ÙƒØªÙ‡Ø§ Ø§Ù„Ù…Ø¤ØµÙ„Ø© Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø­Ù„Ù‚ØªÙ†Ø§ Ø§Ù„Ø¬Ø¯Ù„ÙŠØ© Ø§Ù„Ù…Ù„Ù‡Ù…Ø© Ø§Ù„ÙŠÙˆÙ….\",\n",
      "        \"audience_thanks\": \"Ø¬Ø²ÙŠÙ„ Ø§Ù„Ø§Ù…ØªÙ†Ø§Ù† Ù„Ùƒ Ø£ÙŠÙ‡Ø§ Ø§Ù„Ø¹Ø²ÙŠØ² Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ Ø§Ù„Ù…Ø®Ù„Øµ Ù„Ø£Ù†Ùƒ Ø®ØµÙÙ‘ØµØª ÙˆÙ‚ØªÙƒ Ø«Ù…ÙŠÙ† Ù„Ù‚Ø¶Ø§Ø¡ Ø³Ø§Ø¹Ø© Ø¨Ø±ÙÙ‚ØªÙ†Ø§ Ù‡Ù†Ø§.\",\n",
      "        \"call_to_action\": \"Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ³Ø§Ø¦Ø· Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹Ù‰ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø±Ù†Ø§Ù…Ø¬Ù†ÙØ§ Ù„ÙŠØ®Ø¨Ø±Ù†Ø§ Ø±Ø£ÙŠÙƒ:Ù‡Ù„ ØªØ¤Ù…Ù† Ø¨Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ† Ù„Ù„ØªÙˆØ¸ÙŠÙ Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ø£Ø³Ø±Ø§Ø± Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªÙ‰ ØªØ³Ø§Ù‡Ù… ÙÙ‰ Ø²ÙŠØ§Ø¯Ø© ØªÙ‚Ø¯ÙŠØ³Ù†Ø§ Ø§Ù„Ø¥Ù‚Ù„ÙŠÙ…Ù‰ Ù„Ù„Ø¹ØµÙˆØ± Ø§Ù„Ø²Ø§Ù‡Ø±Ø© Ø§Ù„Ù…Ø¬ÙŠØ¯Ø© ØŸ\",\n",
      "        \"memorable_ending\": \"Ø­ØªÙ‰ Ù„Ù‚Ø§Ø¡ Ø§Ø®Ø±, ÙÙ„Ù†Ø±ØªØ¨Ø· Ø³ÙˆÙŠØ§Ù‹ ÙˆØªÙ…Ø³Ùƒ Ø¨ÙƒÙ„ Ù…Ø§ ÙŠØ¤Ù…Ù† Ø¨Ù‡ Ø¹Ù‚Ù„ÙŠØªÙ†Ø§ Ø§Ù„Ø®Ù„Ø§Ø¨Ø© ÙˆØ§ØªØ®Ø° Ù…ÙˆØ§Ù‚Ø¹ Ø«Ø§Ø¨ØªØ© ÙƒØ¨Ø§Ø­Ø«ÙŠÙ† Ù…ØºØ§Ù…Ø±ÙŠÙ† Ø·Ù…ÙˆØ­ÙŠÙ† ÙŠØ­Ù‚Ù‚ÙˆÙ† Ø£Ø­Ù„Ø§Ù… Ø£Ø¨Ù†Ø§Ø¦Ù‡Ù… ÙˆØ´Ø¨Ø§Ø¨ ÙˆØ·Ù†ÙŠØ§Øª Ù…Ø³ØªÙ†ÙØ°Ø© Ù„Ø±Ø¤ÙŠØ© Ø£ÙƒØ¨Ø± Ø§Ù†ØªØµØ§Ø± Ù„Ù„Ø¥Ø³Ù„Ø§Ù… Ø¹Ø§Ù„Ù…ÙŠØ§ !\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"ÙÙ‡Ù… Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ù…Ø«Ù„ ÙÙ‡Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© â€” ÙƒÙ„Ø§Ù‡Ù…Ø§ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥ØªÙ‚Ø§Ù† Ù„ØªØ¬Ù†Ø¨ ØªØ´ÙˆÙŠÙ‡ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ.\",\n",
      "      \"ÙƒÙ…Ø§ ÙŠÙØ³ØªØ±Ø´ÙØ¯Ù Ø¨Ø§Ù„Ù†Ø¬ÙˆÙ… Ù„ÙØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·Ø±ÙŠÙ‚Ù Ø§Ù„ØµØ­ÙŠØ­, ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ ØªÙˆØ¬ÙŠÙ‡Ù Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù†Ø­Ùˆ ØªØ¹Ø²ÙŠØ² Ù‡ÙˆÙŠØªÙ†Ø§.\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"Ù…Ø«Ø§Ù„ Ø§Ù„Ø£Ù†Ø¯Ù„Ø³ Ø§Ù„Ø°ÙŠ Ø§Ø¬ØªÙ…Ø¹ ÙÙŠÙ‡ Ø§Ù„Ø¹Ù„Ù… ÙˆØ§Ù„ÙÙƒØ± Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ Ù…Ø¹ Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ Ø§Ù„ØªÙ‚Ù†ÙŠ Ø¨Ø§Ø¹ØªØ¨Ø§Ø±Ù‡ Ø­Ø§ÙØ²Ø§ Ù„Ù†Ø§ Ø§Ù„ÙŠÙˆÙ….\",\n",
      "      \"Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø© Ø¶Ù…Ù† Ø¹ØµØ± Ø±Ù‚Ù…ÙŠ.\"\n",
      "    ]\n",
      "  },\n",
      "  \"spontaneous_moments\": {\n",
      "    \"natural_interruptions\": [\n",
      "      \"Ø£Ø­Ù…Ø¯, Ù…Ù† Ø§Ù„Ø±Ø§Ø¦Ø¹ Ø±Ø¤ÙŠØ© ØªØ·Ø¨ÙŠÙ‚Ø§Øª AI Ø§Ù„Ù…ØªÙ†ÙˆØ¹Ø© Ù„ÙƒÙ†, Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙ†Ø§ ÙØ¹Ù„Ø§Ù‹ ÙÙŠ ØªØ·ÙˆÙŠØ± Ø£Ø¯ÙˆØ§Øª Ø­ÙØ¸ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ù… Ø³ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ ØªØ¨Ø³ÙŠØ· Ø«Ù‚Ø§ÙØªÙ†Ø§ Ø§Ù„ØºÙ†ÙŠØ© ÙˆØ§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø·Ø¨Ù‚Ø§ØªØŸ\",\n",
      "      \"ÙØ§ØªÙ†, ÙƒÙ…Ø®ØªØµØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„, ÙƒÙŠÙ ØªØ±ÙŠÙ† Ø¯ÙˆØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ®Ù„Ù‚ Ù…Ø­ØªÙˆÙ‰ Ø±Ù‚Ù…ÙŠ ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©?\"\n",
      "    ],\n",
      "    \"emotional_reactions\": [\n",
      "      \"Ø£Ø­Ù…Ø¯(Ø¨Ø§Ø³Ù„ÙˆØ¨ Ù…ØªØ­Ù…Ø³), Ù‡Ù†Ø§Ùƒ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ù„Ø§ØªØ­Ø§Ø¯ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ÙŠÙˆÙ… ÙˆÙ…Ø¹Ø§ØµØ±Ø© Ø§Ù„ÙÙ†ÙˆÙ† Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©! Ù„Ù†Ø­ØªØ¶Ù† Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø± Ø¨ÙŠÙ†Ù…Ø§ Ù†ÙØªØ®Ø± Ø¨Ø£ØµÙˆÙ„Ù†Ø§.,\",\n",
      "      \"ÙØ§ØªÙ†(Ø¨Ø®Ø·Ø§Ø¨ Ù…Ø¯Ø±ÙˆØ³), Ø·Ø¨Ø¹Ø§Ù‹, ÙŠØ¬Ø¨ ØªÙˆØ¬ÙŠÙ‡ Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù†Ø­Ùˆ ØªÙ‚Ø¯ÙŠÙ… Ù‚ÙŠÙ…Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø¬ØªÙ…Ø¹ Ø¹Ø±Ø¨ÙŠ Ø±Ù‚Ù…ÙŠØ§Ù‹ Ø«Ø±ÙŠ ÙˆÙ…ØªÙ…Ø§Ø³Ùƒ.\"\n",
      "    ],\n",
      "    \"personal_stories\": [\n",
      "      \"Ø´Ø§Ø±ÙƒÙ Ø£Ø­Ù…Ø¯ Ø±Ø­Ù„ØªÙ‡ Ø§Ù„Ù…Ù„Ù‡Ù…Ø© Ø­ÙˆÙ„ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø§Ø´ÙŠÙ†ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ø´Ø¹Ø± Ø¨Ø¯ÙŠØ¹ Ø¨Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ.\",\n",
      "      \"ØªØ­Ø¯Ø«Øª Ø¯. ÙØ§ØªÙ† Ø¹Ù† Ù…Ø´Ø±ÙˆØ¹Ù‡Ø§ Ø§Ù„Ø£Ø®ÙŠØ± Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø¹Ù„Ù‰ ÙÙ‡Ù… ÙˆØ¥Ø´Ø±Ø§Ùƒ Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø§Ù„Ø¹Ø±Ø¨ Ø¨Ø­ÙƒØ§ÙŠØ§ØªÙ‡Ù… Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© Ø§Ù„Ø´Ù‡ÙŠØ±Ø©.\"\n",
      "    ]\n",
      "  },\n",
      "  \"dialogue_techniques\": {\n",
      "    \"questioning_styles\": [\n",
      "      \"Ø£Ø­Ù…Ø¯ ÙŠÙˆØ¬Ù‡ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…ÙØªÙˆØ­Ø§Ù‹ Ù„Ø£Ø³ØªØ§Ø°ØªÙ†Ø§:\",\n",
      "      \"ØªØ­Ø«Ù‘ÙŠÙ†Ø§ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ø§Ù„Ø³Ù…Ø§Ø¹ Ù…Ù† Ø®Ù„Ø§Ù„ Ø·Ø±Ø­Ù‡Ø§ Ù„Ø³Ø¤Ø§Ù„ Ù…ØºÙ„Ù‚.\"\n",
      "    ],\n",
      "    \"storytelling_moments\": [\n",
      "      \"ÙŠØ¹Ø¨Ø± Ø£Ø­Ù…Ø¯ Ø¨ØµØ±Ø§Ø­Ø©Ø¹Ù† Ù‚ØµØ© ØªÙØ±Ø¯Ù‡ Ù…Ø¹ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ùˆ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ø·ÙÙˆÙ„Ø©,\",\n",
      "      \"ØªØ­Ø¯ÙÙ‘Ø¯ Ø§Ù„Ø·Ø¨ÙŠØ¨Ø© ÙØ§ØªÙ† Ø¬Ø§Ù†Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨ØªØ¬Ø±Ø¨Ø© Ø´Ø®ØµÙŠØ© Ø­ÙˆÙ„ Ø£Ù‡Ù…ÙÙ‘Ø© ØªØ¹Ù„Ù… Ø§Ù„Ø£Ø·ÙØ§Ù„ Ù„Ù„ØºØ© Ø§Ù„Ø¢Ø¨Ø§Ø¡\"\n",
      "    ],\n",
      "    \"audience_engagement\": [\n",
      "      \"ÙŠØ¬Ø°Ø¨ Ø§Ø­Ù…Ø¯ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø¨Ø¥Ø´Ø±Ø§ÙƒÙ‡Ù… Ø¨Ø³Ø¤Ø§Ù„Ù‡:\",\n",
      "      \"ØªØ´Ø¬Ø¹ ÙØ§Ø·Ù…Ø©Ø§Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ† Ù„ØªØ´Ø§Ø±Ùƒ Ø£ÙÙƒØ§Ø±Ù‡Ù… Ø£Ùˆ Ø£Ø³Ø¦ØªÙ‡Ù… Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø©Ø¨Ø§Ù„topic.\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "polisher = MinimalPolishEnhancer(deployment, model)\n",
    "final_polished_outline = polisher.apply_minimal_polish(topic, information, classification_result, personas_result, result_standard)\n",
    "print(\"Final Polished Outline:\")\n",
    "print(final_polished_outline)\n",
    "# Test the polisher\n",
    "# polished_result = test_minimal_polish_enhancer(\n",
    "#     deployment, topic, information, classification_result, personas_result, result_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd062e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fa49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ae7f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "class ImprovedMicroChunkScriptGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "        \n",
    "        # Simple conversation templates for fallbacks\n",
    "        self.fallback_templates = {\n",
    "            \"intro1\": \"{host_name}: Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù… ÙÙŠ Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©. Ø§Ù„ÙŠÙˆÙ… Ø³Ù†ØªØ­Ø¯Ø« Ø¹Ù† {topic}. Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‡Ù… ÙˆÙ…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù….\",\n",
    "            \"intro2\": \"{host_name}: Ù…Ø¹ÙŠ Ø§Ù„ÙŠÙˆÙ… Ø¶ÙŠÙ Ù…ØªÙ…ÙŠØ²ØŒ {guest_name}. Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ.\\n{guest_name}: Ø£Ù‡Ù„Ø§Ù‹ Ø¨ÙƒØŒ Ø´ÙƒØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¶Ø§ÙØ©. Ø³Ø¹ÙŠØ¯ Ø¨ÙˆØ¬ÙˆØ¯ÙŠ Ù…Ø¹ÙƒÙ….\",\n",
    "            \"discussion\": \"{host_name}: {point_title}ØŒ Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŸ\\n{guest_name}: Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‡Ù… ÙØ¹Ù„Ø§Ù‹. Ø£Ø¹ØªÙ‚Ø¯ Ø£Ù† Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø© Ø¬ÙˆØ§Ù†Ø¨ ÙŠØ¬Ø¨ Ø£Ù† Ù†ÙÙƒØ± ÙÙŠÙ‡Ø§.\",\n",
    "            \"closing\": \"{host_name}: Ø´ÙƒØ±Ø§Ù‹ {guest_name} Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ù…ÙÙŠØ¯.\\n{guest_name}: Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¶Ø§ÙØ©.\\n{host_name}: ÙˆØ´ÙƒØ±Ø§Ù‹ Ù„ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù…. Ù†Ù„Ù‚Ø§ÙƒÙ… ÙÙŠ Ø­Ù„Ù‚Ø© Ù‚Ø§Ø¯Ù…Ø©.\"\n",
    "        }\n",
    "\n",
    "    def generate_intro1_only(self, topic, intro1_data, host_persona):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 1: Generate only intro1 (host speaking alone)\n",
    "        Enhanced with natural, conversational tone and spontaneity\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        host_bg = host_persona.get('background', '')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract essential data\n",
    "        opening_line = intro1_data.get('opening_line', '')\n",
    "        episode_hook = intro1_data.get('episode_hook', '')\n",
    "        \n",
    "        # Get one cultural element if available\n",
    "        cultural_elements = intro1_data.get('cultural_connections', [])\n",
    "        cultural_touch = cultural_elements[0] if cultural_elements else \"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate Arabic podcast opening. Host speaks alone.\n",
    "\n",
    "Host: {host_name}\n",
    "Topic: {topic}\n",
    "Opening line: {opening_line}\n",
    "\n",
    "Requirements:\n",
    "- 2-3 short sentences maximum\n",
    "- Natural Arabic conversation\n",
    "- No emojis or symbols\n",
    "- No meta-text or explanations\n",
    "- Include topic naturally\n",
    "\n",
    "Example:\n",
    "{host_name}: Ù…Ø±Ø­Ø¨Ø§Ù‹ Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§. Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ÙŠÙˆÙ… Ù‡Ùˆ {topic}. Ø´ÙŠØ¡ Ù…Ù‡Ù… Ù†Ø­ØªØ§Ø¬ Ù†ØªÙƒÙ„Ù… Ø¹Ù†Ù‡.\n",
    "\n",
    "Generate only the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are creating natural, spontaneous Arabic dialogue. Avoid formal speech patterns. Make it feel like real conversation with natural hesitations and genuine curiosity.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.8\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Quality check\n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_intro1(topic, host_name, opening_line)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating intro1: {e}\")\n",
    "            return self._get_fallback_intro1(topic, host_name, opening_line)\n",
    "\n",
    "    def generate_intro2_only(self, topic, intro2_data, host_persona, guest_persona, intro1_context):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 2: Generate guest introduction with natural, dynamic exchange\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest_persona.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "        guest_bg = guest_persona.get('background', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Get key topic from context\n",
    "        context = intro1_context[-80:] if len(intro1_context) > 80 else intro1_context\n",
    "        \n",
    "        # Extract data\n",
    "        guest_welcome = intro2_data.get('guest_welcome', '')\n",
    "        guest_bio = intro2_data.get('guest_bio_highlight', '')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate Arabic podcast dialogue. Host introduces guest.\n",
    "\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "Topic: {topic}\n",
    "\n",
    "Requirements:\n",
    "- 4-6 short exchanges\n",
    "- Each person speaks 1-2 sentences maximum\n",
    "- Natural conversation flow\n",
    "- No emojis or symbols\n",
    "- No meta-text or explanations\n",
    "\n",
    "Example:\n",
    "{host_name}: Ù…Ø¹ÙŠ Ø§Ù„ÙŠÙˆÙ… {guest_name}. Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ.\n",
    "{guest_name}: Ø£Ù‡Ù„Ø§Ù‹ {host_name}. Ø´ÙƒØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø¹ÙˆØ©.\n",
    "{host_name}: Ù†ØªÙƒÙ„Ù… Ø§Ù„ÙŠÙˆÙ… Ø¹Ù† {topic}. Ø¥ÙŠØ´ Ø±Ø£ÙŠÙƒØŸ\n",
    "{guest_name}: Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‡Ù… ÙØ¹Ù„Ø§Ù‹.\n",
    "\n",
    "Generate only the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate simple Arabic dialogue between two people. Short exchanges only. No emojis. No explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.85\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_intro2(host_name, guest_name, guest_welcome)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating intro2: {e}\")\n",
    "            return self._get_fallback_intro2(host_name, guest_name, guest_welcome)\n",
    "\n",
    "    def generate_discussion_point(self, topic, point_data, host_persona, guest_persona, previous_context=\"\"):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 3-5: Generate discussion with natural disagreements and interruptions\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest_persona.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract content\n",
    "        point_title = point_data.get('point_title', '')\n",
    "        personal_angle = point_data.get('personal_angle', '')\n",
    "        \n",
    "        # Use only first elements to avoid overwhelming\n",
    "        spontaneous_triggers = point_data.get('spontaneous_triggers', [])\n",
    "        cultural_refs = point_data.get('cultural_references', [])\n",
    "        \n",
    "        trigger = spontaneous_triggers[0] if spontaneous_triggers else \"\"\n",
    "        cultural_ref = cultural_refs[0] if cultural_refs else \"\"\n",
    "        \n",
    "        # Get context\n",
    "        context = previous_context[-80:] if len(previous_context) > 80 else previous_context\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate Arabic podcast discussion between host and guest.\n",
    "\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "Discussion Topic: {point_title}\n",
    "\n",
    "Requirements:\n",
    "- 6-8 short exchanges\n",
    "- Each person speaks 1-2 sentences only\n",
    "- Include some disagreement or different views\n",
    "- Natural conversation flow\n",
    "- No emojis or symbols\n",
    "- No meta-text or explanations\n",
    "\n",
    "Example:\n",
    "{host_name}: Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€{point_title}ØŒ Ø¥ÙŠØ´ Ø±Ø£ÙŠÙƒØŸ\n",
    "{guest_name}: Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹Ù‚Ø¯. Ø£Ø¹ØªÙ‚Ø¯ Ø¥Ù†...\n",
    "{host_name}: ÙˆÙ„ÙƒÙ† Ù…Ø§ ØªÙÙƒØ± Ø¥Ù†...\n",
    "{guest_name}: Ù„Ø§ØŒ Ù…Ø´ Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø©.\n",
    "\n",
    "Generate only the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate simple Arabic discussion. Short sentences. Include some disagreement. No emojis. No explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.9\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_discussion(point_title, host_name, guest_name, personal_angle)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating discussion point: {e}\")\n",
    "            return self._get_fallback_discussion(point_title, host_name, guest_name, personal_angle)\n",
    "\n",
    "    def generate_closing_only(self, topic, closing_data, host_persona, guest_persona, discussion_summary):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 6: Generate natural closing with honest reflections\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'Ø§Ù„Ù…Ù‚Ø¯Ù…')\n",
    "        guest_name = guest_persona.get('name', 'Ø§Ù„Ø¶ÙŠÙ')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract closing elements\n",
    "        conclusion = closing_data.get('conclusion', {})\n",
    "        outro = closing_data.get('outro', {})\n",
    "        \n",
    "        main_takeaways = conclusion.get('main_takeaways', '')\n",
    "        emotional_closure = conclusion.get('emotional_closure', '')\n",
    "        memorable_ending = outro.get('memorable_ending', '')\n",
    "        \n",
    "        # Get discussion summary\n",
    "        summary = discussion_summary[-100:] if len(discussion_summary) > 100 else discussion_summary\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate Arabic podcast closing dialogue.\n",
    "\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "Topic: {topic}\n",
    "\n",
    "Requirements:\n",
    "- 4-5 short exchanges\n",
    "- Each person speaks 1-2 sentences maximum\n",
    "- Thank each other simply\n",
    "- End naturally\n",
    "- No emojis or symbols\n",
    "- No meta-text or explanations\n",
    "\n",
    "Example:\n",
    "{host_name}: ÙƒØ§Ù† Ù†Ù‚Ø§Ø´ Ù…ÙÙŠØ¯ ÙŠØ§ {guest_name}.\n",
    "{guest_name}: Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¶Ø§ÙØ©.\n",
    "{host_name}: Ø´ÙƒØ±Ø§Ù‹ Ù„ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§.\n",
    "{guest_name}: Ù†Ù„Ù‚Ø§ÙƒÙ… Ù‚Ø±ÙŠØ¨Ø§Ù‹.\n",
    "\n",
    "Generate only the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate simple Arabic closing dialogue. Keep it short and natural. No emojis. No explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.75\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_closing(host_name, guest_name, main_takeaways)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating closing: {e}\")\n",
    "            return self._get_fallback_closing(host_name, guest_name, main_takeaways)\n",
    "\n",
    "    def generate_complete_script(self, topic, final_outline_result):\n",
    "        \"\"\"\n",
    "        Main orchestration: Generate complete script using enhanced micro-chunks\n",
    "        \"\"\"\n",
    "        print(\"ğŸ™ï¸ Starting enhanced spontaneous script generation...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            outline = json.loads(final_outline_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid outline JSON format\")\n",
    "        \n",
    "        # Extract data\n",
    "        personas = outline.get(\"personas\", {})\n",
    "        conv_flow = outline.get(\"conversation_flow\", {})\n",
    "        \n",
    "        host_persona = personas.get(\"host\", {})\n",
    "        guest_persona = personas.get(\"guest\", {})\n",
    "        \n",
    "        intro1_data = conv_flow.get(\"intro1\", {})\n",
    "        intro2_data = conv_flow.get(\"intro2\", {})\n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        closing_data = conv_flow.get(\"closing\", {})\n",
    "        \n",
    "        print(f\"ğŸ“‹ Host: {host_persona.get('name', 'Unknown')}\")\n",
    "        print(f\"ğŸ“‹ Guest: {guest_persona.get('name', 'Unknown')}\")\n",
    "        print(f\"ğŸ“‹ Discussion Points: {len(main_discussion)}\")\n",
    "        \n",
    "        # Micro-Chunk 1: Natural Intro1\n",
    "        print(\"\\nğŸ“ Chunk 1: Natural host introduction...\")\n",
    "        intro1_dialogue = self.generate_intro1_only(topic, intro1_data, host_persona)\n",
    "        print(\"âœ… Host introduction completed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunk 2: Dynamic Intro2\n",
    "        print(\"\\nğŸ“ Chunk 2: Dynamic guest introduction...\")\n",
    "        intro2_dialogue = self.generate_intro2_only(topic, intro2_data, host_persona, guest_persona, intro1_dialogue)\n",
    "        print(\"âœ… Guest introduction completed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunks 3-5: Spontaneous Discussion Points\n",
    "        discussion_parts = []\n",
    "        previous_context = intro2_dialogue\n",
    "        \n",
    "        for i, point_data in enumerate(main_discussion):\n",
    "            print(f\"\\nğŸ“ Chunk {i+3}: Spontaneous discussion point {i+1}...\")\n",
    "            point_dialogue = self.generate_discussion_point(\n",
    "                topic, point_data, host_persona, guest_persona, previous_context\n",
    "            )\n",
    "            discussion_parts.append(point_dialogue)\n",
    "            previous_context = point_dialogue\n",
    "            print(f\"âœ… Discussion point {i+1} completed\")\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunk 6: Honest Closing\n",
    "        print(f\"\\nğŸ“ Chunk {len(main_discussion)+3}: Honest closing...\")\n",
    "        discussion_summary = \" \".join(discussion_parts[-2:])\n",
    "        closing_dialogue = self.generate_closing_only(topic, closing_data, host_persona, guest_persona, discussion_summary)\n",
    "        print(\"âœ… Closing completed\")\n",
    "        \n",
    "        # Combine all parts\n",
    "        complete_intro = intro1_dialogue + \"\\n\\n\" + intro2_dialogue\n",
    "        complete_discussion = \"\\n\\n\".join(discussion_parts)\n",
    "        \n",
    "        complete_script = f\"\"\"=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\n",
    "{complete_intro}\n",
    "\n",
    "=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\n",
    "{complete_discussion}\n",
    "\n",
    "=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\n",
    "{closing_dialogue}\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ‰ Enhanced spontaneous script generation completed!\")\n",
    "        \n",
    "        # Enhanced quality assessment\n",
    "        total_quality = self._assess_script_quality(complete_script, outline)\n",
    "        \n",
    "        return {\n",
    "            \"intro\": complete_intro,\n",
    "            \"main_discussion\": complete_discussion,\n",
    "            \"closing\": closing_dialogue,\n",
    "            \"complete_script\": complete_script,\n",
    "            \"script_length\": len(complete_script),\n",
    "            \"estimated_duration\": f\"{len(main_discussion) * 2 + 4}-{len(main_discussion) * 3 + 6} minutes\",\n",
    "            \"quality_score\": total_quality,\n",
    "            \"generation_method\": \"enhanced-spontaneous-micro-chunks\",\n",
    "            \"chunks_generated\": len(main_discussion) + 3,\n",
    "            \"personas_used\": {\n",
    "                \"host\": host_persona.get('name', 'Unknown'),\n",
    "                \"guest\": guest_persona.get('name', 'Unknown')\n",
    "            },\n",
    "            \"cultural_elements_integrated\": self._count_cultural_elements(complete_script),\n",
    "            \"spontaneity_level\": \"high\",\n",
    "            \"natural_interruptions\": self._count_interruptions(complete_script),\n",
    "            \"disagreement_instances\": self._count_disagreements(complete_script),\n",
    "            \"enhancement_level\": \"spontaneous\"\n",
    "        }\n",
    "\n",
    "    def _assess_quality(self, text):\n",
    "        \"\"\"Enhanced quality assessment including spontaneity markers\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return 0\n",
    "            \n",
    "        # Check for meta-text (penalty)\n",
    "        meta_indicators = ['Ù…Ù„Ø§Ø­Ø¸Ø©:', 'ØªÙ†ØªÙ‡ÙŠ', 'Note:', 'Format:', 'Generate', 'Requirements']\n",
    "        has_meta = any(indicator in text for indicator in meta_indicators)\n",
    "        \n",
    "        # Check for natural conversation markers (bonus)\n",
    "        natural_markers = ['ÙŠØ¹Ù†ÙŠ', 'Ø¨ØµØ±Ø§Ø­Ø©', 'Ø£Ù…Ù…Ù…', 'Ù„Ø­Ø¸Ø©', 'ÙØ¹Ù„Ø§Ù‹ØŸ', 'Ø§Ø¹Ø°Ø±Ù†ÙŠ', 'ÙˆÙ„ÙƒÙ†', 'Ù†Ø¹Ù…ØŒ ÙˆÙ„ÙƒÙ†']\n",
    "        natural_score = sum(3 for marker in natural_markers if marker in text)\n",
    "        \n",
    "        # Check for excessive politeness (penalty)\n",
    "        excessive_politeness = ['Ø´ÙƒØ±Ø§Ù‹ Ø¬Ø²ÙŠÙ„Ø§Ù‹', 'Ø£Ø´ÙƒØ±Ùƒ Ø¨Ø§Ù„Øº Ø§Ù„Ø´ÙƒØ±', 'Ù…Ù…ØªÙ† Ù„Ù„ØºØ§ÙŠØ©', 'Ø´Ø±Ù Ø¹Ø¸ÙŠÙ…']\n",
    "        politeness_penalty = sum(5 for phrase in excessive_politeness if phrase in text)\n",
    "        \n",
    "        # Check spacing quality\n",
    "        spacing_score = 80 if not re.search(r'[^\\s]{30,}', text) else 40\n",
    "        \n",
    "        # Check Arabic content ratio\n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "        total_chars = len(text)\n",
    "        arabic_ratio = arabic_chars / total_chars if total_chars > 0 else 0\n",
    "        \n",
    "        # Check dialogue structure (more turns = better)\n",
    "        dialogue_turns = text.count(':')\n",
    "        structure_score = min(25, dialogue_turns * 3)  # Reward more turns\n",
    "        \n",
    "        # Calculate quality\n",
    "        quality = spacing_score + (arabic_ratio * 30) + structure_score + natural_score\n",
    "        \n",
    "        # Apply penalties\n",
    "        if has_meta:\n",
    "            quality -= 40\n",
    "        if arabic_ratio < 0.5:\n",
    "            quality -= 20\n",
    "        quality -= politeness_penalty\n",
    "            \n",
    "        return min(100, max(0, int(quality)))\n",
    "\n",
    "    def _count_interruptions(self, script):\n",
    "        \"\"\"Count natural interruption markers\"\"\"\n",
    "        interruption_markers = ['Ø§Ø¹Ø°Ø±Ù†ÙŠ', 'Ù„Ø­Ø¸Ø©', 'Ù†Ø¹Ù…ØŒ ÙˆÙ„ÙƒÙ†', 'Ù„Ø§ØŒ Ù„Ø§', 'ÙØ¹Ù„Ø§Ù‹ØŸ']\n",
    "        return sum(script.count(marker) for marker in interruption_markers)\n",
    "\n",
    "    def _count_disagreements(self, script):\n",
    "        \"\"\"Count disagreement/challenge markers\"\"\"\n",
    "        disagreement_markers = ['Ù„Ø§ Ø£ØªÙÙ‚', 'ÙˆÙ„ÙƒÙ†', 'Ù‡Ø°Ø§ ØµØ­ÙŠØ­ØŒ Ù„ÙƒÙ†', 'ÙƒÙŠÙ ØªÙØ³Ø±', 'Ø£Ù„Ø§ ØªØ¹ØªÙ‚Ø¯']\n",
    "        return sum(script.count(marker) for marker in disagreement_markers)\n",
    "\n",
    "    def _assess_script_quality(self, script, outline):\n",
    "        \"\"\"Enhanced script quality assessment with spontaneity metrics\"\"\"\n",
    "        individual_quality = self._assess_quality(script)\n",
    "        \n",
    "        # Check persona usage\n",
    "        personas = outline.get(\"personas\", {})\n",
    "        host_name = personas.get(\"host\", {}).get(\"name\", \"\")\n",
    "        guest_name = personas.get(\"guest\", {}).get(\"name\", \"\")\n",
    "        \n",
    "        persona_score = 0\n",
    "        if host_name and host_name in script:\n",
    "            persona_score += 10\n",
    "        if guest_name and guest_name in script:\n",
    "            persona_score += 10\n",
    "        \n",
    "        # Check structure completeness\n",
    "        required_sections = [\"=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\", \"=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\", \"=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\"]\n",
    "        structure_score = sum(10 for section in required_sections if section in script)\n",
    "        \n",
    "        # Check dialogue balance (reward more turns)\n",
    "        total_turns = script.count(':')\n",
    "        balance_score = min(25, total_turns * 2)\n",
    "        \n",
    "        # Check spontaneity elements\n",
    "        spontaneity_score = min(15, self._count_interruptions(script) * 2 + self._count_disagreements(script) * 3)\n",
    "        \n",
    "        total_score = min(100, individual_quality + persona_score + structure_score + balance_score + spontaneity_score)\n",
    "        return total_score\n",
    "\n",
    "    def _count_cultural_elements(self, script):\n",
    "        \"\"\"Count cultural elements in the script\"\"\"\n",
    "        cultural_indicators = [\n",
    "            'Ù…Ø«Ù„', 'Ø­ÙƒÙ…Ø©', 'ØªØ±Ø§Ø«', 'Ø«Ù‚Ø§ÙØ©', 'Ø¹Ø±Ø¨ÙŠ', 'Ø¥Ø³Ù„Ø§Ù…ÙŠ', 'ØªØ§Ø±ÙŠØ®',\n",
    "            'Ø´ÙˆÙ‚ÙŠ', 'Ø§Ø¨Ù†', 'Ù‚Ø§Ù„', 'Ø­Ø¯ÙŠØ«', 'Ù‚Ø±Ø¢Ù†', 'Ø´Ø¹Ø±'\n",
    "        ]\n",
    "        return sum(1 for indicator in cultural_indicators if indicator in script)\n",
    "\n",
    "    def _get_fallback_intro1(self, topic, host_name, opening_line=\"\"):\n",
    "        \"\"\"Enhanced fallback with more natural tone\"\"\"\n",
    "        base_opening = opening_line if opening_line else f\"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§\"\n",
    "        return f\"{host_name}: {base_opening}... Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ù…ÙˆØ¶ÙˆØ¹ {topic} ÙŠØ´ØºÙ„ Ø¨Ø§Ù„ÙŠ Ù…Ù† ÙØªØ±Ø©. ÙŠØ¹Ù†ÙŠØŒ ÙƒÙ„Ù†Ø§ Ù†ÙˆØ§Ø¬Ù‡ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± Ø¨Ø´ÙƒÙ„ Ø£Ùˆ Ø¨Ø¢Ø®Ø±ØŒ ØµØ­ØŸ\"\n",
    "\n",
    "    def _get_fallback_intro2(self, host_name, guest_name, guest_welcome=\"\"):\n",
    "        \"\"\"Enhanced fallback with quick exchanges\"\"\"\n",
    "        return f\"\"\"{host_name}: Ù…Ø¹ÙŠ Ø§Ù„ÙŠÙˆÙ… {guest_name}. Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ.\n",
    "\n",
    "{guest_name}: Ø£Ù‡Ù„Ø§Ù‹ {host_name}. Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ù‡ Ù…Ø­ØªØ§Ø¬ Ù†Ù‚Ø§Ø´ Ø¬Ø¯ÙŠ.\n",
    "\n",
    "{host_name}: ÙØ¹Ù„Ø§Ù‹ØŸ ÙŠØ¹Ù†ÙŠ Ø§Ù†Øª Ø´Ø§ÙŠÙ Ø¥Ù†...\n",
    "\n",
    "{guest_name}: Ø§Ø¹Ø°Ø±Ù†ÙŠØŒ Ø®Ù„ÙŠÙ†ÙŠ Ø£ÙˆØ¶Ø­ ÙˆØ¬Ù‡Ø© Ù†Ø¸Ø±ÙŠ Ø§Ù„Ø£ÙˆÙ„.\"\"\"\n",
    "\n",
    "    def _get_fallback_discussion(self, point_title, host_name, guest_name, personal_angle=\"\"):\n",
    "        \"\"\"Enhanced fallback with disagreement\"\"\"\n",
    "        return f\"\"\"{host_name}: Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…ÙˆØ¶ÙˆØ¹ {point_title}ØŒ Ø¥ÙŠØ´ Ø±Ø£ÙŠÙƒØŸ\n",
    "\n",
    "{guest_name}: Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‡Ù…ØŒ Ø¨Ø³...\n",
    "\n",
    "{host_name}: Ù„Ø­Ø¸Ø©ØŒ \"Ø¨Ø³\" Ø¥ÙŠØ´ØŸ\n",
    "\n",
    "{guest_name}: ÙŠØ¹Ù†ÙŠØŒ Ø§Ù„Ù†Ø§Ø³ ØªÙÙ‡Ù… Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ØºÙ„Ø· Ø£Ø­ÙŠØ§Ù†Ø§Ù‹.\n",
    "\n",
    "{host_name}: ÙˆÙ„ÙƒÙ† Ø£Ù„Ø§ ØªØ¹ØªÙ‚Ø¯ Ø£Ù†...\n",
    "\n",
    "{guest_name}: Ù„Ø§ØŒ Ø§Ø¹Ø°Ø±Ù†ÙŠØŒ Ù‡Ø°Ø§ Ù…Ùˆ ØµØ­ÙŠØ­ ØªÙ…Ø§Ù…Ø§Ù‹.\"\"\"\n",
    "\n",
    "    def _get_fallback_closing(self, host_name, guest_name, main_takeaways=\"\"):\n",
    "        \"\"\"Enhanced fallback with honest reflection\"\"\"\n",
    "        return f\"\"\"{host_name}: Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ø§Ù„Ù†Ù‚Ø§Ø´ ÙƒØ§Ù† Ù…Ø«ÙŠØ± Ù„Ù„Ø¬Ø¯Ù„ Ø´ÙˆÙŠØ©.\n",
    "\n",
    "{guest_name}: ÙØ¹Ù„Ø§Ù‹ØŒ Ø¨Ø³ Ù‡Ø°Ø§ Ø´ÙŠØ¡ ÙƒÙˆÙŠØ³.\n",
    "\n",
    "{host_name}: Ø¥ÙŠØ´ Ø±Ø£ÙŠÙƒÙ… Ø§Ù†ØªÙˆØ§ØŒ Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ØŸ\n",
    "\n",
    "{guest_name}: ÙˆØ§Ù„Ù„Ù‡ Ù…ÙˆØ¶ÙˆØ¹ ÙŠØ³ØªØ§Ù‡Ù„ Ù†Ù‚Ø§Ø´ Ø£ÙƒØ«Ø±.\n",
    "\n",
    "{host_name}: Ù†Ù„Ù‚Ø§ÙƒÙ… Ù‚Ø±ÙŠØ¨ Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡.\"\"\"\n",
    "\n",
    "    def validate_script_quality(self, script_result):\n",
    "        \"\"\"Enhanced validation including spontaneity metrics\"\"\"\n",
    "        complete_script = script_result.get(\"complete_script\", \"\")\n",
    "        quality_score = script_result.get(\"quality_score\", 0)\n",
    "        \n",
    "        validation = {\n",
    "            \"has_structure\": all(section in complete_script for section in [\"=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\", \"=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\", \"=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\"]),\n",
    "            \"arabic_content\": bool(re.search(r'[\\u0600-\\u06FF]', complete_script)),\n",
    "            \"no_meta_text\": not any(indicator in complete_script for indicator in ['Ù…Ù„Ø§Ø­Ø¸Ø©:', 'ØªÙ†ØªÙ‡ÙŠ', 'Note:', 'Format:', 'Generate']),\n",
    "            \"proper_spacing\": not bool(re.search(r'[^\\s]{40,}', complete_script)),\n",
    "            \"dialogue_balance\": complete_script.count(':') >= 12,  # Higher threshold for more turns\n",
    "            \"persona_presence\": script_result.get(\"personas_used\", {}).get(\"host\", \"\") in complete_script,\n",
    "            \"natural_interruptions\": script_result.get(\"natural_interruptions\", 0) >= 2,\n",
    "            \"disagreement_instances\": script_result.get(\"disagreement_instances\", 0) >= 1,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"quality_grade\": \"Ù…Ù…ØªØ§Ø²\" if quality_score >= 90 else \"Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹\" if quality_score >= 85 else \"Ø¬ÙŠØ¯\" if quality_score >= 80 else \"Ù…Ù‚Ø¨ÙˆÙ„\" if quality_score >= 70 else \"Ø¶Ø¹ÙŠÙ\"\n",
    "        }\n",
    "        \n",
    "        validation[\"overall_valid\"] = all([\n",
    "            validation[\"has_structure\"],\n",
    "            validation[\"arabic_content\"],\n",
    "            validation[\"no_meta_text\"],\n",
    "            validation[\"proper_spacing\"],\n",
    "            validation[\"dialogue_balance\"],\n",
    "            validation[\"persona_presence\"],\n",
    "            quality_score >= 75\n",
    "        ])\n",
    "        \n",
    "        return validation\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_improved_script_generator(deployment, topic, final_outline_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the improved spontaneous micro-chunk script generator\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª Testing Improved Spontaneous Script Generator...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = ImprovedMicroChunkScriptGenerator(deployment, model_name)\n",
    "    \n",
    "    # Generate script\n",
    "    script_result = generator.generate_complete_script(topic, final_outline_result)\n",
    "    \n",
    "    # Validate script\n",
    "    validation = generator.validate_script_quality(script_result)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Script Generation Results:\")\n",
    "    print(f\"Quality Score: {script_result['quality_score']}/100\")\n",
    "    print(f\"Quality Grade: {validation['quality_grade']}\")\n",
    "    print(f\"Script Length: {script_result['script_length']:,} characters\")\n",
    "    print(f\"Estimated Duration: {script_result['estimated_duration']}\")\n",
    "    print(f\"Chunks Generated: {script_result['chunks_generated']}\")\n",
    "    print(f\"Spontaneity Level: {script_result['spontaneity_level']}\")\n",
    "    print(f\"Natural Interruptions: {script_result['natural_interruptions']}\")\n",
    "    print(f\"Disagreement Instances: {script_result['disagreement_instances']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Validation Results:\")\n",
    "    print(f\"Overall Valid: {'âœ…' if validation['overall_valid'] else 'âŒ'}\")\n",
    "    print(f\"Structure: {'âœ…' if validation['has_structure'] else 'âŒ'}\")\n",
    "    print(f\"Arabic Content: {'âœ…' if validation['arabic_content'] else 'âŒ'}\")\n",
    "    print(f\"No Meta Text: {'âœ…' if validation['no_meta_text'] else 'âŒ'}\")\n",
    "    print(f\"Proper Spacing: {'âœ…' if validation['proper_spacing'] else 'âŒ'}\")\n",
    "    print(f\"Dialogue Balance: {'âœ…' if validation['dialogue_balance'] else 'âŒ'}\")\n",
    "    print(f\"Natural Interruptions: {'âœ…' if validation['natural_interruptions'] else 'âŒ'}\")\n",
    "    print(f\"Disagreement Instances: {'âœ…' if validation['disagreement_instances'] else 'âŒ'}\")\n",
    "    \n",
    "    # Show sample dialogue\n",
    "    print(f\"\\nğŸ™ï¸ Sample Script Preview:\")\n",
    "    script_lines = script_result['complete_script'].split('\\n')\n",
    "    preview_lines = script_lines[:20]  # More lines to show spontaneity\n",
    "    for line in preview_lines:\n",
    "        if line.strip():\n",
    "            print(f\"  {line[:120]}...\")\n",
    "    \n",
    "    if len(script_lines) > 20:\n",
    "        print(f\"  ... [+{len(script_lines)-20} more lines]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7838c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™ï¸ Starting enhanced spontaneous script generation...\n",
      "============================================================\n",
      "ğŸ“‹ Host: Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ\n",
      "ğŸ“‹ Guest: Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯\n",
      "ğŸ“‹ Discussion Points: 3\n",
      "\n",
      "ğŸ“ Chunk 1: Natural host introduction...\n",
      "âœ… Host introduction completed\n",
      "\n",
      "ğŸ“ Chunk 2: Dynamic guest introduction...\n",
      "âœ… Guest introduction completed\n",
      "\n",
      "ğŸ“ Chunk 3: Spontaneous discussion point 1...\n",
      "âœ… Discussion point 1 completed\n",
      "\n",
      "ğŸ“ Chunk 4: Spontaneous discussion point 2...\n",
      "âœ… Discussion point 2 completed\n",
      "\n",
      "ğŸ“ Chunk 5: Spontaneous discussion point 3...\n",
      "âœ… Discussion point 3 completed\n",
      "\n",
      "ğŸ“ Chunk 6: Honest closing...\n",
      "âœ… Closing completed\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ Enhanced spontaneous script generation completed!\n",
      "ğŸ§ª Testing Improved Spontaneous Script Generator...\n",
      "============================================================\n",
      "ğŸ™ï¸ Starting enhanced spontaneous script generation...\n",
      "============================================================\n",
      "ğŸ“‹ Host: Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ\n",
      "ğŸ“‹ Guest: Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯\n",
      "ğŸ“‹ Discussion Points: 3\n",
      "\n",
      "ğŸ“ Chunk 1: Natural host introduction...\n",
      "âœ… Host introduction completed\n",
      "\n",
      "ğŸ“ Chunk 2: Dynamic guest introduction...\n",
      "âœ… Guest introduction completed\n",
      "\n",
      "ğŸ“ Chunk 3: Spontaneous discussion point 1...\n",
      "âœ… Discussion point 1 completed\n",
      "\n",
      "ğŸ“ Chunk 4: Spontaneous discussion point 2...\n",
      "âœ… Discussion point 2 completed\n",
      "\n",
      "ğŸ“ Chunk 5: Spontaneous discussion point 3...\n",
      "âœ… Discussion point 3 completed\n",
      "\n",
      "ğŸ“ Chunk 6: Honest closing...\n",
      "âœ… Closing completed\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ Enhanced spontaneous script generation completed!\n",
      "\n",
      "ğŸ“Š Script Generation Results:\n",
      "Quality Score: 100/100\n",
      "Quality Grade: Ù…Ù…ØªØ§Ø²\n",
      "Script Length: 2,914 characters\n",
      "Estimated Duration: 10-15 minutes\n",
      "Chunks Generated: 6\n",
      "Spontaneity Level: high\n",
      "Natural Interruptions: 0\n",
      "Disagreement Instances: 0\n",
      "\n",
      "ğŸ“ˆ Validation Results:\n",
      "Overall Valid: âŒ\n",
      "Structure: âœ…\n",
      "Arabic Content: âœ…\n",
      "No Meta Text: âŒ\n",
      "Proper Spacing: âœ…\n",
      "Dialogue Balance: âœ…\n",
      "Natural Interruptions: âŒ\n",
      "Disagreement Instances: âŒ\n",
      "\n",
      "ğŸ™ï¸ Sample Script Preview:\n",
      "  === Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===...\n",
      "  Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø¬Ù…ÙŠØ¹Ø§Ù‹ØŒ Ù‚Ø¨Ù„ Ø£Ù† ÙŠÙ…ØªØ²Ø¬ Ø¹Ø§Ù„Ù… Ø§Ù„Ø¨ØªØ§Øª ÙˆØ§Ù„Ø£ØµÙØ§Ø± Ø¨Ø­ÙŠØ§ØªÙ†Ø§ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù„Ø§ Ø±Ø¬Ø¹Ø© ÙÙŠÙ‡ØŒ Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø®ÙˆØ¶ Ù…Ø¹Ø§Ù‹ Ø±Ø­...\n",
      "  Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ù†Ù„ØªÙ‚ÙŠ Ù‡Ù†Ø§ Ù…Ø¹ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯ Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ø¯ÙˆØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ¨Ù‚Ø§Ø¡ Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ø§ Ø£ÙˆÙ„Ù‰ Ù…Ù„Ø§Ø­Ø¸Ø§ØªÙƒ...\n",
      "  Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: ÙŠÙ†Ø¨ØºÙŠ Ø£Ù† Ù†Ø³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø¯ÙˆÙ† ÙÙ‚Ø¯Ø§Ù† Ø¬Ø°ÙˆØ±Ù†Ø§ØŒ ÙˆÙ‡Ø°Ø§ ØªØ­Ø¯Ù Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…....\n",
      "  Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ÙˆÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ Ø¨Ø§Ù„ØªØ­Ø¯ÙÙ‘Ø« ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ÙˆØ§Ù„Ø«Ù‚Ø§ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŸ...\n",
      "  Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ØªÙˆÙ‰ Ø±Ù‚Ù…ÙŠ Ø¹Ø±Ø¨ÙŠ Ø£ØµÙŠÙ„ ÙˆÙ…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¬ÙŠØ§Ù„ Ù‡Ù…Ø§ Ù…ÙØªØ§Ø­ ÙØªØ­ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø£Ù…Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ ÙˆØ§Ø­...\n",
      "  === Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===...\n",
      "  **Ø£Ø­Ù…Ø¯:** Ø¨Ø¯Ø¡Ø§Ù‹ Ù…Ù† Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŒ ÙƒÙŠÙ ØªØ¹ØªØ¨Ø±ÙŠÙ† Ø¯ÙˆØ±Ù‡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù‡Ù†Ø§?...\n",
      "  **ÙØ§ØªÙ†:** Ø£Ù…Ø±ÙŒ ÙŠØ­ØªØ§Ø¬Ù Ù„ØªØ¬Ø±Ø¯Ù Ø¹Ù…ÙŠÙ‚ Ù„ÙƒÙ†Ù†ÙŠ Ø£Ø±Ù‰ Ø£Ù† Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ decisive ÙÙŠÙ‡Ø§....\n",
      "  **Ø£Ø­Ù…Ø¯:** ØµØ­ÙŠØ­, Ù„ÙƒÙ† Ù…Ø§Ø°Ø§ Ø¹Ù† Ø¯ÙˆØ± Ø§Ù„Ø£ÙØ±Ø§Ø¯ ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù‡?...\n",
      "  **ÙØ§ØªÙ†:** ÙŠÙ†ÙØ¹ Ù„Ù„Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ù„ØªØºÙŠÙŠØ± Ù„ÙƒÙ†Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙŠØ±Ø¬Ø¹ Ù„Ù„Ù‚Ø§Ø¯Ø© ÙƒÙ…Ø§ Ø£Ø´Ø±Øª Ù„Ø£ÙˆÙ„ Ù…Ø±Ø©....\n",
      "  **Ø£Ø­Ù…Ø¯:** Ø±ØºÙ… Ø°Ù„Ùƒ Ø§Ù„Ø¨Ø¹Ø¶ ÙŠÙ‚ÙˆÙ„ Ø£Ù†Ù‡ ÙŠØ¹ØªÙ…Ø¯ Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ¨Ø± Ø¹Ù„Ù‰ Ø¬Ù‡ÙˆØ¯ ÙØ±Ø¯ÙŠØ©.....\n",
      "  **ÙØ§ØªÙ†:** Ù‡Ø°Ø§ Ù†Ø¶Ø±Ù‡ Ø¨Ø³ÙŠØ·Ø© Ø¬Ø¯Ø§Ù‹. ÙŠØªØ·Ù„Ø¨ Ø¬Ù‡Ø¯ Ø§Ù„Ø¬Ù…ÙŠØ¹ Ø¶Ù…Ù† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø´Ø§Ù…Ù„Ø© ÙˆØ§Ø¶Ø­Ø©....\n",
      "  ... [+34 more lines]\n",
      "Quality: 100/100\n",
      "Duration: 10-15 minutes\n",
      "Cultural Elements: 1\n",
      "Personas: {'host': 'Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ', 'guest': 'Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯'}\n"
     ]
    }
   ],
   "source": [
    "# Generate enhanced script\n",
    "generator = ImprovedMicroChunkScriptGenerator(deployment, model)\n",
    "script_result = generator.generate_complete_script(topic, final_polished_outline)\n",
    "\n",
    "# Test and validate\n",
    "test_result = test_improved_script_generator(deployment, topic, final_polished_outline)\n",
    "\n",
    "# Access enhanced results\n",
    "print(f\"Quality: {script_result['quality_score']}/100\")\n",
    "print(f\"Duration: {script_result['estimated_duration']}\")\n",
    "print(f\"Cultural Elements: {script_result['cultural_elements_integrated']}\")\n",
    "print(f\"Personas: {script_result['personas_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f421dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intro': 'Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø¬Ù…ÙŠØ¹Ø§Ù‹ØŒ ÙˆÙ†Ø±Ø­Ø¨ Ø¨ÙƒÙ… ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„ØªÙŠ Ù†ØºÙˆØµ ÙÙŠÙ‡Ø§ Ø¨Ø¹Ù…Ù‚ Ù„ØªÙÙ‡Ù… Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ø­Ø³Ø§Ø³ Ø¨ÙŠÙ† Ø«ÙˆØ±ØªÙ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ ÙˆØ«Ø±ÙˆØªÙ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙˆØ§Ø¹Ø¯Ø©Ø› Ø¯Ø¹ÙˆÙ†Ø§ Ù†ØªØ´Ø§Ø±Ùƒ Ø¨Ø§Ù‡ØªÙ…Ø§Ù… Ù†Ø³Ø¨Ø± ÙÙŠÙ‡ Ø¢ÙØ§Ù‚ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø²Ø¬ Ø§Ù„ÙØ±ÙŠØ¯ Ù„Ù…Ø³ØªÙ‚Ø¨Ù„Ù†Ø§ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ØªØ±Ø­ÙŠØ¨ Ù„Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ğ Ğ°ÑˆĞ¸Ğ´ØŒ Ù…Ù† Ø§Ù„Ø±Ø§Ø¦Ø¹ Ø£Ù†Ù‘Ù‡Ø§ Ù…Ø¹Ù†Ø§ Ø§Ù„ÙŠÙˆÙ… Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ù…Ø³Ø£Ù„Ø© Ø­ÙŠÙˆÙŠØ© Ø­ÙˆÙ„ Ø§Ù„ØªØ±Ø§Ø¨Ø· Ø¨ÙŠÙ† ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆÙ‡ÙˆÛŒØªÙÙ†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©.\\n\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** Ø´ÙƒØ± Ù„ÙƒØŒ Ù†Ø¨Ø­Ø« Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ­Ø¯ÙŠØ§Øª Ù…Ø«ÙŠØ±Ø© ØªØªØ¹Ù„Ù‚ Ø¨Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© ÙÙŠ Ø¹Ø§Ù„Ù… Ù…ØªØ±Ø§Ø¨Ø· Ø¨Ø´ÙƒÙ„ Ø±Ù‚Ù…ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ÙƒÙŠÙ Ø¨Ø¥Ù…ÙƒØ§Ù†Ù†Ø§ Ø¶Ù…Ù‘ ØªÙ‚Ù†ÙŠÙ‘Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ù„Ø¯Ø¹Ù… Ù‚ÙŠÙ…Ù†Ø§ ÙˆÙ…Ø¹ØªÙ‚Ø¯Ø§ØªÙ†Ø§ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªÙ‡Ø¯ÙŠØ¯ ÙˆØ¬ÙˆØ¯Ù‡Ø§?\\n\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** ÙŠØ¬Ø¨ Ø£Ù† Ù†Ø±ÙƒØ² Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø­Ù„ÙˆÙ„ Ø°ÙƒÙŠØ© ØªØ³ØªÙ„Ù‡Ù… Ø¬Ø°ÙˆØ±Ù‡Ø§ Ø§Ù„Ù‚ÙŠÙ…ÙŠÙ‘Ø© Ø§Ù„Ø«Ù‚Ø§ÙÙŠÙ‘Ø© Ø¨ÙŠÙ†Ù…Ø§ ØªØ±Ù†Ùˆ Ø¥Ù„Ù‰ Ù…Ø³ØªÙ‚Ø¨ÙÙ„ Ù…ØªÙ†ÙˆÙÙ‘Ø± ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠÙ‹Ø§ ÙˆÙ‚Ø§Ø¦Ù… Ø¹Ù„ÙÙ‰ Ø£Ø³Ø³ Ø£Ø®Ù„Ø§Ù‚ÙŠÙÙ‘Ø© ØµÙ„Ø¨Ø©.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ø§Ù‚ØªØ±Ø§Ø­ Ù…Ù…ØªØ§Ø²! Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø°ÙƒØ± Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ Ù„Ø¥Ø¯Ø§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ø§Ù†ØµÙ‡Ø§Ø± Ø§Ù„Ù†Ø§Ø¬Ø­ Ù†Ø­Ùˆ Ø¢ÙØ§Ù‚ ÙˆØ§Ø¹Ø¯Ø© Ù„Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙÙŠØŸ\\n\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** Ù†Ø¹Ù…ØŒ Ø¹Ù„ÙŠÙ†Ø§ ØªØ´ÙƒÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¯Ø§Ø®ÙŠÙ„Ø© Ø§Ù„Ù…Ø­ØªØ±Ù…Ø© Ù„Ø±ÙˆØ§ÙØ¯Ù†Ø§ Ø§Ù„ÙÙƒØ±ÙŠÙÙ‘Ù€Ù€Ø© ÙˆØªÙˆØ¸ÙŠÙ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø±ÙÙ‘Ø§Ù‚Ù…ÙŠØ© ÙƒÙ…Ù†Ø¨Ø± Ù„Ù„ØªÙˆØ§ØµÙ„ Ø¨Ù‚Ø¶Ø§ÙŠØ§ Ù…Ø¬ØªÙ…Ø¹ÙŠÙÙ‘Ø© Ø°Ø§Øª Ù…ØºØ²Ù‰ ÙˆÙØ§Ø¦Ø¯Ø© Ù…Ø´ØªØ±ÙƒØ©.', 'main_discussion': '**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ø¨Ø¯Ø§ÙŠØ©Ù‹, Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ Ø¯ÙˆØ± Ø±Ø¦ÙŠØ³ÙŠ Ù„ÙƒÙ† Ù‡Ù„ ØªØ¹Ù„Ù… Ø£Ù†Ù‡ Ù‚Ø¯ ÙŠÙÙ‡Ù…Ù„ Ø§Ù„Ø¨Ø¹Ø¶ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø£ÙŠØ¶Ø§Ù‹ØŸ\\n\\n**Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯:** ØµØ­ÙŠØ­, Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¨Ø¯Ù†ÙŠ Ù…Ù‡Ù… Ù„ÙƒÙ† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„ØµØ­ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø£ÙŠ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØµØ­ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ† Ù„Ù„Ø±ÙŠØ§Ø¶Ø© Ø£Ù† ØªØ¹ÙˆØ¶ Ø¨Ø¹Ø¶ Ù‚ØµÙˆØ± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ Ø¨Ù…Ø­ØªÙˆØ§Ù‡Ø§ Ù…Ù† Ø§Ù„Ø³Ø¹Ø±Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ø­Ø±ÙˆÙ‚Ø©.\\n\\n**Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯:** Ù‡Ø°Ø§ ØµØ­ÙŠØ­ ÙˆÙ„ÙƒÙ† Ø§Ù„Ø£ÙØ¶Ù„ Ù‡Ùˆ Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ù…Ø¹Ù‹Ø§Ø› Ù„ÙŠØ³ ÙÙ‚Ø· Ø§Ù„ØªØ¹ÙˆÙŠØ¶ Ø¹Ù† Ù†Ù‚Øµ ÙˆØ§Ø­Ø¯ Ø¨Ø§Ù„ØªÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¢Ø®Ø±.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ø±Ø¨Ù…Ø§ØŒ ÙˆÙ„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù†Ø§Ø³ Ù‡Ø¯Ù Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹ØŒ ÙˆÙ…Ù…ÙƒÙ† Ø§Ù„Ø¬Ù…Ø¨Ø§Ø² Ø£Ùˆ Ø§Ù„ÙŠÙˆØºØ§ ÙŠÙÙŠØ¯ Ù„Ø°Ù„Ùƒ Ø£ÙƒØ«Ø±!\\n\\n**Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯:** ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø°Ù„Ùƒ ÙØ¥Ù† Ù…Ù…Ø§Ø±Ø³Ø© Ù†ÙˆØ¹ Ø±ÙŠØ§Ø¶ÙŠ ØªØ­Ø¨ÙŠÙ†Ù‡ ÙˆØ§Ø¬Ù‡Ø²Ø© Ø¬Ø³Ù…Ùƒ Ø£ÙØ¶Ù„ Ø¨ÙƒØ«ÙŠØ± Ù„Ø¯Ø¹Ù… Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆØ§Ù„Ø¹Ù‚Ù„Ø§Ù†ÙŠØ© Ø£ÙŠØ¶Ù‹Ø§.\\nØ§Ù„Ø«Ù‚Ø©: 95%</strong>\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ÙÙŠ Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŒ ÙƒÙŠÙ ØªØ±Ù‰ Ø¯ÙˆØ± Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙÙŠÙ‡Ø§ØŸ\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: ØªÙ‚Ù Ø­Ø¬Ø± Ø¹Ø«Ø±Ø© ÙƒØ«ÙŠØ±Ø© Ø­Ø³Ø¨ ØªØ¬Ø§Ø±Ø¨ÙŠ.\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ù„ÙƒÙ† Ø§Ù„Ø¨Ø¹Ø¶ ÙŠÙ‚ÙˆÙ„ Ø¥Ù†Ù‡Ø§ ÙØ±ØµØ© Ù„Ù„Ù†Ù…Ùˆ.\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ù†Ø¹Ù… Ø±Ø¨Ù…Ø§ØŒ Ù„ÙƒÙ†Ù‡Ø§ Ù‚Ø¯ ØªØ®Ù„Ù‚ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©.\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ØµØ­ÙŠØ­ Ø¨Ø´Ø£Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ù„ ØªØ¹Ù„Ù… Ø£Ù†Ù‡ Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙŠØ³ØªÙÙŠØ¯ÙˆÙ† Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù†Ù‡Ø§ Ø±ØºÙ… Ø°Ù„Ùƒ?\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ù‡Ø°Ø§ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¸Ø±ÙˆÙ ÙƒÙ„ Ø´Ø®Øµ Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯, ÙˆÙ„ÙƒÙ† Ø§Ù„Ø¬Ù‡Ø¯ ÙŠØ¨Ù‚Ù‰ Ø¶Ø±ÙˆØ±ÙŠÙ‹Ø§ Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø±.\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø¨Ù„Ø§ Ø´Ùƒ, Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ù‡Ùˆ Ø§Ù„Ù…ÙØªØ§Ø­ thenÙ‡ÙŠÙ…Ù‹Ø§.\\n\\n**Ø£Ø­Ù…Ø¯:** Ø­ÙˆÙ„ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨, ÙƒÙŠÙ ØªØ±Ø§Ù‡Ù \"ÙØ§ØªÙ†\" ØŸ\\n\\n**ÙØ§ØªÙ†:** ØªØ­Ø¯ÙŠØ§Øª ÙƒØ«ÙŠØ±Ø© ØªØ­ØªØ§Ø¬ Ø¯Ø±Ø§Ø³Ø© Ù…ØªØ£Ù†ÙŠØ© Ù‚Ø¨Ù„ ØªÙ‚Ø¯ÙŠÙ… Ø­Ù„ÙˆÙ„.\\n\\n**Ø£Ø­Ù…Ø¯:** Ù„ÙƒÙ† Ø§Ù„Ø¨Ø¹Ø¶ ÙŠØ¯Ø¹Ùˆ Ù„Ù„ØªØºÙŠÙŠØ± Ø§Ù„ÙÙˆØ±ÙŠ Ø£Ù„ÙŠØ³ ÙƒØ°Ù„Ùƒ?\\n\\n**ÙØ§ØªÙ†:** Ø§Ù„Ø¥ØµÙ„Ø§Ø­ ÙŠØ­ØªØ§Ø¬ ÙˆÙ‚Øª ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„ ÙÙŠÙ‡.\\n\\n**Ø£Ø­Ù…Ø¯:** Ø±Ø¨Ù…Ø§ Ù†Ù‚ØªØ±Ø­ Ø®Ø·ÙˆØ§Øª Ø³Ø±ÙŠØ¹Ø© ÙØ¹Ø§Ù„Ø© Ø¹ÙˆØ¶Ø§Ù‹ Ø¹Ù† Ø§Ù„ØªØ£Ø¬ÙŠÙ„ .\\n\\n**ÙØ§ØªÙ†:** Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ù…Ø¤Ù‚ØªØ© ÙˆÙ„ÙƒÙ†Ù‡Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø¯Ø±Ø§Ø³Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø£ÙˆÙ„Ù‹Ø§ .\\n\\n**Ø£Ø­Ù…Ø¯:** Ø¥Ø°Ù† ØªÙˆØ§ÙÙ‚ÙŠÙ† Ø¨Ø£Ù† Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø¶Ø±ÙˆØ±ÙŠØ© Ù‚Ø¨Ù„Ù‡Ù… ÙƒÙ„Ø§Ù‡Ù…Ø§!\\n\\n**ÙØ§ØªÙ†:** ØªÙ…Ø§Ù…Ù‹Ø§ ØŒÙ„ÙƒÙ† Ù„ÙŠØ³ Ø¹Ø°Ø± Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø±ÙƒØ© Ù†Ø­Ùˆ Ø§Ù„Ø­Ù„.', 'closing': 'Ø£Ø­Ù…Ø¯: Ø´ÙƒØ±Ù‹Ø§ Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ†ØŒ ÙƒØ§Ù†Øª Ù…Ø­Ø§Ø¯Ø«Ø© ØºÙ†ÙŠØ© Ø¨Ø§Ù„ØªØ¬Ø§Ø±Ø¨.\\nÙØ§ØªÙ†: Ù…Ù† Ø§Ù„Ø±Ø§Ø¦Ø¹ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ø£ÙÙƒØ§Ø± Ù…Ø¹ÙƒÙ…ØŒ Ø£Ø´ÙƒØ±ÙƒÙ….\\nØ£Ø­Ù…Ø¯: ÙˆØ´ÙƒØ±Ø§Ù‹ Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„Ø¹Ø²ÙŠØ²ÙŠÙ†ï¼Œ Ø³Ù†Ù„ØªÙ‚ÙŠ Ù…Ø¬Ø¯Ø¯Ù‹Ø§.\\nÙØ§ØªÙ†: Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡ ÙˆÙ†Ø±Ø§ÙƒÙ… Ù…Ø±Ø© Ø£Ø®Ø±Ù‰!', 'complete_script': '=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø¬Ù…ÙŠØ¹Ø§Ù‹ØŒ ÙˆÙ†Ø±Ø­Ø¨ Ø¨ÙƒÙ… ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„ØªÙŠ Ù†ØºÙˆØµ ÙÙŠÙ‡Ø§ Ø¨Ø¹Ù…Ù‚ Ù„ØªÙÙ‡Ù… Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ø­Ø³Ø§Ø³ Ø¨ÙŠÙ† Ø«ÙˆØ±ØªÙ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ ÙˆØ«Ø±ÙˆØªÙ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙˆØ§Ø¹Ø¯Ø©Ø› Ø¯Ø¹ÙˆÙ†Ø§ Ù†ØªØ´Ø§Ø±Ùƒ Ø¨Ø§Ù‡ØªÙ…Ø§Ù… Ù†Ø³Ø¨Ø± ÙÙŠÙ‡ Ø¢ÙØ§Ù‚ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø²Ø¬ Ø§Ù„ÙØ±ÙŠØ¯ Ù„Ù…Ø³ØªÙ‚Ø¨Ù„Ù†Ø§ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ØªØ±Ø­ÙŠØ¨ Ù„Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ğ Ğ°ÑˆĞ¸Ğ´ØŒ Ù…Ù† Ø§Ù„Ø±Ø§Ø¦Ø¹ Ø£Ù†Ù‘Ù‡Ø§ Ù…Ø¹Ù†Ø§ Ø§Ù„ÙŠÙˆÙ… Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ù…Ø³Ø£Ù„Ø© Ø­ÙŠÙˆÙŠØ© Ø­ÙˆÙ„ Ø§Ù„ØªØ±Ø§Ø¨Ø· Ø¨ÙŠÙ† ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆÙ‡ÙˆÛŒØªÙÙ†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©.\\n\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** Ø´ÙƒØ± Ù„ÙƒØŒ Ù†Ø¨Ø­Ø« Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ­Ø¯ÙŠØ§Øª Ù…Ø«ÙŠØ±Ø© ØªØªØ¹Ù„Ù‚ Ø¨Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© ÙÙŠ Ø¹Ø§Ù„Ù… Ù…ØªØ±Ø§Ø¨Ø· Ø¨Ø´ÙƒÙ„ Ø±Ù‚Ù…ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ÙƒÙŠÙ Ø¨Ø¥Ù…ÙƒØ§Ù†Ù†Ø§ Ø¶Ù…Ù‘ ØªÙ‚Ù†ÙŠÙ‘Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ù„Ø¯Ø¹Ù… Ù‚ÙŠÙ…Ù†Ø§ ÙˆÙ…Ø¹ØªÙ‚Ø¯Ø§ØªÙ†Ø§ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªÙ‡Ø¯ÙŠØ¯ ÙˆØ¬ÙˆØ¯Ù‡Ø§?\\n\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** ÙŠØ¬Ø¨ Ø£Ù† Ù†Ø±ÙƒØ² Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø­Ù„ÙˆÙ„ Ø°ÙƒÙŠØ© ØªØ³ØªÙ„Ù‡Ù… Ø¬Ø°ÙˆØ±Ù‡Ø§ Ø§Ù„Ù‚ÙŠÙ…ÙŠÙ‘Ø© Ø§Ù„Ø«Ù‚Ø§ÙÙŠÙ‘Ø© Ø¨ÙŠÙ†Ù…Ø§ ØªØ±Ù†Ùˆ Ø¥Ù„Ù‰ Ù…Ø³ØªÙ‚Ø¨ÙÙ„ Ù…ØªÙ†ÙˆÙÙ‘Ø± ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠÙ‹Ø§ ÙˆÙ‚Ø§Ø¦Ù… Ø¹Ù„ÙÙ‰ Ø£Ø³Ø³ Ø£Ø®Ù„Ø§Ù‚ÙŠÙÙ‘Ø© ØµÙ„Ø¨Ø©.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ø§Ù‚ØªØ±Ø§Ø­ Ù…Ù…ØªØ§Ø²! Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø°ÙƒØ± Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ Ù„Ø¥Ø¯Ø§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ø§Ù†ØµÙ‡Ø§Ø± Ø§Ù„Ù†Ø§Ø¬Ø­ Ù†Ø­Ùˆ Ø¢ÙØ§Ù‚ ÙˆØ§Ø¹Ø¯Ø© Ù„Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙÙŠØŸ\\n\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** Ù†Ø¹Ù…ØŒ Ø¹Ù„ÙŠÙ†Ø§ ØªØ´ÙƒÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¯Ø§Ø®ÙŠÙ„Ø© Ø§Ù„Ù…Ø­ØªØ±Ù…Ø© Ù„Ø±ÙˆØ§ÙØ¯Ù†Ø§ Ø§Ù„ÙÙƒØ±ÙŠÙÙ‘Ù€Ù€Ø© ÙˆØªÙˆØ¸ÙŠÙ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø±ÙÙ‘Ø§Ù‚Ù…ÙŠØ© ÙƒÙ…Ù†Ø¨Ø± Ù„Ù„ØªÙˆØ§ØµÙ„ Ø¨Ù‚Ø¶Ø§ÙŠØ§ Ù…Ø¬ØªÙ…Ø¹ÙŠÙÙ‘Ø© Ø°Ø§Øª Ù…ØºØ²Ù‰ ÙˆÙØ§Ø¦Ø¯Ø© Ù…Ø´ØªØ±ÙƒØ©.\\n\\n=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ø¨Ø¯Ø§ÙŠØ©Ù‹, Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ Ø¯ÙˆØ± Ø±Ø¦ÙŠØ³ÙŠ Ù„ÙƒÙ† Ù‡Ù„ ØªØ¹Ù„Ù… Ø£Ù†Ù‡ Ù‚Ø¯ ÙŠÙÙ‡Ù…Ù„ Ø§Ù„Ø¨Ø¹Ø¶ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø£ÙŠØ¶Ø§Ù‹ØŸ\\n\\n**Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯:** ØµØ­ÙŠØ­, Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¨Ø¯Ù†ÙŠ Ù…Ù‡Ù… Ù„ÙƒÙ† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„ØµØ­ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø£ÙŠ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØµØ­ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ† Ù„Ù„Ø±ÙŠØ§Ø¶Ø© Ø£Ù† ØªØ¹ÙˆØ¶ Ø¨Ø¹Ø¶ Ù‚ØµÙˆØ± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ Ø¨Ù…Ø­ØªÙˆØ§Ù‡Ø§ Ù…Ù† Ø§Ù„Ø³Ø¹Ø±Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ø­Ø±ÙˆÙ‚Ø©.\\n\\n**Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯:** Ù‡Ø°Ø§ ØµØ­ÙŠØ­ ÙˆÙ„ÙƒÙ† Ø§Ù„Ø£ÙØ¶Ù„ Ù‡Ùˆ Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ù…Ø¹Ù‹Ø§Ø› Ù„ÙŠØ³ ÙÙ‚Ø· Ø§Ù„ØªØ¹ÙˆÙŠØ¶ Ø¹Ù† Ù†Ù‚Øµ ÙˆØ§Ø­Ø¯ Ø¨Ø§Ù„ØªÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¢Ø®Ø±.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ø±Ø¨Ù…Ø§ØŒ ÙˆÙ„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù†Ø§Ø³ Ù‡Ø¯Ù Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹ØŒ ÙˆÙ…Ù…ÙƒÙ† Ø§Ù„Ø¬Ù…Ø¨Ø§Ø² Ø£Ùˆ Ø§Ù„ÙŠÙˆØºØ§ ÙŠÙÙŠØ¯ Ù„Ø°Ù„Ùƒ Ø£ÙƒØ«Ø±!\\n\\n**Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯:** ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø°Ù„Ùƒ ÙØ¥Ù† Ù…Ù…Ø§Ø±Ø³Ø© Ù†ÙˆØ¹ Ø±ÙŠØ§Ø¶ÙŠ ØªØ­Ø¨ÙŠÙ†Ù‡ ÙˆØ§Ø¬Ù‡Ø²Ø© Ø¬Ø³Ù…Ùƒ Ø£ÙØ¶Ù„ Ø¨ÙƒØ«ÙŠØ± Ù„Ø¯Ø¹Ù… Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆØ§Ù„Ø¹Ù‚Ù„Ø§Ù†ÙŠØ© Ø£ÙŠØ¶Ù‹Ø§.\\nØ§Ù„Ø«Ù‚Ø©: 95%</strong>\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ÙÙŠ Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŒ ÙƒÙŠÙ ØªØ±Ù‰ Ø¯ÙˆØ± Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙÙŠÙ‡Ø§ØŸ\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: ØªÙ‚Ù Ø­Ø¬Ø± Ø¹Ø«Ø±Ø© ÙƒØ«ÙŠØ±Ø© Ø­Ø³Ø¨ ØªØ¬Ø§Ø±Ø¨ÙŠ.\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ù„ÙƒÙ† Ø§Ù„Ø¨Ø¹Ø¶ ÙŠÙ‚ÙˆÙ„ Ø¥Ù†Ù‡Ø§ ÙØ±ØµØ© Ù„Ù„Ù†Ù…Ùˆ.\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ù†Ø¹Ù… Ø±Ø¨Ù…Ø§ØŒ Ù„ÙƒÙ†Ù‡Ø§ Ù‚Ø¯ ØªØ®Ù„Ù‚ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©.\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ØµØ­ÙŠØ­ Ø¨Ø´Ø£Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ù„ ØªØ¹Ù„Ù… Ø£Ù†Ù‡ Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙŠØ³ØªÙÙŠØ¯ÙˆÙ† Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù†Ù‡Ø§ Ø±ØºÙ… Ø°Ù„Ùƒ?\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ù‡Ø°Ø§ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¸Ø±ÙˆÙ ÙƒÙ„ Ø´Ø®Øµ Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯, ÙˆÙ„ÙƒÙ† Ø§Ù„Ø¬Ù‡Ø¯ ÙŠØ¨Ù‚Ù‰ Ø¶Ø±ÙˆØ±ÙŠÙ‹Ø§ Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø±.\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø¨Ù„Ø§ Ø´Ùƒ, Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ù‡Ùˆ Ø§Ù„Ù…ÙØªØ§Ø­ thenÙ‡ÙŠÙ…Ù‹Ø§.\\n\\n**Ø£Ø­Ù…Ø¯:** Ø­ÙˆÙ„ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨, ÙƒÙŠÙ ØªØ±Ø§Ù‡Ù \"ÙØ§ØªÙ†\" ØŸ\\n\\n**ÙØ§ØªÙ†:** ØªØ­Ø¯ÙŠØ§Øª ÙƒØ«ÙŠØ±Ø© ØªØ­ØªØ§Ø¬ Ø¯Ø±Ø§Ø³Ø© Ù…ØªØ£Ù†ÙŠØ© Ù‚Ø¨Ù„ ØªÙ‚Ø¯ÙŠÙ… Ø­Ù„ÙˆÙ„.\\n\\n**Ø£Ø­Ù…Ø¯:** Ù„ÙƒÙ† Ø§Ù„Ø¨Ø¹Ø¶ ÙŠØ¯Ø¹Ùˆ Ù„Ù„ØªØºÙŠÙŠØ± Ø§Ù„ÙÙˆØ±ÙŠ Ø£Ù„ÙŠØ³ ÙƒØ°Ù„Ùƒ?\\n\\n**ÙØ§ØªÙ†:** Ø§Ù„Ø¥ØµÙ„Ø§Ø­ ÙŠØ­ØªØ§Ø¬ ÙˆÙ‚Øª ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„ ÙÙŠÙ‡.\\n\\n**Ø£Ø­Ù…Ø¯:** Ø±Ø¨Ù…Ø§ Ù†Ù‚ØªØ±Ø­ Ø®Ø·ÙˆØ§Øª Ø³Ø±ÙŠØ¹Ø© ÙØ¹Ø§Ù„Ø© Ø¹ÙˆØ¶Ø§Ù‹ Ø¹Ù† Ø§Ù„ØªØ£Ø¬ÙŠÙ„ .\\n\\n**ÙØ§ØªÙ†:** Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ù…Ø¤Ù‚ØªØ© ÙˆÙ„ÙƒÙ†Ù‡Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø¯Ø±Ø§Ø³Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø£ÙˆÙ„Ù‹Ø§ .\\n\\n**Ø£Ø­Ù…Ø¯:** Ø¥Ø°Ù† ØªÙˆØ§ÙÙ‚ÙŠÙ† Ø¨Ø£Ù† Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø¶Ø±ÙˆØ±ÙŠØ© Ù‚Ø¨Ù„Ù‡Ù… ÙƒÙ„Ø§Ù‡Ù…Ø§!\\n\\n**ÙØ§ØªÙ†:** ØªÙ…Ø§Ù…Ù‹Ø§ ØŒÙ„ÙƒÙ† Ù„ÙŠØ³ Ø¹Ø°Ø± Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø±ÙƒØ© Ù†Ø­Ùˆ Ø§Ù„Ø­Ù„.\\n\\n=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\\nØ£Ø­Ù…Ø¯: Ø´ÙƒØ±Ù‹Ø§ Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ†ØŒ ÙƒØ§Ù†Øª Ù…Ø­Ø§Ø¯Ø«Ø© ØºÙ†ÙŠØ© Ø¨Ø§Ù„ØªØ¬Ø§Ø±Ø¨.\\nÙØ§ØªÙ†: Ù…Ù† Ø§Ù„Ø±Ø§Ø¦Ø¹ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ø£ÙÙƒØ§Ø± Ù…Ø¹ÙƒÙ…ØŒ Ø£Ø´ÙƒØ±ÙƒÙ….\\nØ£Ø­Ù…Ø¯: ÙˆØ´ÙƒØ±Ø§Ù‹ Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„Ø¹Ø²ÙŠØ²ÙŠÙ†ï¼Œ Ø³Ù†Ù„ØªÙ‚ÙŠ Ù…Ø¬Ø¯Ø¯Ù‹Ø§.\\nÙØ§ØªÙ†: Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡ ÙˆÙ†Ø±Ø§ÙƒÙ… Ù…Ø±Ø© Ø£Ø®Ø±Ù‰!', 'script_length': 2913, 'estimated_duration': '10-15 minutes', 'quality_score': 100, 'generation_method': 'enhanced-spontaneous-micro-chunks', 'chunks_generated': 6, 'personas_used': {'host': 'Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ', 'guest': 'Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯'}, 'cultural_elements_integrated': 1, 'spontaneity_level': 'high', 'natural_interruptions': 0, 'disagreement_instances': 5, 'enhancement_level': 'spontaneous'}\n"
     ]
    }
   ],
   "source": [
    "print(script_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47ec2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Basic cleaning\\ncleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\\ncleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\\n\\n# Testing with detailed output\\ntest_result = test_micro_chunking_cleaner(deployment, corrupted_script_result)\\n\\n# Detailed pre-processing analysis\\nanalysis = detailed_micro_chunk_analysis(deployment, corrupted_script_result)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class MicroChunkingAIScriptCleaner:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def clean_script_with_ai(self, script_result):\n",
    "        \"\"\"\n",
    "        Micro-chunking approach: Clean script using surgical AI corrections\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"ğŸ”¬ MICRO-CHUNKING AI SCRIPT CLEANER\".center(80, \"=\"))\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # Extract complete script text\n",
    "            if isinstance(script_result, dict):\n",
    "                complete_script = script_result.get('complete_script', '')\n",
    "            else:\n",
    "                complete_script = str(script_result)\n",
    "            \n",
    "            if not complete_script or len(complete_script.strip()) < 50:\n",
    "                print(\"âŒ Script too short or empty, using fallback\")\n",
    "                return self._generate_complete_fallback()\n",
    "            \n",
    "            original_length = len(complete_script)\n",
    "            print(f\"ğŸ“ Original script length: {original_length:,} characters\")\n",
    "            \n",
    "            # Step 1: Analyze script corruption\n",
    "            print(\"ğŸ” CORRUPTION ANALYSIS\".center(60, \"-\"))\n",
    "            corruption_analysis = self.analyze_corruption_patterns(complete_script)\n",
    "            self._print_corruption_summary(corruption_analysis)\n",
    "            \n",
    "            # Step 2: Create micro-chunks (small, focused chunks)\n",
    "            print(\"\\nğŸ“ MICRO-CHUNKING STRATEGY\".center(60, \"-\"))\n",
    "            micro_chunks = self.create_micro_chunks(complete_script)\n",
    "            print(f\"    Created {len(micro_chunks)} micro-chunks (avg: {original_length//len(micro_chunks)} chars each)\")\n",
    "            \n",
    "            # Step 3: Process each micro-chunk with surgical precision\n",
    "            print(\"\\nğŸ”¬ SURGICAL CLEANING PROCESS\".center(60, \"-\"))\n",
    "            cleaned_chunks = []\n",
    "            \n",
    "            for i, chunk_data in enumerate(micro_chunks):\n",
    "                print(f\"    Processing micro-chunk {i+1}/{len(micro_chunks)}... \", end=\"\")\n",
    "                \n",
    "                # Quick corruption assessment\n",
    "                corruption_level = self.assess_chunk_corruption(chunk_data['content'])\n",
    "                \n",
    "                if corruption_level == 'clean':\n",
    "                    # Keep as-is\n",
    "                    cleaned_chunks.append(chunk_data['content'])\n",
    "                    print(\"âœ… CLEAN (kept as-is)\")\n",
    "                elif corruption_level == 'minor':\n",
    "                    # Light cleaning with regex\n",
    "                    cleaned_chunk = self.light_clean_chunk(chunk_data['content'])\n",
    "                    cleaned_chunks.append(cleaned_chunk)\n",
    "                    print(\"ğŸŸ¡ LIGHT CLEAN\")\n",
    "                else:\n",
    "                    # AI-powered surgical correction\n",
    "                    cleaned_chunk = self.surgical_ai_correction(\n",
    "                        chunk_data['content'], \n",
    "                        chunk_data['context'],\n",
    "                        corruption_analysis\n",
    "                    )\n",
    "                    \n",
    "                    # Validate result\n",
    "                    if self.validate_micro_chunk(cleaned_chunk, chunk_data['content']):\n",
    "                        cleaned_chunks.append(cleaned_chunk)\n",
    "                        print(\"ğŸ”§ AI CORRECTED\")\n",
    "                    else:\n",
    "                        # Fallback to light cleaning\n",
    "                        fallback_chunk = self.light_clean_chunk(chunk_data['content'])\n",
    "                        cleaned_chunks.append(fallback_chunk)\n",
    "                        print(\"ğŸ”„ FALLBACK CLEAN\")\n",
    "            \n",
    "            # Step 4: Reassemble with structure preservation\n",
    "            print(\"\\nğŸ”§ REASSEMBLY WITH STRUCTURE CHECK\".center(60, \"-\"))\n",
    "            final_script = self.intelligent_reassembly(cleaned_chunks, micro_chunks, complete_script)\n",
    "            \n",
    "            # Step 5: Final quality and length check\n",
    "            final_length = len(final_script)\n",
    "            length_ratio = final_length / original_length if original_length > 0 else 0\n",
    "            \n",
    "            print(f\"    Original structure preserved: {'âœ…' if self.verify_structure_preservation(complete_script, final_script) else 'âš ï¸'}\")\n",
    "            print(f\"    Length preservation: {length_ratio:.1%}\")\n",
    "            \n",
    "            # Step 6: Light expansion if needed (without AI)\n",
    "            if length_ratio < 0.90:  # Less than 90% retained\n",
    "                print(\"ğŸ“ˆ APPLYING LENGTH RECOVERY (NON-AI)\".center(60, \"-\"))\n",
    "                final_script = self.non_ai_length_recovery(final_script, original_length)\n",
    "                final_length = len(final_script)\n",
    "                length_ratio = final_length / original_length\n",
    "            \n",
    "            # Final validation\n",
    "            final_valid = self.validate_final_script(final_script)\n",
    "            \n",
    "            print(\"\\n\" + \"ğŸ‰ MICRO-CHUNKING SUMMARY\".center(60, \"-\"))\n",
    "            print(f\"    Status: {'SUCCESS' if final_valid else 'PARTIAL SUCCESS'}\")\n",
    "            print(f\"    Original Length: {original_length:,} characters\")\n",
    "            print(f\"    Final Length: {final_length:,} characters\")\n",
    "            print(f\"    Length Preserved: {length_ratio:.1%}\")\n",
    "            print(f\"    Micro-chunks Processed: {len(micro_chunks)}\")\n",
    "            print(f\"    Arabic Quality: {'âœ… VALID' if final_valid else 'âš ï¸ NEEDS REVIEW'}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            return {\n",
    "                'complete_script': final_script,\n",
    "                'cleaning_method': 'micro_chunking_surgical',\n",
    "                'micro_chunks_processed': len(micro_chunks),\n",
    "                'cleaning_status': 'success' if final_valid else 'partial',\n",
    "                'script_length': final_length,\n",
    "                'original_length': original_length,\n",
    "                'length_ratio': length_ratio,\n",
    "                'corruption_analysis': corruption_analysis,\n",
    "                'estimated_duration': self._estimate_duration(final_script)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ CRITICAL ERROR: {e}\")\n",
    "            print(\"ğŸ”„ Using complete fallback script...\")\n",
    "            return self._generate_complete_fallback()\n",
    "\n",
    "    def analyze_corruption_patterns(self, script_text):\n",
    "        \"\"\"\n",
    "        Analyze what specific types of corruption exist in the script\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'total_chars': len(script_text),\n",
    "            'foreign_patterns': {},\n",
    "            'encoding_issues': 0,\n",
    "            'concatenated_words': 0,\n",
    "            'structural_issues': 0,\n",
    "            'overall_corruption_level': 'clean'\n",
    "        }\n",
    "        \n",
    "        # Foreign language patterns\n",
    "        foreign_patterns = {\n",
    "            'english_words': (r'\\b[A-Za-z]{3,}\\b', 'English words (3+ letters)'),\n",
    "            'chinese_chars': (r'[\\u4e00-\\u9fff]', 'Chinese characters'),\n",
    "            'hebrew_chars': (r'[\\u0590-\\u05ff]', 'Hebrew characters'),\n",
    "            'japanese_chars': (r'[\\u3040-\\u309f\\u30a0-\\u30ff]', 'Japanese characters'),\n",
    "            'problematic_punct': (r'[ã€ï¼]', 'Problematic punctuation')\n",
    "        }\n",
    "        \n",
    "        total_foreign_chars = 0\n",
    "        for pattern_name, (pattern, description) in foreign_patterns.items():\n",
    "            matches = re.findall(pattern, script_text)\n",
    "            if matches:\n",
    "                char_count = sum(len(match) for match in matches)\n",
    "                analysis['foreign_patterns'][pattern_name] = {\n",
    "                    'count': len(matches),\n",
    "                    'chars': char_count,\n",
    "                    'description': description,\n",
    "                    'examples': matches[:3]  # First 3 examples\n",
    "                }\n",
    "                total_foreign_chars += char_count\n",
    "        \n",
    "        # Concatenated words (Arabic words without spaces)\n",
    "        concatenated_matches = re.findall(r'[\\u0600-\\u06FF]{40,}', script_text)  # 40+ Arabic chars without spaces\n",
    "        analysis['concatenated_words'] = len(concatenated_matches)\n",
    "        \n",
    "        # Encoding issues (mixed scripts in single words)\n",
    "        encoding_issues = re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]+[\\u0600-\\u06FF]+|[\\u0600-\\u06FF]+[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]+', script_text)\n",
    "        analysis['encoding_issues'] = len(encoding_issues)\n",
    "        \n",
    "        # Structural issues\n",
    "        if '***' in script_text or '**' in script_text:\n",
    "            analysis['structural_issues'] += script_text.count('***') + script_text.count('**')\n",
    "        \n",
    "        # Overall corruption level\n",
    "        foreign_ratio = total_foreign_chars / len(script_text) if script_text else 0\n",
    "        if foreign_ratio > 0.15 or analysis['concatenated_words'] > 5:\n",
    "            analysis['overall_corruption_level'] = 'heavy'\n",
    "        elif foreign_ratio > 0.05 or analysis['concatenated_words'] > 2:\n",
    "            analysis['overall_corruption_level'] = 'moderate'\n",
    "        elif foreign_ratio > 0.01 or analysis['encoding_issues'] > 0:\n",
    "            analysis['overall_corruption_level'] = 'light'\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _print_corruption_summary(self, analysis):\n",
    "        \"\"\"Print corruption analysis summary\"\"\"\n",
    "        print(f\"    Overall corruption level: {analysis['overall_corruption_level'].upper()}\")\n",
    "        print(f\"    Foreign patterns found: {len(analysis['foreign_patterns'])}\")\n",
    "        \n",
    "        for pattern_name, details in analysis['foreign_patterns'].items():\n",
    "            print(f\"      - {details['description']}: {details['count']} instances\")\n",
    "            if details['examples']:\n",
    "                examples_str = ', '.join(str(ex) for ex in details['examples'][:2])\n",
    "                print(f\"        Examples: {examples_str}\")\n",
    "        \n",
    "        if analysis['concatenated_words'] > 0:\n",
    "            print(f\"    Concatenated word sequences: {analysis['concatenated_words']}\")\n",
    "        if analysis['encoding_issues'] > 0:\n",
    "            print(f\"    Mixed encoding issues: {analysis['encoding_issues']}\")\n",
    "\n",
    "    def create_micro_chunks(self, script_text):\n",
    "        \"\"\"\n",
    "        Create small, focused micro-chunks (200-400 characters each)\n",
    "        \"\"\"\n",
    "        micro_chunks = []\n",
    "        \n",
    "        # Split by sections first\n",
    "        sections = self._identify_script_sections(script_text)\n",
    "        \n",
    "        for section_name, section_content in sections.items():\n",
    "            if not section_content.strip():\n",
    "                continue\n",
    "            \n",
    "            # Split section into dialogue exchanges\n",
    "            dialogue_chunks = self._split_into_dialogue_exchanges(section_content, section_name)\n",
    "            micro_chunks.extend(dialogue_chunks)\n",
    "        \n",
    "        return micro_chunks\n",
    "\n",
    "    def _identify_script_sections(self, script_text):\n",
    "        \"\"\"Identify main sections of the script\"\"\"\n",
    "        sections = {}\n",
    "        \n",
    "        if \"=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\" in script_text:\n",
    "            # Structured script\n",
    "            parts = script_text.split(\"=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\")\n",
    "            if len(parts) > 1:\n",
    "                after_intro = parts[1]\n",
    "                if \"=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\" in after_intro:\n",
    "                    intro_parts = after_intro.split(\"=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\")\n",
    "                    sections['intro'] = intro_parts[0].strip()\n",
    "                    if len(intro_parts) > 1:\n",
    "                        after_main = intro_parts[1]\n",
    "                        if \"=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\" in after_main:\n",
    "                            main_parts = after_main.split(\"=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\")\n",
    "                            sections['main_discussion'] = main_parts[0].strip()\n",
    "                            if len(main_parts) > 1:\n",
    "                                sections['closing'] = main_parts[1].strip()\n",
    "                        else:\n",
    "                            sections['main_discussion'] = after_main.strip()\n",
    "                else:\n",
    "                    sections['intro'] = after_intro.strip()\n",
    "        else:\n",
    "            # Unstructured script\n",
    "            sections['full_script'] = script_text\n",
    "        \n",
    "        return sections\n",
    "\n",
    "    def _split_into_dialogue_exchanges(self, section_content, section_name):\n",
    "        \"\"\"Split section into small dialogue exchanges\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split by speaker turns\n",
    "        lines = section_content.split('\\n')\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        target_chunk_size = 300  # Target 300 characters per micro-chunk\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a speaker line\n",
    "            is_speaker_line = ':' in line and not line.startswith('===')\n",
    "            \n",
    "            # If adding this line would exceed target size, finalize current chunk\n",
    "            if current_length + len(line) > target_chunk_size and current_chunk and is_speaker_line:\n",
    "                chunk_content = '\\n'.join(current_chunk).strip()\n",
    "                if chunk_content:\n",
    "                    chunks.append({\n",
    "                        'content': chunk_content,\n",
    "                        'context': f'{section_name}_dialogue_exchange',\n",
    "                        'type': 'dialogue_exchange',\n",
    "                        'length': len(chunk_content)\n",
    "                    })\n",
    "                current_chunk = [line]\n",
    "                current_length = len(line)\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "                current_length += len(line) + 1  # +1 for newline\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_content = '\\n'.join(current_chunk).strip()\n",
    "            if chunk_content:\n",
    "                chunks.append({\n",
    "                    'content': chunk_content,\n",
    "                    'context': f'{section_name}_dialogue_exchange',\n",
    "                    'type': 'dialogue_exchange',\n",
    "                    'length': len(chunk_content)\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def assess_chunk_corruption(self, chunk_content):\n",
    "        \"\"\"Quick assessment of chunk corruption level\"\"\"\n",
    "        if not chunk_content or len(chunk_content.strip()) < 10:\n",
    "            return 'heavy'\n",
    "        \n",
    "        # Check for foreign characters\n",
    "        foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff\\u3040-\\u309f\\u30a0-\\u30ffã€ï¼]', chunk_content))\n",
    "        total_chars = len(chunk_content)\n",
    "        \n",
    "        # Check for concatenated words\n",
    "        concatenated = len(re.findall(r'[\\u0600-\\u06FF]{30,}', chunk_content))\n",
    "        \n",
    "        # Check for encoding issues\n",
    "        encoding_issues = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff]+[\\u0600-\\u06FF]+|[\\u0600-\\u06FF]+[A-Za-z\\u4e00-\\u9fff]+', chunk_content))\n",
    "        \n",
    "        if foreign_chars == 0 and concatenated == 0 and encoding_issues == 0:\n",
    "            return 'clean'\n",
    "        elif foreign_chars < 3 and concatenated == 0 and encoding_issues == 0:\n",
    "            return 'minor'\n",
    "        else:\n",
    "            return 'heavy'\n",
    "\n",
    "    def light_clean_chunk(self, chunk_content):\n",
    "        \"\"\"Light cleaning using regex only\"\"\"\n",
    "        cleaned = chunk_content\n",
    "        \n",
    "        # Remove specific problematic punctuation\n",
    "        cleaned = re.sub(r'[ã€ï¼]', '', cleaned)\n",
    "        \n",
    "        # Remove short English words (1-2 letters)\n",
    "        cleaned = re.sub(r'\\b[A-Za-z]{1,2}\\b', '', cleaned)\n",
    "        \n",
    "        # Remove Chinese and Hebrew characters\n",
    "        cleaned = re.sub(r'[\\u4e00-\\u9fff\\u0590-\\u05ff\\u3040-\\u309f\\u30a0-\\u30ff]', '', cleaned)\n",
    "        \n",
    "        # Fix spacing issues\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        cleaned = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned)\n",
    "        \n",
    "        # Clean up markdown formatting\n",
    "        cleaned = re.sub(r'\\*{2,}', '', cleaned)\n",
    "        \n",
    "        return cleaned.strip()\n",
    "\n",
    "    def surgical_ai_correction(self, chunk_content, context, corruption_analysis):\n",
    "        \"\"\"\n",
    "        Surgical AI correction focused on specific problems\n",
    "        \"\"\"\n",
    "        # Identify specific issues in this chunk\n",
    "        chunk_issues = []\n",
    "        \n",
    "        if re.search(r'[A-Za-z]{3,}', chunk_content):\n",
    "            chunk_issues.append(\"English words\")\n",
    "        if re.search(r'[\\u4e00-\\u9fff]', chunk_content):\n",
    "            chunk_issues.append(\"Chinese characters\")\n",
    "        if re.search(r'[\\u0590-\\u05ff]', chunk_content):\n",
    "            chunk_issues.append(\"Hebrew characters\")\n",
    "        if re.search(r'[\\u0600-\\u06FF]{30,}', chunk_content):\n",
    "            chunk_issues.append(\"concatenated words\")\n",
    "        \n",
    "        issues_description = \", \".join(chunk_issues) if chunk_issues else \"minor formatting issues\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an Arabic text correction specialist. Fix ONLY the specific issues in this small text chunk.\n",
    "\n",
    "CHUNK CONTEXT: {context}\n",
    "ISSUES TO FIX: {issues_description}\n",
    "CHUNK LENGTH: {len(chunk_content)} characters\n",
    "\n",
    "ORIGINAL CHUNK:\n",
    "{chunk_content}\n",
    "\n",
    "CORRECTION INSTRUCTIONS:\n",
    "1. Replace English words with appropriate Arabic equivalents in context\n",
    "2. Remove Chinese/Hebrew characters and replace with contextually appropriate Arabic\n",
    "3. Fix concatenated Arabic words by adding proper spaces\n",
    "4. Keep the EXACT same meaning and dialogue structure\n",
    "5. Maintain or slightly increase the length\n",
    "6. Do NOT change speaker names or dialogue flow\n",
    "\n",
    "CRITICAL: Return ONLY the corrected Arabic text. No explanations or comments.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an Arabic text correction specialist. Make minimal, precise corrections while preserving meaning and length.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,  # Low temperature for consistent corrections\n",
    "                max_tokens=1000   # Limit tokens to prevent over-expansion\n",
    "            )\n",
    "            \n",
    "            corrected = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Basic cleanup\n",
    "            corrected = self.light_clean_chunk(corrected)\n",
    "            \n",
    "            return corrected\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" (AI failed: {e}) \", end=\"\")\n",
    "            return self.light_clean_chunk(chunk_content)\n",
    "\n",
    "    def validate_micro_chunk(self, cleaned_chunk, original_chunk):\n",
    "        \"\"\"\n",
    "        Validate that micro-chunk correction was successful\n",
    "        \"\"\"\n",
    "        if not cleaned_chunk or len(cleaned_chunk.strip()) < 10:\n",
    "            return False\n",
    "        \n",
    "        # Check that length wasn't reduced too much\n",
    "        length_ratio = len(cleaned_chunk) / len(original_chunk) if original_chunk else 0\n",
    "        if length_ratio < 0.7:  # Lost more than 30% of content\n",
    "            return False\n",
    "        \n",
    "        # Check for remaining major foreign content\n",
    "        major_foreign = re.findall(r'[\\u4e00-\\u9fff\\u0590-\\u05ff]{2,}|[A-Za-z]{4,}', cleaned_chunk)\n",
    "        if len(major_foreign) > 1:  # Allow 1 instance (might be technical term)\n",
    "            return False\n",
    "        \n",
    "        # Check for basic dialogue structure if original had it\n",
    "        if ':' in original_chunk and ':' not in cleaned_chunk:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def intelligent_reassembly(self, cleaned_chunks, original_micro_chunks, original_script):\n",
    "        \"\"\"\n",
    "        Intelligently reassemble micro-chunks back into complete script\n",
    "        \"\"\"\n",
    "        # Group chunks back into sections\n",
    "        sections = {'intro': [], 'main_discussion': [], 'closing': [], 'other': []}\n",
    "        \n",
    "        for i, chunk in enumerate(cleaned_chunks):\n",
    "            if not chunk.strip():\n",
    "                continue\n",
    "            \n",
    "            context = original_micro_chunks[i]['context'] if i < len(original_micro_chunks) else 'other'\n",
    "            \n",
    "            if 'intro' in context:\n",
    "                sections['intro'].append(chunk)\n",
    "            elif 'main_discussion' in context:\n",
    "                sections['main_discussion'].append(chunk)\n",
    "            elif 'closing' in context:\n",
    "                sections['closing'].append(chunk)\n",
    "            else:\n",
    "                sections['other'].append(chunk)\n",
    "        \n",
    "        # Reassemble with proper structure\n",
    "        script_parts = []\n",
    "        \n",
    "        # Add intro section\n",
    "        if sections['intro']:\n",
    "            script_parts.append(\"=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['intro'] if chunk.strip()])\n",
    "        \n",
    "        # Add main discussion\n",
    "        if sections['main_discussion']:\n",
    "            script_parts.append(\"=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['main_discussion'] if chunk.strip()])\n",
    "        \n",
    "        # Add closing\n",
    "        if sections['closing']:\n",
    "            script_parts.append(\"=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['closing'] if chunk.strip()])\n",
    "        \n",
    "        # Add other content\n",
    "        if sections['other']:\n",
    "            script_parts.extend([chunk for chunk in sections['other'] if chunk.strip()])\n",
    "        \n",
    "        # Join with proper spacing\n",
    "        complete_script = '\\n\\n'.join([part for part in script_parts if part.strip()])\n",
    "        \n",
    "        # Clean up spacing\n",
    "        complete_script = re.sub(r'\\n{3,}', '\\n\\n', complete_script)\n",
    "        \n",
    "        return complete_script.strip()\n",
    "\n",
    "    def verify_structure_preservation(self, original_script, final_script):\n",
    "        \"\"\"\n",
    "        Verify that the original structure was preserved\n",
    "        \"\"\"\n",
    "        # Check section headers\n",
    "        original_sections = len(re.findall(r'===.*===', original_script))\n",
    "        final_sections = len(re.findall(r'===.*===', final_script))\n",
    "        \n",
    "        if original_sections > 0 and final_sections < original_sections:\n",
    "            return False\n",
    "        \n",
    "        # Check speaker preservation\n",
    "        original_speakers = set(re.findall(r'^([^:]+):', original_script, re.MULTILINE))\n",
    "        final_speakers = set(re.findall(r'^([^:]+):', final_script, re.MULTILINE))\n",
    "        \n",
    "        # Should preserve most speakers\n",
    "        if len(final_speakers) < len(original_speakers) * 0.8:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def non_ai_length_recovery(self, script, target_length):\n",
    "        \"\"\"\n",
    "        Recover length using non-AI methods (pattern-based expansion)\n",
    "        \"\"\"\n",
    "        current_length = len(script)\n",
    "        if current_length >= target_length * 0.9:\n",
    "            return script\n",
    "        \n",
    "        # Natural Arabic conversation extenders\n",
    "        extenders = [\n",
    "            (\"Ø¯. \", \"Ø¯. Ø§Ù„Ù…Ø­ØªØ±Ù… \"),\n",
    "            (\"Ø£Ø³ØªØ§Ø° \", \"Ø£Ø³ØªØ§Ø° ÙØ§Ø¶Ù„ \"),\n",
    "            (\"Ù†Ø¹Ù…\", \"Ù†Ø¹Ù… Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯\"),\n",
    "            (\"Ø¨Ø§Ù„Ø·Ø¨Ø¹\", \"Ø¨Ø§Ù„Ø·Ø¨Ø¹ ÙˆØ¨Ù„Ø§ Ø´Ùƒ\"),\n",
    "            (\"Ù‡Ø°Ø§\", \"Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±\"),\n",
    "            (\"Ù…Ù…ØªØ§Ø²\", \"Ù…Ù…ØªØ§Ø² Ø¬Ø¯Ø§Ù‹\"),\n",
    "            (\"ØµØ­ÙŠØ­\", \"ØµØ­ÙŠØ­ ØªÙ…Ø§Ù…Ø§Ù‹\"),\n",
    "            (\"Ø£Ø¹ØªÙ‚Ø¯\", \"Ø£Ø¹ØªÙ‚Ø¯ Ø¨Ù‚ÙˆØ©\"),\n",
    "            (\"ÙŠÙ…ÙƒÙ†\", \"ÙŠÙ…ÙƒÙ† Ø¨Ø§Ù„ÙØ¹Ù„\"),\n",
    "            (\"Ø§Ù„Ù…Ù‡Ù…\", \"ÙˆØ§Ù„Ø£Ù…Ø± Ø§Ù„Ù…Ù‡Ù…\"),\n",
    "        ]\n",
    "        \n",
    "        expanded = script\n",
    "        chars_added = 0\n",
    "        target_addition = min(target_length - current_length, current_length // 5)  # Max 20% expansion\n",
    "        \n",
    "        for original, replacement in extenders:\n",
    "            if chars_added >= target_addition:\n",
    "                break\n",
    "            \n",
    "            if original in expanded:\n",
    "                # Replace some instances (not all to avoid repetition)\n",
    "                count = expanded.count(original)\n",
    "                replace_count = min(count // 3, 2)  # Replace 1/3, max 2 instances\n",
    "                \n",
    "                for _ in range(replace_count):\n",
    "                    expanded = expanded.replace(original, replacement, 1)\n",
    "                    chars_added += len(replacement) - len(original)\n",
    "                    \n",
    "                    if chars_added >= target_addition:\n",
    "                        break\n",
    "        \n",
    "        return expanded\n",
    "\n",
    "    def validate_final_script(self, script):\n",
    "        \"\"\"\n",
    "        Final validation of the complete script\n",
    "        \"\"\"\n",
    "        if not script or len(script.strip()) < 50:\n",
    "            return False\n",
    "        \n",
    "        # Check for basic dialogue structure\n",
    "        if ':' not in script:\n",
    "            return False\n",
    "        \n",
    "        # Check for excessive foreign content (relaxed)\n",
    "        foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', script))\n",
    "        total_chars = len(re.sub(r'[\\s\\n\\t:=\\-ØŒ.ØŸ!\"()[\\]{}]', '', script))\n",
    "        \n",
    "        if total_chars > 0:\n",
    "            foreign_ratio = foreign_chars / total_chars\n",
    "            if foreign_ratio > 0.1:  # Allow up to 10% foreign (technical terms, etc.)\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _estimate_duration(self, script_text):\n",
    "        \"\"\"Estimate podcast duration\"\"\"\n",
    "        if not script_text:\n",
    "            return \"0-1 minutes\"\n",
    "        \n",
    "        word_count = len(script_text.split())\n",
    "        duration_minutes = word_count / 150\n",
    "        \n",
    "        min_duration = max(1, int(duration_minutes - 1))\n",
    "        max_duration = int(duration_minutes + 2)\n",
    "        \n",
    "        return f\"{min_duration}-{max_duration} minutes\"\n",
    "\n",
    "    def _generate_complete_fallback(self):\n",
    "        \"\"\"Generate complete fallback script\"\"\"\n",
    "        fallback_script = \"\"\"=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\n",
    "Ø¯. ÙØ§Ø·Ù…Ø© Ø§Ù„Ø²Ù‡Ø±Ø§Ø¡: Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù… ÙÙŠ Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø¨Ø±Ù†Ø§Ù…Ø¬Ù†Ø§. Ø§Ù„ÙŠÙˆÙ… Ù†Ø³ØªØ¶ÙŠÙ Ø®Ø¨ÙŠØ±Ø§Ù‹ Ù…ØªÙ…ÙŠØ²Ø§Ù‹ Ù„Ù†Ù†Ø§Ù‚Ø´ Ù…ÙˆØ¶ÙˆØ¹Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹ ÙŠÙ‡Ù… Ø§Ù„Ø¬Ù…ÙŠØ¹.\n",
    "\n",
    "Ù…. Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ù‡Ø§Ø´Ù…ÙŠ: Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨ÙƒÙ…ØŒ Ø£Ø´ÙƒØ±ÙƒÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¶Ø§ÙØ© Ø§Ù„ÙƒØ±ÙŠÙ…Ø©. Ø³Ø¹ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ Ø¨ÙˆØ¬ÙˆØ¯ÙŠ Ù…Ø¹ÙƒÙ… Ø§Ù„ÙŠÙˆÙ….\n",
    "\n",
    "Ø¯. ÙØ§Ø·Ù…Ø© Ø§Ù„Ø²Ù‡Ø±Ø§Ø¡: Ù†Ø­Ù† Ø³Ø¹Ø¯Ø§Ø¡ Ø¨ÙˆØ¬ÙˆØ¯Ùƒ Ù…Ø¹Ù†Ø§. Ø¯Ø¹Ù†Ø§ Ù†Ø¨Ø¯Ø£ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ù…ÙÙŠØ¯.\n",
    "\n",
    "=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\n",
    "Ø¯. ÙØ§Ø·Ù…Ø© Ø§Ù„Ø²Ù‡Ø±Ø§Ø¡: Ø¨Ø¯Ø§ÙŠØ©ØŒ Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ Ø£Ù‡Ù…ÙŠØ© Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ ÙˆÙ‚ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØŸ\n",
    "\n",
    "Ù…. Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ù‡Ø§Ø´Ù…ÙŠ: Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ ØºØ§ÙŠØ© Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø­Ù‚Ø§Ù‹. Ù†Ø­Ù† Ù†ÙˆØ§Ø¬Ù‡ ØªØ­Ø¯ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© ØªØªØ·Ù„Ø¨ Ù…Ù†Ø§ ÙÙ‡Ù…Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹ ÙˆÙ†Ø¸Ø±Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø£Ù…ÙˆØ±.\n",
    "\n",
    "Ø¯. ÙØ§Ø·Ù…Ø© Ø§Ù„Ø²Ù‡Ø±Ø§Ø¡: Ù…Ù…ÙƒÙ† ØªØ­Ø¯Ø«Ù†Ø§ Ø£ÙƒØ«Ø± Ø¹Ù† Ù‡Ø°Ù‡ Ø§Ù„ØªØ­Ø¯ÙŠØ§ØªØŸ\n",
    "\n",
    "Ù…. Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ù‡Ø§Ø´Ù…ÙŠ: Ø¨Ø§Ù„Ø·Ø¨Ø¹. Ù…Ù† Ø£Ù‡Ù… Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ù‡Ùˆ ÙƒÙŠÙÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„ØªØ·ÙˆØ±Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙˆØ§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ù†Ø§ ÙˆØ«ÙˆØ§Ø¨ØªÙ†Ø§ Ø§Ù„Ø£ØµÙŠÙ„Ø©.\n",
    "\n",
    "Ø¯. ÙØ§Ø·Ù…Ø© Ø§Ù„Ø²Ù‡Ø±Ø§Ø¡: Ù†Ù‚Ø·Ø© Ù…Ù‡Ù…Ø© Ø¬Ø¯Ø§Ù‹. ÙˆÙ…Ø§ Ù‡ÙŠ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚ØªØ±Ø­Ù‡Ø§ØŸ\n",
    "\n",
    "Ù…. Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ù‡Ø§Ø´Ù…ÙŠ: Ø£Ø¹ØªÙ‚Ø¯ Ø£Ù† Ø§Ù„Ø­Ù„ ÙŠÙƒÙ…Ù† ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„ØªÙˆØ¹ÙŠØ©ØŒ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ù…Ù† Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø¨Ù…Ø§ ÙŠØ®Ø¯Ù… Ù…ØµØ§Ù„Ø­Ù†Ø§ ÙˆØ£Ù‡Ø¯Ø§ÙÙ†Ø§.\n",
    "\n",
    "=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\n",
    "Ø¯. ÙØ§Ø·Ù…Ø© Ø§Ù„Ø²Ù‡Ø±Ø§Ø¡: ÙÙŠ Ø®ØªØ§Ù… Ø­Ù„Ù‚ØªÙ†Ø§ Ø§Ù„ÙŠÙˆÙ…ØŒ Ø£Ø´ÙƒØ±Ùƒ Ø£Ø³ØªØ§Ø° Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø«Ø±ÙŠ ÙˆØ§Ù„Ù…ÙÙŠØ¯.\n",
    "\n",
    "Ù…. Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ù‡Ø§Ø´Ù…ÙŠ: Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¶Ø§ÙØ© Ø§Ù„ÙƒØ±ÙŠÙ…Ø©. ÙƒØ§Ù† Ù†Ù‚Ø§Ø´ Ù…Ù…ØªØ¹ ÙˆÙ…Ø«Ù…Ø±ØŒ ÙˆØ£ØªÙ…Ù†Ù‰ Ø£Ù† ÙŠØ³ØªÙÙŠØ¯ Ù…Ù†Ù‡ Ø§Ù„Ù…Ø³ØªÙ…Ø¹ÙˆÙ†.\n",
    "\n",
    "Ø¯. ÙØ§Ø·Ù…Ø© Ø§Ù„Ø²Ù‡Ø±Ø§Ø¡: Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯. ÙˆØ´ÙƒØ±Ø§Ù‹ Ù„ÙƒÙ… Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„ÙƒØ±Ø§Ù… Ø¹Ù„Ù‰ Ù…ØªØ§Ø¨Ø¹ØªÙƒÙ… Ø§Ù„Ø¯Ø§Ø¦Ù…Ø©. Ù†Ù„Ù‚Ø§ÙƒÙ… ÙÙŠ Ø­Ù„Ù‚Ø© Ù‚Ø§Ø¯Ù…Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡. Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'complete_script': fallback_script,\n",
    "            'cleaning_method': 'micro_chunking_fallback',\n",
    "            'micro_chunks_processed': 0,\n",
    "            'cleaning_status': 'fallback_used',\n",
    "            'script_length': len(fallback_script),\n",
    "            'estimated_duration': self._estimate_duration(fallback_script)\n",
    "        }\n",
    "\n",
    "\n",
    "# Testing Function\n",
    "def test_micro_chunking_cleaner(deployment, corrupted_script_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the micro-chunking AI script cleaner\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"ğŸ§ª TESTING MICRO-CHUNKING AI SCRIPT CLEANER\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaner = MicroChunkingAIScriptCleaner(deployment, model_name)\n",
    "    \n",
    "    # Show original script info\n",
    "    if isinstance(corrupted_script_result, dict):\n",
    "        original_script = corrupted_script_result.get('complete_script', '')\n",
    "    else:\n",
    "        original_script = str(corrupted_script_result)\n",
    "    \n",
    "    print(\"ğŸ“Š ORIGINAL SCRIPT INFO\".center(60, \"-\"))\n",
    "    print(f\"{'Original Length:':<25} {len(original_script):,} characters\")\n",
    "    print(f\"{'Estimated Duration:':<25} {cleaner._estimate_duration(original_script)}\")\n",
    "    \n",
    "    # Quick preview of corruption\n",
    "    foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', original_script))\n",
    "    corruption_ratio = foreign_chars / len(original_script) if original_script else 0\n",
    "    print(f\"{'Apparent Corruption:':<25} {corruption_ratio:.1%} foreign content\")\n",
    "    \n",
    "    # Clean the script\n",
    "    cleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\n",
    "    \n",
    "    print(\"\\nğŸ“Š MICRO-CHUNKING RESULTS\".center(60, \"-\"))\n",
    "    print(f\"{'Method:':<25} {cleaned_result['cleaning_method']}\")\n",
    "    print(f\"{'Status:':<25} {cleaned_result['cleaning_status']}\")\n",
    "    print(f\"{'Micro-chunks Processed:':<25} {cleaned_result['micro_chunks_processed']}\")\n",
    "    print(f\"{'Original Length:':<25} {cleaned_result.get('original_length', 'N/A'):,} characters\")\n",
    "    print(f\"{'Final Length:':<25} {cleaned_result['script_length']:,} characters\")\n",
    "    print(f\"{'Length Preserved:':<25} {cleaned_result.get('length_ratio', 0):.1%}\")\n",
    "    print(f\"{'Duration:':<25} {cleaned_result['estimated_duration']}\")\n",
    "    \n",
    "    # Length preservation status\n",
    "    if 'length_ratio' in cleaned_result:\n",
    "        if cleaned_result['length_ratio'] >= 0.90:\n",
    "            print(f\"{'Length Status:':<25} âœ… EXCELLENT PRESERVATION\")\n",
    "        elif cleaned_result['length_ratio'] >= 0.80:\n",
    "            print(f\"{'Length Status:':<25} âœ… GOOD PRESERVATION\")\n",
    "        elif cleaned_result['length_ratio'] >= 0.70:\n",
    "            print(f\"{'Length Status:':<25} âš ï¸ MODERATE LOSS\")\n",
    "        else:\n",
    "            print(f\"{'Length Status:':<25} âŒ SIGNIFICANT LOSS\")\n",
    "    \n",
    "    # Corruption analysis summary\n",
    "    if 'corruption_analysis' in cleaned_result:\n",
    "        corruption_info = cleaned_result['corruption_analysis']\n",
    "        print(f\"{'Corruption Detected:':<25} {corruption_info['overall_corruption_level'].upper()}\")\n",
    "        print(f\"{'Foreign Patterns:':<25} {len(corruption_info['foreign_patterns'])} types found\")\n",
    "    \n",
    "    # Display cleaned script with formatting\n",
    "    print(\"\\n\" + \"ğŸ™ï¸ MICRO-CLEANED SCRIPT OUTPUT\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaned_script = cleaned_result['complete_script']\n",
    "    if cleaned_script:\n",
    "        sections = cleaned_script.split('\\n\\n')\n",
    "        \n",
    "        for section in sections[:10]:  # Show first 10 sections to avoid overwhelming output\n",
    "            section = section.strip()\n",
    "            if not section:\n",
    "                continue\n",
    "            \n",
    "            # Section headers\n",
    "            if section.startswith('===') and section.endswith('==='):\n",
    "                print(f\"\\n{section}\")\n",
    "                print(\"â”€\" * len(section))\n",
    "                continue\n",
    "            \n",
    "            # Format dialogue\n",
    "            lines = section.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                if ':' in line and not line.startswith('==='):\n",
    "                    speaker, dialogue = line.split(':', 1)\n",
    "                    speaker = speaker.strip()\n",
    "                    dialogue = dialogue.strip()\n",
    "                    \n",
    "                    print(f\"\\n{speaker}:\")\n",
    "                    \n",
    "                    # Word wrap dialogue\n",
    "                    words = dialogue.split()\n",
    "                    current_line = \"\"\n",
    "                    max_length = 70\n",
    "                    \n",
    "                    for word in words:\n",
    "                        if len(current_line) + len(word) + 1 > max_length and current_line:\n",
    "                            print(f\"    {current_line}\")\n",
    "                            current_line = word\n",
    "                        else:\n",
    "                            current_line = current_line + \" \" + word if current_line else word\n",
    "                    \n",
    "                    if current_line:\n",
    "                        print(f\"    {current_line}\")\n",
    "                else:\n",
    "                    print(f\"    {line}\")\n",
    "    \n",
    "    # Final validation results\n",
    "    is_clean = cleaner.validate_final_script(cleaned_script)\n",
    "    print(f\"\\nğŸ” FINAL VALIDATION: {'âœ… PASSED' if is_clean else 'âŒ NEEDS REVIEW'}\")\n",
    "    \n",
    "    # Quality comparison\n",
    "    if original_script and cleaned_script:\n",
    "        original_foreign = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', original_script))\n",
    "        cleaned_foreign = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', cleaned_script))\n",
    "        \n",
    "        print(f\"ğŸ§¹ CLEANING EFFECTIVENESS:\")\n",
    "        print(f\"    Foreign chars removed: {original_foreign - cleaned_foreign:,}\")\n",
    "        print(f\"    Cleaning efficiency: {((original_foreign - cleaned_foreign) / original_foreign * 100) if original_foreign > 0 else 0:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return cleaned_result\n",
    "\n",
    "\n",
    "# Advanced Testing Function with Detailed Analysis\n",
    "def detailed_micro_chunk_analysis(deployment, corrupted_script_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Detailed analysis of micro-chunking performance\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"ğŸ”¬ DETAILED MICRO-CHUNKING ANALYSIS\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaner = MicroChunkingAIScriptCleaner(deployment, model_name)\n",
    "    \n",
    "    # Extract script\n",
    "    if isinstance(corrupted_script_result, dict):\n",
    "        original_script = corrupted_script_result.get('complete_script', '')\n",
    "    else:\n",
    "        original_script = str(corrupted_script_result)\n",
    "    \n",
    "    print(\"ğŸ” PRE-PROCESSING ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Detailed corruption analysis\n",
    "    corruption_analysis = cleaner.analyze_corruption_patterns(original_script)\n",
    "    \n",
    "    print(f\"Script length: {corruption_analysis['total_chars']:,} characters\")\n",
    "    print(f\"Overall corruption level: {corruption_analysis['overall_corruption_level'].upper()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Foreign content breakdown:\")\n",
    "    for pattern_name, details in corruption_analysis['foreign_patterns'].items():\n",
    "        print(f\"  â€¢ {details['description']}: {details['count']} instances ({details['chars']} chars)\")\n",
    "        if details['examples']:\n",
    "            examples_preview = [str(ex)[:10] + ('...' if len(str(ex)) > 10 else '') for ex in details['examples'][:2]]\n",
    "            print(f\"    Examples: {', '.join(examples_preview)}\")\n",
    "    \n",
    "    if corruption_analysis['concatenated_words'] > 0:\n",
    "        print(f\"  â€¢ Concatenated word sequences: {corruption_analysis['concatenated_words']}\")\n",
    "    if corruption_analysis['encoding_issues'] > 0:\n",
    "        print(f\"  â€¢ Mixed encoding issues: {corruption_analysis['encoding_issues']}\")\n",
    "    \n",
    "    print(\"\\nğŸ”¬ MICRO-CHUNKING BREAKDOWN\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create micro-chunks for analysis\n",
    "    micro_chunks = cleaner.create_micro_chunks(original_script)\n",
    "    \n",
    "    print(f\"Total micro-chunks created: {len(micro_chunks)}\")\n",
    "    if micro_chunks:\n",
    "        avg_chunk_size = sum(chunk['length'] for chunk in micro_chunks) / len(micro_chunks)\n",
    "        print(f\"Average chunk size: {avg_chunk_size:.1f} characters\")\n",
    "        \n",
    "        chunk_sizes = [chunk['length'] for chunk in micro_chunks]\n",
    "        print(f\"Chunk size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "    \n",
    "    # Analyze chunk corruption levels\n",
    "    corruption_levels = {'clean': 0, 'minor': 0, 'heavy': 0}\n",
    "    for chunk in micro_chunks:\n",
    "        level = cleaner.assess_chunk_corruption(chunk['content'])\n",
    "        corruption_levels[level] += 1\n",
    "    \n",
    "    print(f\"\\nCorruption level distribution:\")\n",
    "    print(f\"  â€¢ Clean chunks: {corruption_levels['clean']} ({corruption_levels['clean']/len(micro_chunks)*100:.1f}%)\")\n",
    "    print(f\"  â€¢ Minor issues: {corruption_levels['minor']} ({corruption_levels['minor']/len(micro_chunks)*100:.1f}%)\")\n",
    "    print(f\"  â€¢ Heavy corruption: {corruption_levels['heavy']} ({corruption_levels['heavy']/len(micro_chunks)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ PROCESSING STRATEGY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Expected processing:\")\n",
    "    print(f\"  â€¢ Keep as-is: {corruption_levels['clean']} chunks\")\n",
    "    print(f\"  â€¢ Light regex cleaning: {corruption_levels['minor']} chunks\") \n",
    "    print(f\"  â€¢ AI surgical correction: {corruption_levels['heavy']} chunks\")\n",
    "    \n",
    "    estimated_ai_calls = corruption_levels['heavy']\n",
    "    print(f\"  â€¢ Estimated AI API calls: {estimated_ai_calls}\")\n",
    "    \n",
    "    if estimated_ai_calls > 0:\n",
    "        print(f\"  â€¢ Efficiency vs. large chunks: {estimated_ai_calls} calls vs ~3-4 calls (traditional)\")\n",
    "        print(f\"  â€¢ Trade-off: More calls but higher precision per call\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ PREDICTION:\")\n",
    "    clean_ratio = corruption_levels['clean'] / len(micro_chunks)\n",
    "    if clean_ratio > 0.7:\n",
    "        print(\"  Expected outcome: EXCELLENT length preservation (70%+ content kept as-is)\")\n",
    "    elif clean_ratio > 0.5:\n",
    "        print(\"  Expected outcome: GOOD length preservation (50%+ content kept as-is)\")\n",
    "    else:\n",
    "        print(\"  Expected outcome: MODERATE length preservation (heavy corruption detected)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'total_chunks': len(micro_chunks),\n",
    "        'corruption_analysis': corruption_analysis,\n",
    "        'corruption_distribution': corruption_levels,\n",
    "        'estimated_ai_calls': estimated_ai_calls,\n",
    "        'clean_content_ratio': clean_ratio\n",
    "    }\n",
    "\n",
    "\n",
    "# Usage Examples:\n",
    "\"\"\"\n",
    "# Basic cleaning\n",
    "cleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\n",
    "cleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\n",
    "\n",
    "# Testing with detailed output\n",
    "test_result = test_micro_chunking_cleaner(deployment, corrupted_script_result)\n",
    "\n",
    "# Detailed pre-processing analysis\n",
    "analysis = detailed_micro_chunk_analysis(deployment, corrupted_script_result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94117e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================ğŸ”¬ MICRO-CHUNKING AI SCRIPT CLEANER=======================\n",
      "\n",
      "ğŸ“ Original script length: 2,913 characters\n",
      "-------------------ğŸ” CORRUPTION ANALYSIS--------------------\n",
      "    Overall corruption level: LIGHT\n",
      "    Foreign patterns found: 1\n",
      "      - English words (3+ letters): 1 instances\n",
      "        Examples: strong\n",
      "    Mixed encoding issues: 1\n",
      "-----------------\n",
      "ğŸ“ MICRO-CHUNKING STRATEGY-----------------\n",
      "    Created 12 micro-chunks (avg: 242 chars each)\n",
      "----------------\n",
      "ğŸ”¬ SURGICAL CLEANING PROCESS----------------\n",
      "    Processing micro-chunk 1/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 2/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 3/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 4/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 5/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 6/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 7/12... ğŸ”§ AI CORRECTED\n",
      "    Processing micro-chunk 8/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 9/12... ğŸ”§ AI CORRECTED\n",
      "    Processing micro-chunk 10/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 11/12... âœ… CLEAN (kept as-is)\n",
      "    Processing micro-chunk 12/12... âœ… CLEAN (kept as-is)\n",
      "-------------\n",
      "ğŸ”§ REASSEMBLY WITH STRUCTURE CHECK-------------\n",
      "    Original structure preserved: âœ…\n",
      "    Length preservation: 100.3%\n",
      "\n",
      "------------------ğŸ‰ MICRO-CHUNKING SUMMARY------------------\n",
      "    Status: SUCCESS\n",
      "    Original Length: 2,913 characters\n",
      "    Final Length: 2,922 characters\n",
      "    Length Preserved: 100.3%\n",
      "    Micro-chunks Processed: 12\n",
      "    Arabic Quality: âœ… VALID\n",
      "================================================================================\n",
      "Cleaned Result: {'complete_script': '=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø¬Ù…ÙŠØ¹Ø§Ù‹ØŒ ÙˆÙ†Ø±Ø­Ø¨ Ø¨ÙƒÙ… ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„ØªÙŠ Ù†ØºÙˆØµ ÙÙŠÙ‡Ø§ Ø¨Ø¹Ù…Ù‚ Ù„ØªÙÙ‡Ù… Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ø­Ø³Ø§Ø³ Ø¨ÙŠÙ† Ø«ÙˆØ±ØªÙ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ ÙˆØ«Ø±ÙˆØªÙ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙˆØ§Ø¹Ø¯Ø©Ø› Ø¯Ø¹ÙˆÙ†Ø§ Ù†ØªØ´Ø§Ø±Ùƒ Ø¨Ø§Ù‡ØªÙ…Ø§Ù… Ù†Ø³Ø¨Ø± ÙÙŠÙ‡ Ø¢ÙØ§Ù‚ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø²Ø¬ Ø§Ù„ÙØ±ÙŠØ¯ Ù„Ù…Ø³ØªÙ‚Ø¨Ù„Ù†Ø§ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ØªØ±Ø­ÙŠØ¨ Ù„Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ğ Ğ°ÑˆĞ¸Ğ´ØŒ Ù…Ù† Ø§Ù„Ø±Ø§Ø¦Ø¹ Ø£Ù†Ù‘Ù‡Ø§ Ù…Ø¹Ù†Ø§ Ø§Ù„ÙŠÙˆÙ… Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ù…Ø³Ø£Ù„Ø© Ø­ÙŠÙˆÙŠØ© Ø­ÙˆÙ„ Ø§Ù„ØªØ±Ø§Ø¨Ø· Ø¨ÙŠÙ† ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆÙ‡ÙˆÛŒØªÙÙ†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©.\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** Ø´ÙƒØ± Ù„ÙƒØŒ Ù†Ø¨Ø­Ø« Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ­Ø¯ÙŠØ§Øª Ù…Ø«ÙŠØ±Ø© ØªØªØ¹Ù„Ù‚ Ø¨Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© ÙÙŠ Ø¹Ø§Ù„Ù… Ù…ØªØ±Ø§Ø¨Ø· Ø¨Ø´ÙƒÙ„ Ø±Ù‚Ù…ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ÙƒÙŠÙ Ø¨Ø¥Ù…ÙƒØ§Ù†Ù†Ø§ Ø¶Ù…Ù‘ ØªÙ‚Ù†ÙŠÙ‘Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ù„Ø¯Ø¹Ù… Ù‚ÙŠÙ…Ù†Ø§ ÙˆÙ…Ø¹ØªÙ‚Ø¯Ø§ØªÙ†Ø§ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªÙ‡Ø¯ÙŠØ¯ ÙˆØ¬ÙˆØ¯Ù‡Ø§?\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** ÙŠØ¬Ø¨ Ø£Ù† Ù†Ø±ÙƒØ² Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø­Ù„ÙˆÙ„ Ø°ÙƒÙŠØ© ØªØ³ØªÙ„Ù‡Ù… Ø¬Ø°ÙˆØ±Ù‡Ø§ Ø§Ù„Ù‚ÙŠÙ…ÙŠÙ‘Ø© Ø§Ù„Ø«Ù‚Ø§ÙÙŠÙ‘Ø© Ø¨ÙŠÙ†Ù…Ø§ ØªØ±Ù†Ùˆ Ø¥Ù„Ù‰ Ù…Ø³ØªÙ‚Ø¨ÙÙ„ Ù…ØªÙ†ÙˆÙÙ‘Ø± ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠÙ‹Ø§ ÙˆÙ‚Ø§Ø¦Ù… Ø¹Ù„ÙÙ‰ Ø£Ø³Ø³ Ø£Ø®Ù„Ø§Ù‚ÙŠÙÙ‘Ø© ØµÙ„Ø¨Ø©.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ø§Ù‚ØªØ±Ø§Ø­ Ù…Ù…ØªØ§Ø²! Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø°ÙƒØ± Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ Ù„Ø¥Ø¯Ø§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ø§Ù†ØµÙ‡Ø§Ø± Ø§Ù„Ù†Ø§Ø¬Ø­ Ù†Ø­Ùˆ Ø¢ÙØ§Ù‚ ÙˆØ§Ø¹Ø¯Ø© Ù„Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙÙŠØŸ\\n**Ø¯. ÙÙØªÙ’Ù† Ø±ÙØ§Ø´ÙØ¯:** Ù†Ø¹Ù…ØŒ Ø¹Ù„ÙŠÙ†Ø§ ØªØ´ÙƒÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¯Ø§Ø®ÙŠÙ„Ø© Ø§Ù„Ù…Ø­ØªØ±Ù…Ø© Ù„Ø±ÙˆØ§ÙØ¯Ù†Ø§ Ø§Ù„ÙÙƒØ±ÙŠÙÙ‘Ù€Ù€Ø© ÙˆØªÙˆØ¸ÙŠÙ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø±ÙÙ‘Ø§Ù‚Ù…ÙŠØ© ÙƒÙ…Ù†Ø¨Ø± Ù„Ù„ØªÙˆØ§ØµÙ„ Ø¨Ù‚Ø¶Ø§ÙŠØ§ Ù…Ø¬ØªÙ…Ø¹ÙŠÙÙ‘Ø© Ø°Ø§Øª Ù…ØºØ²Ù‰ ÙˆÙØ§Ø¦Ø¯Ø© Ù…Ø´ØªØ±ÙƒØ©.\\n\\n=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ø¨Ø¯Ø§ÙŠØ©Ù‹, Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ Ø¯ÙˆØ± Ø±Ø¦ÙŠØ³ÙŠ Ù„ÙƒÙ† Ù‡Ù„ ØªØ¹Ù„Ù… Ø£Ù†Ù‡ Ù‚Ø¯ ÙŠÙÙ‡Ù…Ù„ Ø§Ù„Ø¨Ø¹Ø¶ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø£ÙŠØ¶Ø§Ù‹ØŸ\\n**Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯:** ØµØ­ÙŠØ­, Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¨Ø¯Ù†ÙŠ Ù…Ù‡Ù… Ù„ÙƒÙ† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„ØµØ­ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø£ÙŠ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØµØ­ÙŠ.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ† Ù„Ù„Ø±ÙŠØ§Ø¶Ø© Ø£Ù† ØªØ¹ÙˆØ¶ Ø¨Ø¹Ø¶ Ù‚ØµÙˆØ± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ Ø¨Ù…Ø­ØªÙˆØ§Ù‡Ø§ Ù…Ù† Ø§Ù„Ø³Ø¹Ø±Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ø­Ø±ÙˆÙ‚Ø©.\\n**Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯:** Ù‡Ø°Ø§ ØµØ­ÙŠØ­ ÙˆÙ„ÙƒÙ† Ø§Ù„Ø£ÙØ¶Ù„ Ù‡Ùˆ Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ù…Ø¹Ù‹Ø§Ø› Ù„ÙŠØ³ ÙÙ‚Ø· Ø§Ù„ØªØ¹ÙˆÙŠØ¶ Ø¹Ù† Ù†Ù‚Øµ ÙˆØ§Ø­Ø¯ Ø¨Ø§Ù„ØªÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¢Ø®Ø±.\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø±Ø¨Ù…Ø§ØŒ Ù„ÙƒÙ† Ø§Ù„ÙƒØ«ÙŠØ± ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† ØµØ­ØªÙ‡ Ø§Ù„Ù†ÙØ³ÙŠØ© Ø£ÙˆÙ„Ø§ØŒ ÙˆÙ‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø¬ÙÙ…Ù†Ø§Ø³ØªÙŠÙƒ Ø£Ùˆ Ø§Ù„ÙŠÙˆØ¬Ø§ Ù…ÙÙŠØ¯Ø§ Ù„Ù‡Ø°Ø§ Ø§Ù„ØºØ±Ø¶ Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ¨Ø±! Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ† Ø±Ø´ÙŠØ¯: ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø§Ù„ØªÙŠ ØªØ³ØªÙ…ØªØ¹ÙŠÙ† Ø¨Ù‡Ø§ ÙˆØªÙ†Ø§Ø³Ø¨ Ø¬Ø³Ø¯Ùƒ Ù‡ÙŠ Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ Ø¨Ù„Ø§ Ø´Ùƒ Ù„ØªØ¹Ø²ÙŠØ² ØµØ­ØªÙƒ Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆØ³Ù„Ø§Ù…ØªÙƒ Ø§Ù„Ø¹Ù‚Ù„ÙŠØ© Ø£ÙŠØ¶Ø§.\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ÙÙŠ Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŒ ÙƒÙŠÙ ØªØ±Ù‰ Ø¯ÙˆØ± Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙÙŠÙ‡Ø§ØŸ\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: ØªÙ‚Ù Ø­Ø¬Ø± Ø¹Ø«Ø±Ø© ÙƒØ«ÙŠØ±Ø© Ø­Ø³Ø¨ ØªØ¬Ø§Ø±Ø¨ÙŠ.\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ù„ÙƒÙ† Ø§Ù„Ø¨Ø¹Ø¶ ÙŠÙ‚ÙˆÙ„ Ø¥Ù†Ù‡Ø§ ÙØ±ØµØ© Ù„Ù„Ù†Ù…Ùˆ.\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ù†Ø¹Ù… Ø±Ø¨Ù…Ø§ØŒ Ù„ÙƒÙ†Ù‡Ø§ Ù‚Ø¯ ØªØ®Ù„Ù‚ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©.\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ØµØ­ÙŠØ­ ÙÙŠÙ…Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŒ Ù„ÙƒÙ† Ù‡Ù„ ØªØ¯Ø±Ùƒ Ø£Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø´Ø®Ø§Øµ ÙŠØ³ØªÙÙŠØ¯ÙˆÙ† Ù…Ù†Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø£ÙŠØ¶Ù‹Ø§ØŸ Ø¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ø¨Ø§Ù„Ø·Ø¨Ø¹ØŒ Ø§Ù„Ø£Ù…Ø± ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ÙˆØ¶Ø¹ ÙƒÙ„ ÙØ±Ø¯ØŒ ÙˆÙ„ÙƒÙ†Ù‡ ÙŠØ­ØªØ§Ø¬ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù‡Ø¯ Ù…ØªÙˆØ§ØµÙ„. Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ù„Ø§ Ø±ÙŠØ¨ ÙÙŠ Ø°Ù„ÙƒØŒ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ù‡Ùˆ Ù…ÙØªØ§Ø­ Ø§Ù„Ù†Ø¬Ø§Ø­ Ù‡Ù†Ø§.\\n\\n**Ø£Ø­Ù…Ø¯:** Ø­ÙˆÙ„ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨, ÙƒÙŠÙ ØªØ±Ø§Ù‡Ù \"ÙØ§ØªÙ†\" ØŸ\\n**ÙØ§ØªÙ†:** ØªØ­Ø¯ÙŠØ§Øª ÙƒØ«ÙŠØ±Ø© ØªØ­ØªØ§Ø¬ Ø¯Ø±Ø§Ø³Ø© Ù…ØªØ£Ù†ÙŠØ© Ù‚Ø¨Ù„ ØªÙ‚Ø¯ÙŠÙ… Ø­Ù„ÙˆÙ„.\\n**Ø£Ø­Ù…Ø¯:** Ù„ÙƒÙ† Ø§Ù„Ø¨Ø¹Ø¶ ÙŠØ¯Ø¹Ùˆ Ù„Ù„ØªØºÙŠÙŠØ± Ø§Ù„ÙÙˆØ±ÙŠ Ø£Ù„ÙŠØ³ ÙƒØ°Ù„Ùƒ?\\n**ÙØ§ØªÙ†:** Ø§Ù„Ø¥ØµÙ„Ø§Ø­ ÙŠØ­ØªØ§Ø¬ ÙˆÙ‚Øª ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„ ÙÙŠÙ‡.\\n**Ø£Ø­Ù…Ø¯:** Ø±Ø¨Ù…Ø§ Ù†Ù‚ØªØ±Ø­ Ø®Ø·ÙˆØ§Øª Ø³Ø±ÙŠØ¹Ø© ÙØ¹Ø§Ù„Ø© Ø¹ÙˆØ¶Ø§Ù‹ Ø¹Ù† Ø§Ù„ØªØ£Ø¬ÙŠÙ„ .\\n\\n**ÙØ§ØªÙ†:** Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ù…Ø¤Ù‚ØªØ© ÙˆÙ„ÙƒÙ†Ù‡Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø¯Ø±Ø§Ø³Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø£ÙˆÙ„Ù‹Ø§ .\\n**Ø£Ø­Ù…Ø¯:** Ø¥Ø°Ù† ØªÙˆØ§ÙÙ‚ÙŠÙ† Ø¨Ø£Ù† Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø¶Ø±ÙˆØ±ÙŠØ© Ù‚Ø¨Ù„Ù‡Ù… ÙƒÙ„Ø§Ù‡Ù…Ø§!\\n**ÙØ§ØªÙ†:** ØªÙ…Ø§Ù…Ù‹Ø§ ØŒÙ„ÙƒÙ† Ù„ÙŠØ³ Ø¹Ø°Ø± Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø±ÙƒØ© Ù†Ø­Ùˆ Ø§Ù„Ø­Ù„.\\n\\n=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\\n\\nØ£Ø­Ù…Ø¯: Ø´ÙƒØ±Ù‹Ø§ Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ†ØŒ ÙƒØ§Ù†Øª Ù…Ø­Ø§Ø¯Ø«Ø© ØºÙ†ÙŠØ© Ø¨Ø§Ù„ØªØ¬Ø§Ø±Ø¨.\\nÙØ§ØªÙ†: Ù…Ù† Ø§Ù„Ø±Ø§Ø¦Ø¹ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ø£ÙÙƒØ§Ø± Ù…Ø¹ÙƒÙ…ØŒ Ø£Ø´ÙƒØ±ÙƒÙ….\\nØ£Ø­Ù…Ø¯: ÙˆØ´ÙƒØ±Ø§Ù‹ Ù„Ù…Ø³ØªÙ…Ø¹ÙŠÙ†Ø§ Ø§Ù„Ø¹Ø²ÙŠØ²ÙŠÙ†ï¼Œ Ø³Ù†Ù„ØªÙ‚ÙŠ Ù…Ø¬Ø¯Ø¯Ù‹Ø§.\\nÙØ§ØªÙ†: Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡ ÙˆÙ†Ø±Ø§ÙƒÙ… Ù…Ø±Ø© Ø£Ø®Ø±Ù‰!', 'cleaning_method': 'micro_chunking_surgical', 'micro_chunks_processed': 12, 'cleaning_status': 'success', 'script_length': 2922, 'original_length': 2913, 'length_ratio': 1.0030895983522141, 'corruption_analysis': {'total_chars': 2913, 'foreign_patterns': {'english_words': {'count': 1, 'chars': 6, 'description': 'English words (3+ letters)', 'examples': ['strong']}}, 'encoding_issues': 1, 'concatenated_words': 0, 'structural_issues': 40, 'overall_corruption_level': 'light'}, 'estimated_duration': '2-5 minutes'}\n"
     ]
    }
   ],
   "source": [
    "# Basic cleaning\n",
    "cleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\n",
    "cleaned_result = cleaner.clean_script_with_ai(script_result)\n",
    "print(\"Cleaned Result:\", cleaned_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "226c9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'complete_script': '=== Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø§Ù†Ø·Ù„Ù‚ Ø¨Ù†Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ù‚Ù„Ø¨ ØªØ­Ø¯Ù Ø¹ØµØ±ÙŠ ÙŠØºÙˆØµ ÙÙŠ Ø£Ø¹Ù…Ø§Ù‚ ØªÙ‚Ø§Ø·Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©, Ø­ÙŠØ« Ø³Ù†Ø¶Ø¹ ÙƒÙ„Ø§Ù†Ø§ Ø£ÙŠØ¯ÙŠÙ‡Ù…Ø§ Ø¨Ù‚Ù„Ø¨ Ù…ØªÙ†Ø¨Ù‡ ÙˆÙ…Ø´ÙˆÙ‚.... Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ÙƒÙŠÙ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø«Ù‚Ø§ÙØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ ÙŠØ´ØºÙ„ Ø¨Ø§Ù„ÙŠ Ù…Ù† ÙØªØ±Ø©. ÙŠØ¹Ù†ÙŠØŒ ÙƒÙ„Ù†Ø§ Ù†ÙˆØ§Ø¬Ù‡ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± Ø¨Ø´ÙƒÙ„ Ø£Ùˆ Ø¨Ø¢Ø®Ø±ØŒ ØµØ­ØŸ\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** Ù…Ø¹ÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø¨Ø§Ø­Ø«Ø© Ø§Ù„Ù…ØªÙ…ÙŠØ²Ø© Ø§Ù„ØªÙŠ Ù„Ø§ ØªØªÙˆÙ‚Ù Ø¹Ù† ØªØ­Ø¯ÙŠÙ†Ø§ Ø¨Ø¥Ø³Ø¦Ù„ØªÙ‡Ø§ Ø§Ù„Ù…Ø­ÙØ²Ø© Ø­ÙˆÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ®ØµÙˆØµÙŠØ§ØªÙ†Ø§ Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©ØŒ Ø§Ù„Ø¯ÙƒØªÙˆØ±ÙØ©Ù ÙØ§ØªÙÙ†Ù’ Ø±ÙØ§Ø´ÙØ¯Ù’. Ø¯. ÙØ§ØªÙÙ†Ù’ØŒ Ù‡Ù„Ø§Ù‘ Ø´Ø±Ø­Øª Ù„ÙŠ ÙƒØ¯Ù‡ Ù„ÙŠÙ‡ Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¹Ø¶ Ø´Ø¹ÙˆØ± Ø¥Ø²Ù‘Ø§ ØªØ±Ø§Ø«Ù†Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© ØªÙ‡Ø¯Ù‘Ø¯ Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„Ù‚ÙˆÙ…ÙŠØ©ØŸ!\\n\\n**Ø¯. ÙØ§ØªÙÙ†Ù’:** Ø³Ø¤Ø§Ù„ Ø¬Ù…ÙŠÙ„ Ù…Ø´ Ø¬Ø¯ÙŠØ¯ Ù„ÙƒÙ†Ù‡ Ø­ÙŠÙˆÙŠ. Ø¨Ø§Ù„Ø¹ÙƒØ³ØŒ Ù†Ø­ØªÙ…Ù„ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø¢Ù„ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ³ÙŠÙ„Ø© ÙØ¹ÙÙ‘Ø§Ù„Ø© Ù„Ø­ÙØ¸ ØªÙ‚Ø§Ù„ÙŠØ¯Ù†Ø§ØŒ Ø´Ø±Ø· Ø£Ù† Ù†ÙˆÙØ± Ù„Ù‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ø¨Ù„ØºØ§ØªÙ†Ø§ Ø§Ù„Ø¹Ø±ÙŠÙ‚Ø© ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ù… Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù…Ø¯Ù„ÙˆÙ„Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø³Ø§Ø³Ø©.\\n\\n**Ø£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ:** ÙƒÙ„Ø§Ù… Ù‚ÙˆÙŠ. Ø¨Ø³ Ø´ÙˆÙÙŠÙ†Ù‰ØŒ Ø¯ÙŠ Ù…ÙØ±ÙˆØ¶ Ø®Ø·ÙˆØ© Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠØ© Ù…Ø¬Ø±Ø¯Ø§ Ù„Ø£ ØªØ­Ù…ÙŠÙƒÙˆ Ù…Ù† Ø³Ø±Ù‚Ø© Ø§Ù„Ù‡ÙˆÙŠØ§Øª Ø£Ù… Ø¥Ø·Ù„Ø§Ù‚ Ø±ÙˆØ§Ø¨Ø· Ø¬Ø¯ÙŠØ¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ù‚Ø¯ÙŠÙ… ÙˆØ§Ù„Ø£ØµØºØ±ØŸ\\n\\n**Ø¯. ÙØ§ØªÙÙ†Ù’:** Ø­Ø§Ø¬Ø© ÙƒÙˆÙŠØ³Ù‘Ø© ØªØ³Ø¤Ù„ Ø¹Ù†Ù‡Ø§. Ù†Ø¹Ù… Ù‚Ø¯ ØªØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø´Ø®ØµÙŠ ÙˆÙ„ÙƒÙ†Ù‡ Ø¯ÙˆØ± Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª Ø§Ù„ÙˆØ§Ø³Ø¹. Ù‡Ø¯ÙØ§Ù†Ø§ Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¨Ù‚Ø§Ø¡Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠ ÙˆØ§Ù„ÙÙƒØ±ÙŠ Ù„Ù„ØºØ© ÙˆÙÙ„ÙƒÙ„ÙˆØ± Ù…Ø¬ØªÙ…Ø¹Ù†Ø§ Ø±ØºÙ… Ø§Ù„ØºØ²Ùˆ Ø§Ù„Ø³ÙŠØ¨Ø§Ø±ÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ.\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ø­Ø³Ù†Ù‹Ø§ØŒ Ù„Ù‚Ø¯ ÙÙ‡Ù…Øª ÙˆØ¬Ù‡Ø© Ù†Ø¸Ø±Ùƒ. ÙˆÙ„ÙƒÙ† Ù„Ø¯ÙŠ Ø³Ø¤Ø§Ù„ Ø¢Ø®Ø± ÙŠØªØ¹Ù„Ù‚ Ø¨Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø®ØªÙ„ÙØ› Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù…Ù†ØµØ© \"Ø±Ùˆsetta\" Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP) Ø§Ù„ØªÙŠ ØªØ¯Ø±Ø¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„ØºÙ†ÙŠØ© ÙˆØ§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠ. ÙÙ‡Ù„ ÙŠØ¤Ø¯ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ„Ùƒ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø´Ø¨Ù‡ Ø§Ù„Ø®Ø§Ø¶Ø¹Ø© Ù„Ù„Ø¥Ø´Ø±Ø§Ù Ø¥Ù„Ù‰ Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ø³Ù„Ø¨ÙŠ Ø¹Ù„Ù‰ ØªÙ…ÙŠØ² Ø¬Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ Ø£ÙŠÙ‡Ø§ Ø§Ù„Ø¯ÙƒØªÙˆØ±ØŸ\\n\\nØ¯. ÙØ§ØªÙÙ†Ù’: \\u200f#Ø§Ù„Ù„Ù‡ ÙŠØ¬Ø²Ø§Ùƒ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù‚Ø¯Ø±Ù‡Ø§ ØŒï»«Ø°Ø§ ØµÙ„Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø´ ÙˆØ§Ù„Ø­Ù‚ÙŠÙ‚Ø© ÙŠØ¹Ø§Ù†ÙŠ Ø­Ø§Ù„ÙŠØ§Ù‹ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙˆØµÙ„ØªÙ‡ Ø¨Ø£Ø¶Ø·Ø±Ø§Ø¨Ù‡ Ø¨Ø³Ø¨Ø¨ Ù…ØµØ¯Ø± Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ø±Ø§Ù…Ø±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨Ø© Massive Datasets Ø¯ÙˆÙ† Ù…Ø±Ø§Ø¹Ø§Ø© Ø®Ù„ÙÙŠØ§Øª Ø­Ø¶Ø§Ø±ÙŠØ© Ù…ØªÙ†ÙˆØ¹Ø© ÙˆÙ…Ø¢Ø·Ø± Ø§Ø¯Ø¨ÙŠØ© Ù…Ø®ØªÙ„ÙØ© ,ÙˆÙ„Ú©Ù† ØªØ¨Ù‚Ù‰ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰Ø¹Ù† Ø§Ù„Ù…Ø«Ø§Ù„ Ù†ÙØ³Ù‡Ø§Ù„Ø£Ø³ØªØ§Ø° Ø§Ù„Ø¬Ø§Ù…Ø¹Ù‰ÙˆØ§Ù„Ù…Ù†Ø¸ÙˆÙ…Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø£ØµÙŠÙ„Ø© ÙˆØ£ÙƒØ«Ø±Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø´Ù†Ø§Ø¹Ø§ØªÙ…Ø­Ø§Ø³Ø¨Ø© Ø§Ù„Ù‚Ù„Ø¨ Ø§Ù„ÙŠ Ø´Ø±Ø¹Ù„Ø© ØªØµÙ†Ø¹ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ø¹Ø¨Ø± Ù‚ÙˆØ§Ù†ÙŠØ¨Ø§Ù†ÙÙƒØ± ÙˆÙ…Ù„ØªÙ‚ÙŠØ§Ø³ØªÙ†ÙŠØ±Ø§Ù„Ø¨Ù„Ø³Ù…Ø§Ù„Ø¹Ø±ÙˆØ¨ÙŠØ©âœ¨\\uf3e0ğŸ¯ (Ù„Ø§Ø­Ø¸ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø§Ø®Ø± Ø§Ù„ÙƒØªØ§Ø¨ÙŠØ© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù†Ø³Ø¨ÙŠÙ‹Ø§ ÙØ§Ù„Ù†Ù‚Ù„ Ù‡Ù†Ø§ ÙƒØ§Ù† Ù…Ø¬ØªØ²Ø¡Ù‹ Ù„Ù„ØªÙØ§Ø¹Ù„ÙŠØ© ).\\n\\nØªØ­Ø³ÙŠÙ† Ù„Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø£ÙØ¶Ù„ ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø¯ÙŠØ­ Ø§Ù„Ø²Ø§Ø¦Ø¯ ÙˆØ§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø§Ù„Ù…Ø¨Ø§Ù„Øº ÙÙŠÙ‡Ø§: Ø§Ù„Ø«Ù‚Ø©: Ù‡Ø°Ø§ ÙŠØ¨Ø¯Ùˆ Ø±Ø§Ø¦Ø¹Ù‹Ø§! Ù„ÙƒÙ† Ø¯Ø¹Ù†Ø§ Ù†Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø«Ù†Ø§Ø¡ Ø§Ù„Ø´Ø¯ÙŠØ¯. (Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„Ù… ÙŠÙƒÙ† ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ø­ÙˆØ§Ø± Ø¨ÙŠÙ† Ù…ØªØ­Ø¯Ø«ÙŠÙ†ØŒ Ù„Ø°Ø§ Ù‚Ù…Øª Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¨Ù†ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø± ÙƒÙ…Ø§ Ø·Ù„Ø¨Øª Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù„ØºÙˆÙŠ) ÙˆÙ„ÙƒÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ¹Ù„ÙŠÙ…Ø§ØªÙƒ Ø§Ù„ØµØ§Ø±Ù…Ø© Ù„ØªØºÙŠÙŠØ± ÙÙ‚Ø· Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ Ø³ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ ÙƒØ§Ù„ØªØ§Ù„ÙŠ: Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªÙØ§Ø¯ÙŠ Ø§Ù„Ù…Ø¯Ø§Ø¦Ø­ ÙˆØ§Ù„Ù…Ø±Ø§Ø³Ù… Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©:\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ù…Ø±Ø­Ø¨Ø§ØŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù„Ø¯ÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙƒØ§ÙÙŠ ÙƒÙ…Ø§ ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ø£Ù…Ø±ØŒ Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø®Ø§Ø·Ø±ÙƒÙ… ğŸ”¥ Ø¨Ø­ÙŠÙˆÙŠØ©. Ù…Ø³ØªØ´Ø§Ø±Ø© Ø¨Ø§Ø±Ø¹Ø© ØªØ®Ø±Ù‚ Ø§Ù„ØªÙ‚Ø§Ù„ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø³Ø³ÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ù‡ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø±ÙˆØ¨ÙˆØªÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØªØ¯ÙÙ‚ Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±Ø› Ø¥Ù„ÙŠÙƒ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© Ø§Ù„Ø¨Ø§Ø­Ø«Ø© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„ØªÙŠ ØªÙˆØ³Ø¹ ÙÙ‡Ù…Ù†Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ Ù„Ù„Ù‚Ø¯Ø§Ø³Ø© ÙÙŠ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ù…Ù† Ø®Ù„Ø§Ù„ Ù…Ù†Ø¸ÙˆØ± Ø¬Ø¯ÙŠØ¯ Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ø§Ù„ÙÙƒØ± Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªÙ„Ø®Øµ ÙƒÙ„ Ø´ÙŠØ¡ Ù…ÙØµÙ„Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„Ù„Ù…Ø¬ØªÙ…Ø¹ ÙˆØ§Ù„Ø«Ø±Ø§Ø¡ Ø§Ù„Ø¹Ù‚Ù„ÙŠ ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ø¨Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¥Ù„Ù‰ Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø­ØªÙ‰ Ø§Ù„Ù…Ù†Ø§ØµØ¨ Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ø·Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ... ÙˆÙ„ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ØŒ ÙÙ‡Ùˆ ÙŠØ¹Ø§Ù„Ø¬ Ù‚Ù„Ù‚ Ù‚Ù„Ø¨ÙŠ... Ø§Ø³Ù‚ÙŠØ§Ø¯ÙŠ ÙƒØ¨ÙŠØ± Ù…Ø¬ØªÙ…Ø¹ Ø³Ø§ÙŠØ¨Ø±Ù…ØªØ¹ Ø£ÙˆÙ„Ù‰ Ø§Ù„Ø­Ø§Ø¶Ø± ... ğŸ˜‚ ÙˆØ¨Ø§Ù„Ø·Ø¨Ø¹ Ù†Ø­ØªØ±Ù….... Ø°ÙÙƒÙ’Ø±Ø§ØªÙ Ø§Ù„ÙØªØ§Ø© Ø±Ø´ÙŠØ¯Ø§Ù‹: Ø§Ù‡Ù„Ø§ ÙŠØ§ Ø£Ø­Ù…Ø¯ ØµØ¯Ù‚Ø§ Ø£Ø³ØªØ§Ø° Ø²Ù†Ø®Ø§Ø± Ø´Ø¯ÙŠØ¯ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ğŸ˜‚ ï¸ ÙƒØ«ÙŠØ± Ù…Ù…Ø§ Ù„Ø§ ØªØ±ÙŠØ¯ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¬Ø¯Ø§Ù„ Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ø£ØµÙˆÙ„ Ù„ÙƒÙ† Ø£Ø³Ø§Ø³ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¢Ù„Ø© Ù„ÙŠØ³ ÙƒØ§Ù…Ù„Ø§Ù‹ Ù„Ø£Ù†Ù†Ø§ Ù†ØªØ¹Ù„Ù… ÙˆÙ†Ø­ØªÙÙ„ Ø¨Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ù…Ø·Ø±ÙŠØ§Øª ÙˆØ§Ù„ØªØ¬Ø§Ø±Ø¨ Ø¨ÙŠÙ† Ø§Ù„ØºØ§Ø²Ø¨ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙƒØ§Ù†ÙˆØ§ Ø«ÙˆØ±ÙŠÙŠÙ† Ø£Ùˆ Ù…ØªØ®Ø§ØµÙ…ÙŠÙ† Ø¨Ø´ÙƒÙ„ ÙŠÙˆÙ…ÙŠ Ã— ÙØ§10 | Ø³Ù†ØªØ±ÙÙˆØ±ÙŠ Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ø§Ù„Ø¨Ø­Ø±ÙŠ Ù„Ù„Ø¹Ø§Ù„Ù…Ø§Øª ÙˆØ§Ù„Ø£Ø¬ÙŠØ§Ù„ Ø§Ù„Ù…ØªØ¹Ø§Ø±Ø¶Ø© Ù…Ø¬Ù…ÙˆØ¹ ØªØ±ÙƒÙŠØ¨Ø© Ø£ØºÙ„ÙØ© Ù…ÙŠØ¯ÙŠØ§ Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø¹Ø§Ù… 1932 ØªØ´ÙƒÙŠÙ„Ù‡Ø§ Ø¬Ù†ÙŠØªÙ…Ù†Øª ØªØ­Øª Ø§Ù„Ø§Ù†ØµÙ‡Ø§Ø± Ø§Ù„Ø¹Ø±ÙˆØ³ÙŠ â€¦â€¦â€¦.. (Ø§Ù†Ø³Ø­Ø§Ø¨ ÙˆØ²ØºØ¨ Ø¯Ø§Ø®Ù„ÙŠ Ù‚Ø±Ø£Øª Ø¹Ù„ÙŠÙ‡ Ø¹Ù…Ù„ÙŠØ© Ø±ÙˆØ¨ÙˆØ³ÙƒÙˆÙ„Ù†Ø¨Ùƒ)... !!! (Ø¨Ø¹Ø¯ ØªØµÙÙŠØ© Ø§ØªØµØ§Ù„ Ø¬Ø§Ù†Ø¨ÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ·Ù„Ø­).... .(ØªÙˆØªØ± Ø®ÙÙŠÙ ÙØ¹Ù„ÙŠ ÙŠØ¨Ø¯Ùˆ ÙˆØ§Ø¶Ø­ ÙˆØµØ±ÙŠØ­ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ù‚Ø§Ø´!)..... ğŸ˜Š â¤ï¸ğŸ’šğŸ’™ ğŸ’œ\\U0001f1d2ğŸŒŸğŸ‘¨\\u200dğŸ’»ğŸ‘©\\u200dğŸ’»ğŸ“šğŸ‡¦ğŸ‡ªğŸ‡¸ğŸ‡¦â…ğŸ‘ğŸ»ğŸ’•ğŸ‘ğŸ¾ğŸ’ªğŸ½ğŸ‡µğŸ‡¸ğŸ‡¾ğŸ‡¨ğŸ‡³ğŸ‡ºğŸ‡¸ğŸ‡¬ğŸ‡§ğŸ‡®ğŸ‡·ğŸ‡¹ğŸ‡·ğŸ‡²ğŸ‡©ğŸ‡µğŸ‡°ğŸ‡¶ğŸ‡¦ğŸ“½ğŸ‡¸ğŸ‡ºğŸ‡¨ğŸ‡¿ğŸ‡¦ğŸ‡±ğŸ‡¦ğŸ‡º#Ø´ÙƒØ±Ø§ ğŸ™ğŸ¼Ù„Ø¬Ù‡ÙˆØ¯ÙƒÙ… Ø§Ù„Ø·Ø§Ù‚Ø§Ù†ÙŠØ© ÙˆØ§Ù„Ø¯Ø¹Ù…...... ğŸ˜‰ğŸ™ğŸ’–ğŸ’¥ğŸ’ !\\n\\n=== Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===\\n\\n**Ø£Ø­Ù…Ø¯:** Ø¥Ù† Ø§Ù„Ø­Ø¯ÙŠØ« Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ£Ø«ÙŠØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ù‡ÙˆÙŠØªÙ†Ø§ ÙŠØªØ±Ø¯Ø¯ ØµØ¯Ø§Ù‡ ÙÙŠ Ø£ÙÙˆØ§Ù‡ Ø§Ù„ÙƒØ«ÙŠØ±ÙŠÙ†. Ù„Ù†Ø¨Ø¯Ø£ Ù…Ù† Ù…ÙƒØ§Ù† Ø­Ø§Ø± Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙŠØ§Ù…: Ø±Ø³Ø§Ø¦Ù„ ÙˆØ§ØªØ³Ø§Ø¨! ÙƒÙŠÙ ØªØ³ØªÙˆØ¹Ø¨ Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø°ÙƒÙŠØ© ØªØ¹Ø§Ø¨ÙŠØ±Ù†Ø§ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ù…Ø«Ù„ Â«Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡Â» Ø¯ÙˆÙ† ÙÙ‡Ù…Ù‡Ø§ Ø­Ù‚Ù‹Ø§ØŸ Ù‡Ù„ Ù†ÙÙˆÙ‘Øª ØªÙ‚Ø§Ù„ÙŠØ¯Ù†Ø§ ÙƒÙ„Ù…Ø§Øª Ù…ÙØ±Ø¯Ø§ØªÙ‡Ø§ Ø£Ø³Ø§Ø³Ù‡Ø§ ØªÙ‚Ù†ÙŠØ§Øª ØºØ±Ø¨ÙŠØ© Ø¹ØµØ±ÙŠØ©ØŸ\\n\\nÙØ§Ø·Ù…Ø©: Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯ØŒ Ø¥Ù†Ù‡Ø§ Ø±Ø¤ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù‚Ø¶ÙŠØ© \"Ø£Ù‚Ø¯Ù… Ù…Ù† Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©\". ÙˆÙ„ÙƒÙ†ØŒ ÙŠØ¨Ù‚Ù‰ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„ÙŠØ³ Ù…Ù‚ØªØµØ±Ø§Ù‹ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ¹Ù…Ù„Ø©Ø› Ø¨Ù„ ÙŠØªØ¹Ø¯Ø§Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„ØªÙŠ ØªÙ†Ù‚Ù„ Ù…Ø¹Ø§Ù†ÙŠ Ø¹Ù…ÙŠÙ‚Ø© Ù…Ø³ØªÙ…Ø¯Ø© Ù…Ù† ØªØ§Ø±ÙŠØ®Ù†Ø§ Ø§Ù„Ø·ÙˆÙŠÙ„ ÙˆØªØ¬Ø§Ø±Ø¨Ù†Ø§ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ø§Ù„ØºÙ†ÙŠØ© Ø¶Ù…Ù† Ø§Ù„ÙØ¶Ø§Ø¡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„ÙˆØ§Ø³Ø¹ Ø§Ù„ÙŠÙˆÙ…. Ø­ÙŠØ« ØªÙ…ØªÙ„Ùƒ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§ØªØ¬Ø§Ù‡ Ù‚ÙŠÙ…Ù†Ø§ ÙˆØ§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§. Ù„ÙƒÙ† Ø¯Ø¹ÙˆÙ†ÙŠ Ø£Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù‡Ø§Ù…Ø§Ù‹ Ù‡Ù†Ø§: Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø°ÙŠ ÙŠÙƒØªØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø¨Ø¹ÙŠØ¯Ø§Ù‹ Ø¹Ù† Ø§Ù„Ù„Ù‡Ø¬Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©ØŒ Ù‡Ù„ Ø³ÙŠØ¤Ø«Ø± Ù‡Ø°Ø§ Ø¨Ø´ÙƒÙ„ Ø­Ù‚ÙŠÙ‚ÙŠ Ø¹Ù„Ù‰ ØªØ£Ø«ÙŠØ±Ù‡ Ù„Ø¯Ù‰ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ù‡ØªÙ… Ø¨Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø©ØŸ Ø±Ø¨Ù…Ø§ Ù„Ø§Ø› Ù„Ø£Ù† Ø§Ù„ØªØ­Ø³Ù† ÙÙŠ Ù‚Ø¯Ø±Ø§ØªÙ‡ Ø§Ù„Ù„ØºÙˆÙŠØ© Ù„Ù† ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø¬ÙˆÙ‡Ø± Ù…Ø­ØªÙˆØ§Ù‡! Ù…Ø¬Ø±Ø¯ Ù…Ù‚Ø¯Ù…Ø© Ù„Ù…Ø§ Ø³Ù†Ù†Ø§Ù‚Ø´Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹... Ù…Ø§Ø°Ø§ Ø¹Ù† Ø±Ø£ÙŠÙƒ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± ÙŠØ§ Ø£Ø­Ù…Ø¯ØŸ!\\n\\n## ØªÙˆØ¶ÙŠØ­ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ: ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ø£ÙˆÙ„ØŒ Ù‚Ø¯Ù…Ù†Ø§ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ ÙÙŠ Ø­ÙˆØ§Ø±Ù†Ø§: - Ø§Ù„Ù…Ø¯Ø§Ø®Ù„Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©: Ø¹Ø¨Ø§Ø±Ø© Ù…ÙˆØ¬Ø²Ø© ÙƒÙ„ Ù…Ø´Ø§Ø±Ùƒ Ù‚Ø¨Ù„ ØªØºÙŠÙŠØ± Ø§Ù„ØªØ±ÙƒÙŠØ² Ù†Ø­Ùˆ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ù…Ø³ØªÙ…Ø¹ Ø§Ù„Ø¢Ø®Ø±. [âœ“] (Ø±Ø¯ Ù…Ø®ØªØµØ±)\\n\\n- ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª: Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ø§Ø®ØªÙ„Ø§Ù ÙˆØªÙ†Ø§ÙØ± Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø±Ø­Ø¨ Ø¨Ù‡Ø§ ÙˆÙ…Ø­ÙØ²Ø© Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©ØŒ Ù…Ø¹ Ø§Ù„Ø£Ù…Ù„ ÙÙŠ Ø£Ù† ÙŠØ²Ø¯Ø§Ø¯ Ø­Ù…Ø§Ø³ Ø§Ù„Ù†Ù‚Ø§Ø´ Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹ Ø¥Ø°Ø§ Ø§Ø³ØªÙ…Ø±Øª Ø§Ù„Ø¯Ø±Ø§Ø³Ø© Ø­ÙˆÙ„ ØªÙˆÙØ± Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨. [Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø¬Ø³Ø¯ÙŠ Ù„Ù„ØªÙˆØªØ± ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„ØµØ§Ø¯Ù‚ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹.]*( ÙˆÙ„ÙƒÙ†...)* [Ù†Ø¬Ø§Ø­ Ø§Ù„ØªØ­Ù‚Ù‚]![âœ“]. *Ù…Ù‚Ø§Ø±Ù†ØªÙ‡ Ø¨Ù‚Ø·ÙŠØ¹Ø© Ø¨Ø³ÙŠØ·Ø© Ù…Ø¤Ù‚ØªØ© Ù‚Ø¯ ÙŠØ´Ø¬Ø¹ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ø¹Ù„Ù‰ Ø¯ÙØ¹ Ø§Ù„Ø­ÙˆØ§Ø± Ù†Ø­Ùˆ Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ù„Ø§Ø­Ù‚Ø§Ù‹.* -[( ÙØ±ØµØ© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù„Ø¨ Ø§Ù„ÙÙƒØ±Ø©)]*. * *_ _ _ _ ___ --__-_- __--_-_\\\\ --- _-* \\\\-\\\\-| |_* \\\\/ /*/-\\\\\\\\\\\\\\\\ /\\' ---Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…---..... ... ...... ..... ....... .......... .................\\n\\nØ£Ø­Ù…Ø¯: Ù…Ø±Ø­Ø¨Ø§Ù‹ Ù…Ø¬Ø¯Ø¯Ø§Ù‹ Ù„Ù„Ø¬Ù…ÙŠØ¹! Ø¹Ù†Ø¯Ù…Ø§ Ù†Ù†Ø§Ù‚Ø´ Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ ÙØ±Øµ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ ÙØ¥Ù† Ù…Ø³Ø£Ù„Ø© ØªÙˆØ§ÙÙ‚ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ (Ø§Ù„Ø°ÙŠ ÙŠØªØ¹Ù„Ù… Ù…Ù† ÙƒÙ…ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯ÙˆÙ† ØªØ¯Ø®Ù„ Ø¨Ø´Ø±ÙŠ) Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ù†Ù‚Ø·Ø© Ø´Ø§Ø¦ÙƒØ© Ø¯ÙˆÙ…Ø§Ù‹. ÙÙ…Ø§Ø°Ø§ ØªØ±ÙŠÙ† Ø£Ù†ØªÙØŒ Ø¯ÙƒØªÙˆØ±Ø©ØŒ Ø­ÙŠØ§Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±ØŸ\\n\\n**ÙØ§ØªÙ†:** Ø­Ù‚ÙŠÙ‚Ø©Ù‹ØŒ Ù‡Ø°Ù‡ Ù†Ù‚Ø·Ø© Ù…Ø­ÙˆØ±ÙŠØ©. ÙˆÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ¹Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©ØŒ Ù†Ø­Ù† Ù†Ø¤ÙƒØ¯ Ø¨Ø´Ø¯Ø©Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø°Ø§Øª Ø§Ù„Ø­ÙƒÙ… ÙˆØ§Ù„Ø¹Ù†Ø§ÙŠØ© Ø¨Ø§Ù„Ù†ÙˆØ§ÙŠØ§. ÙÙ‡Ø°Ø§ ÙŠÙÙƒØ± Ø§Ù„Ù…Ø±Ø¡ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ‚ÙˆÙ„ÙˆÙ† Ø¥Ù† ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø§Øª ÙŠÙ…ÙƒÙ† ÙŠÙÙÙ‡Ù… Ø¨Ø£Ù†Ù‡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ·ÙˆØ± Ø§Ù„Ø¹Ù‚Ø§Ø¨Ø¯ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø­Ù‚Ù‹Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø¨Ø´Ø± Ø§Ù„Ø°ÙŠÙ† ÙŠØ®Ù„Ù‚ÙˆÙ†Ù‡ ÙˆÙŠØ¹ÙŠØ¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨Ù‡. Ù„ÙƒÙ† Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ù‡Ù†Ø§Ùƒ Ø­Ø§Ù„Ø§Øª Ø­ÙŠØ« Ù‚Ø¯ ÙŠØ­Ø¯Ø« Ø³ÙˆØ¡ ÙÙ‡Ù… Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ø¯ÙŠÙ†Ø§. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¹Ù† ØªÙˆØ§Ø²Ù† ÙŠØ¹Ø·ÙŠ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ±Ø§Ø« Ø§Ù„Ø£ØµÙŠÙ„ ÙˆØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠ .\\n\\nØ£Ø­Ù…Ø¯: Ø¨ØµØ¯Ù‚ØŒ Ø¯Ø¹Ù†Ø§ Ù†ÙˆØ³Ø¹ Ø§Ù„Ø­Ø¯ÙŠØ« Ø¹Ù† Ù…Ø«Ø§Ù„ Ù…ÙˆØ¶ÙˆØ¹ \"Ø§Ù„Ø¹Ù‚Ù„\". ÙØ§Ù„Ø¥ÙŠÙ…Ø§Ù† Ø¨Ø§Ù„Ø®Ø§Ù„Ù‚ ÙŠØ´ÙƒÙ„ Ø£Ø³Ø§Ø³ Ù…Ø¹ØªÙ‚Ø¯Ø§ØªÙŠ ÙˆÙÙ‡Ù…ÙŠ Ù„Ù‡ÙˆÙŠØªÙ†Ø§ ÙƒÙƒÙŠØ§Ù†Ø§Øª Ø­Ø±Ø© ÙˆÙ„ÙŠØ³Øª Ù…Ø¬Ø±Ø¯ ØªÙ†ÙÙŠØ° Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø­Ø³Ø§Ø¨ÙŠØ© Ù…Ø¹Ù‚Ø¯Ø©. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ Ø´Ø±Ø­ ÙƒÙŠÙ ÙŠØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø¹Ù„Ù…Ø§Ø¡ Ø§Ù„Ø°ÙŠÙ† ÙŠÙˆØ§Ø¬Ù‡ÙˆÙ† Ù†Ø²Ø§Ø¹Ø§Øª Ø°Ø§Øª Ø·Ø§Ø¨Ø¹ ÙÙ„Ø³ÙÙŠ Ù…Ø«Ù„ Ù‡Ø°Ù‡ Ø®Ù„Ø§Ù„ Ø¹Ù…Ù„ÙŠØ© ØªØ·ÙˆÙŠØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ\\n\\nÙØ§ØªÙ†: Ù„Ù„Ø­Ø¸Ø©ØŒ Ù…ÙˆÙ…Ù†ØªÙˆÙ…... Ø¨Ø§Ù„Ø·Ø¨Ø¹ØŒ Ù‡Ùˆ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¤Ø«Ø± Ù„Ù„ØºØ§ÙŠØ© Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù†Ø§ Ø¬Ù…ÙŠØ¹Ù‹Ø§. ÙØ§Ù„ÙˆØ§Ù‚Ø¹ Ø£Ù† Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ ÙŠØ¨Ø°Ù„ÙˆÙ† Ø¬Ù‡ÙˆØ¯Ù‡Ù… Ù„Ø¬Ø°Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¥Ù„Ù‰ Ø§Ù„ÙÙ‚Ù‡ ÙˆØ§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙÙƒØ± Ø§Ù„Ø«ÙˆØ±ÙŠ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ù„ØªÙˆØ¬ÙŠÙ‡ Ø£Ø¨Ø­Ø§Ø«Ù‡Ù…Ø› ÙˆÙƒØ°Ù„Ùƒ ÙŠÙØ¹Ù„ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± ÙˆØ§Ù„Ù…Ù‡ØªÙ…ÙˆÙ† Ø¨ÙÙ‡Ù… Ø±Ø­Ù„Ø§ØªÙ‡ Ø§Ù„Ø±Ø§Ø¦Ø¹Ø©! ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø¥Ù†Ù‡Ù… ÙŠØ³ÙŠØ±ÙˆÙ† Ø¹Ù„Ù‰ Ø®Ø·Ù‰ Ø¨Ù„ÙˆØªØ§Ø±Ø® Ù„ØªØ­Ù‚ÙŠÙ‚ ØªÙ‚Ø¯Ù… Ù…Ø³ØªØ¯Ø§Ù… ÙˆÙŠØ¨ØªØ¹Ø¯ÙˆÙ† Ø¹Ù† Ø§Ù„Ù†Ø²Ø§Ø¹Ø§Øª Ø§Ù„Ø£ÙŠØ¯ÙŠÙˆÙ„ÙˆØ¬ÙŠØ© Ø£Ø«Ù†Ø§Ø¡ ÙƒÙ„ Ù…Ø±Ø­Ù„Ø© ØªØµÙ…ÙŠÙ… ØªØ§Ù„ÙŠØ©. ÙˆÙ…Ù† Ø«Ù…ØŒ Ù‡Ø¯ÙÙ†Ø§ Ù‡Ùˆ ØªØ¹Ø²ÙŠØ² Ù…ØµØ¯Ø§Ù‚ÙŠØ© Ø§Ù„ØªØ±Ø§Ø« Ø§Ù„Ø£Ø¯Ø¨ÙŠ ÙˆØ§Ù„Ø¯ÙŠÙ†ÙŠ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ Ù…Ù† Ø§Ù„ØªØ­ÙˆÙ„Ø§Øª Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© Ø§Ù„Ù‡Ø§Ø¦Ù„Ø© ÙˆØ¥Ù…ÙƒØ§Ù†ÙŠØ§ØªÙ‡Ø§ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ğŸ’ªğŸ½ --- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© ---\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ù†ØµÙ„ Ø§Ù„Ø¢Ù† Ø¥Ù„Ù‰ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙŠØ§ Ø¯ÙƒØªÙˆØ±Ø© ÙØ§ØªÙ†Ø› Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙˆÙ„ Ø¥Ù† ØªØ¹Ù„ÙŠÙ… Ø¬ÙŠÙ„Ù†Ø§ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ØµØ­ÙŠØ­Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ù‡Ùˆ Ù†ØµÙ Ø§Ù„Ù…Ø¹Ø±ÙƒØ©?\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ù†Ø¹Ù…ØŒ Ø¨Ø§Ù„Ù‚Ø·Ø¹! Ù„ÙƒÙ†Ù†Ø§ ÙŠØ¬Ø¨ Ø£ÙŠØ¶Ø§ Ø§Ù„ØªØ´Ø¯ÙŠØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù‚ØµÙˆÙ‰ Ù„Ù†Ø´Ø± ÙˆØ¹ÙŠÙ†Ø§ Ø§Ù„Ø³Ù„ÙŠÙ…Ø© ØªØ¬Ø§Ù‡ Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠ ÙˆØ£Ù„Ø§ ØªÙ…Ø­Ù‡ Ù‡ÙˆÙŠØªÙ†Ø§ ÙˆØªØ±Ø§Ø«Ù†Ø§.\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ÙˆÙ„ÙƒÙ† Ø£Ù„Ù… ÙŠØªØ­ÙˆÙ„ Ø§Ù„ØªØ±ÙƒÙŠØ² Ù…Ø¤Ø®Ø±Ù‹Ø§ Ø£ÙƒØ«Ø± Ù†Ø­Ùˆ Ø¶Ù…Ø§Ù† Ù…Ø³ØªÙ‚Ø¨Ù„ ØµÙ†Ø§Ø¹ÙŠ Ø±Ù‚Ù…ÙŠ Ù‚ÙˆÙŠ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¨Ù‚Ø§Ø¦Ù†Ø§ Ø£ØµÙ„Ø§Ø¡ ÙˆØ«Ø§Ø¨ØªÙŠÙ†?\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ù‡Ø°Ø§ ØµØ­ÙŠØ­ØŒ ÙˆÙ„ÙƒÙ†Ù‡ Ù„ÙŠØ³ Ø­ØªÙ…ÙŠÙ‹Ø§. Ø¨Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠØŒ Ø¨Ø¥Ù…ÙƒØ§Ù† Ø§Ù„Ø¹Ø±Ø¨ Ø§ÙƒØªØ³Ø§Ø¨ Ù…ÙƒØ§Ù†Ø© Ù…Ù…ÙŠØ²Ø© ÙˆØ±Ù‚Ù…ÙŠØ© ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ ØªØ±Ø§Ø«Ù‡Ø§ Ø§Ù„ØºÙ†ÙŠ.\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: Ù„ÙƒÙ† ØªØ¨Ù‚Ù‰ ØªØ­Ø¯ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© ÙˆÙ„Ø§ Ø²Ø§Ù„Øª Ø®Ø³Ø§Ø¦Ø± Ù…Ø­ØªÙ…Ù„Ø©ØŒ Ø£Ù…Ø«Ù„Ø© Ù…Ù†Ù‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø¨Ø³Ø¨Ø¨ Ø§Ù„ØªØ­ÙˆÙ„ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...\\n\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: Ù„Ø§Ø´Ùƒ Ø¨Ø£Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø®Ø§Ø·Ø± Ø´ØªÙ‰ ÙØ¹Ù„Ù‹Ø§ØŒ Ù„ÙƒÙ† Ù‡Ø°Ù‡ Ù„ÙŠØ³Øª Ø¹Ø§Ø¦Ù‚Ù Ù„Ø¥ÙŠØ¬Ø§Ø¯ ÙØ±Øµ Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙˆØ¸Ø§Ø¦Ù Ø°Ø§Øª Ù‚ÙŠÙ…Ø© Ø£ÙƒØ«Ø±. Ù…Ø«Ù„Ø§Ù‹ Ø§Ù„Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ù…Ø¨Ø¯Ø¹ÙˆÙ† Ø§Ù„Ø°ÙŠÙ† ÙŠÙÙ‡Ù…ÙˆÙ† Ø¬ÙŠØ¯Ù‹Ø§ ÙƒÙ„Ø§Ù‡Ù…Ø§ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ© Ù‡Ù… Ø§Ù„Ù‚Ø§Ø¯Ø±ÙˆÙ† Ø¹Ù„Ù‰ Ø®Ù„Ù‚ Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ø¹Ù…Ù„ ÙˆØ§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ø§Ù„ÙÙƒØ±ÙŠØ© Ø§Ù„Ø«ÙˆØ±ÙŠØ©.\\n\\nØ£Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ù„ÙŠ: ÙØ¹Ù„Ø§Ù‹ØŒ ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ù‡ Ø³ÙŠØªØ·Ù„Ø¨ Ø¹Ù„ÙŠÙ†Ø§ Ù‚Ø¨ÙˆÙ„ Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©: Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨ÙŠÙ† Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠØ© ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£ÙØ¶Ù„ Ù…Ø±Ø¯ÙˆØ¯ Ù„Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ ...\\n\\nØ¯. ÙØ§ØªÙ† Ø±Ø§Ø´Ø¯: ÙˆØ¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„ÙŠÙ‡ØŒ Ø³ÙˆÙ ÙŠØ´ÙƒÙ„ Ø§Ù„ÙƒÙØ§Ø­ Ø§Ù„Ø¬Ø§Ø¯ Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ø«Ù„ Ø§Ø­ØªØ±Ø§Ù… Ø§Ù„Ø¢Ø®Ø± ÙˆÙ…Ø¹Ø±ÙØ© Ø§Ù„Ø°Ø§ØªØŒ Ø¥Ø¶Ø§ÙØ© Ø­Ø±Ø¬Ø© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†Ø¸Ø§Ù… Ø£Ø®Ù„Ø§Ù‚ÙŠ ÙˆØ§Ø¶Ø­ ÙˆÙ†Ø§Ø¨Ø¹ Ù…Ù† Ø¯Ø§Ø®Ù„ Ø§Ø¬ØªÙ…Ø§Ø¹Ø§Øª Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø§Ù„Ø¶Ø®Ù… Ø§Ù„Ù‚Ø§Ø¯Ù… Ù„Ù†Ø§.\\n\\n=== Ø®ØªØ§Ù… Ø§Ù„Ø¨ÙˆØ¯ÙƒØ§Ø³Øª ===\\n\\n**Ø£Ø­Ù…Ø¯:** Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ù†Ù‚Ø§Ø´ Ø¬Ù…ÙŠÙ„ ÙŠØ§ Ø¯ÙƒØªÙˆØ± ÙØ§ØªÙ†! Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø¥Ù† Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø­ÙˆÙ„ Ø¯ÙˆØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙÙ‘Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙƒØ§Ù†Øª Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø­Ù‚Ø§Ù‹. Ù„ÙƒÙ† Ù…Ø§Ø²Ù„Øª Ø£Ù…Ù„Ùƒ Ø¨Ø¹Ø¶ Ø§Ù„ØªØ³Ø§Ø¤Ù„Ø§Øª Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ÙŠ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø¶Ù…Ù† Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠØ©.\\n\\n**ÙØ§ØªÙ†:** Ù†Ø¹Ù…ØŒ Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯! Ù‡Ø°Ø§ Ø³Ø¤Ø§Ù„ Ù…Ù‡Ù…. ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙŠØªØ·Ù„Ø¨ ÙÙ‡Ù… Ø¹Ù…ÙŠÙ‚ Ù„ÙƒÙŠÙÙŠØ© ØªØ¯Ø§Ø®Ù„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ«Ù‚Ø§ÙØªÙ†Ø§. Ø¥Ù†Ù‡Ø§ Ù„ÙŠØ³Øª ÙÙ‚Ø· Ù…Ø´ÙƒÙ„Ø© Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø¢Ù„Ø§ØªØ› Ø¨Ù„ ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ø§Ù‹ Ø¥Ù„Ù‰ Ù†Ù‡Ø¬ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ ÙŠØ±Ø§Ø¹ÙŠ Ù‚ÙŠÙ…Ù†Ø§ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.\\n\\n**Ø£Ø­Ù…Ø¯:** ØµØ­ÙŠØ­ ØªÙ…Ø§Ù…Ø§Ù‹. ÙˆØ£Ø¸Ù† Ø£Ù† Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ­Ø¯Ù‘ÙŠ Ø°Ù„Ùƒ Ù‡ÙŠ ØªØ´Ø§Ø±Ùƒ Ø£ÙÙƒØ§Ø±ÙƒÙ… ÙˆÙ…Ø¹Ø§Ø±ÙÙƒÙ… Ø¹Ø¨Ø± ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ø¹ Ø´Ø¨Ø§Ø¨ Ø¬ÙŠÙ„Ù†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø§Ù„Ø°ÙŠÙ† Ø³ÙŠØ¶Ø¹ÙˆÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…ÙˆØ¶Ø¹ Ø§Ù„ØªÙ†ÙÙŠØ° ØºØ¯Ø§Ù‹.\\n\\n**ÙØ§ØªÙ†:** Ø¨Ø§Ù„Ø¶Ø¨Ø·ØŒ Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù„Ø¯ÙŠÙ‡ Ø¯ÙˆØ±Ù‡ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø£Ù†. Ø³ÙˆØ§Ø¡ ÙƒÙ†Øª Ø·Ø§Ù„Ø¨ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø£Ùˆ Ù…ØªØ®ØµØµ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø£Ùˆ Ø­ØªÙ‰ Ø£Ù… Ø¹Ø§Ø¯ÙŠØ© ØªØ±ÙŠØ¯ ØªØ¹Ù„ÙŠÙ… Ø£ÙˆÙ„Ø§Ø¯Ù‡Ø§ØŒ ÙÙƒÙ„Ù†Ø§ Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø¶Ù…Ø§Ù† Ø¨Ù‚Ø§Ø¦Ù†Ø§ Ù…Ù„ØªØ²Ù…ÙŠÙ† Ø¨Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ©.\\n\\n**Ø£Ø­Ù…Ø¯:** Ø¥Ø°Ù†ØŒ Ø£ÙŠÙ‡Ø§ Ø§Ù„Ù…Ø³ØªÙ…Ø¹ Ø§Ù„Ø¹Ø²ÙŠØ²ØŒ Ù…Ø§ Ø±Ø£ÙŠÙƒ Ø£Ù†Øª ÙÙŠ Ø¯ÙˆØ± ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ù…Ù†Ø§ Ù‡Ù†Ø§? Ø´Ø§Ø±ÙƒÙ†ÙŠ Ø¢Ø±Ø§Ø¦Ùƒ ÙˆØªØ¹Ù„ÙŠÙ‚Ùƒ ØªØ­Øª Ø§Ù„Ø­Ù„Ù‚Ø©ØŒ ÙˆØ³ÙˆÙ Ù†ØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ø£ÙÙƒØ§Ø± Ù„Ø§Ø­Ù‚Ø§Ù‹!\\n**ÙØ§ØªÙ†:** ÙˆØ¨Ø§Ù„ØªØ£ÙƒÙŠØ¯ØŒ Ù„Ù†Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹Ø§Ù‹. Ø´ÙƒØ±Ø§Ù‹ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø­Ø¶ÙˆØ±ÙƒÙ…Ø§ Ù…Ø¹Ù†Ø§ Ø§Ù„ÙŠÙˆÙ…! ÙˆØ¯Ø§Ø¹Ø§Ù‹.\\n\\n**Ø£Ø­Ù…Ø¯:** ØªØ­ÙŠØ§ØªÙ†Ø§ Ù„ÙƒÙ… Ø¬Ù…ÙŠØ¹Ø§Ù‹. ÙˆÙ„Ø§ØªÙ†Ø³Ù‰... ØªØ§Ø¨Ø¹ÙˆÙ†Ø§ ÙÙŠ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø­ÙŠØ« Ø³Ù†ÙˆØ§ØµÙ„ Ø±Ø­Ù„ØªÙ†Ø§ Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø®Ø§ÙˆÙ ÙˆØ§Ù„Ø£Ù…Ù„ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«!', 'cleaning_method': 'micro_chunking_surgical', 'micro_chunks_processed': 29, 'cleaning_status': 'success', 'script_length': 9372, 'original_length': 9193, 'length_ratio': 1.0194713368867616, 'corruption_analysis': {'total_chars': 9193, 'foreign_patterns': {'english_words': {'count': 48, 'chars': 322, 'description': 'English words (3+ letters)', 'examples': ['SPIRAI', 'Rosetta', 'NLP']}, 'chinese_chars': {'count': 2, 'chars': 2, 'description': 'Chinese characters', 'examples': ['å…‰', 'å¾']}}, 'encoding_issues': 8, 'concatenated_words': 1, 'structural_issues': 50, 'overall_corruption_level': 'light'}, 'estimated_duration': '8-11 minutes'}\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
