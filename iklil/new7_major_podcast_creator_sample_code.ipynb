{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76e5796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# import openai\n",
    "from openai import AzureOpenAI\n",
    "# !pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3174aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def APIKeyManager(model_type, key_path):\n",
    "    \n",
    "    load_dotenv(dotenv_path=key_path, override=True)\n",
    "    if model_type=='azure':\n",
    "        client = AzureOpenAI(\n",
    "            api_version=os.environ[\"AZURE_API_VERSION\"],\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_API_KEY\"],\n",
    "        )\n",
    "        return client\n",
    "    elif model_type=='fanar':\n",
    "        client = OpenAI(\n",
    "            base_url = \"https://api.fanar.qa/v1\",\n",
    "            api_key  = os.environ[\"FANAR_API_KEY\"],\n",
    "        )\n",
    "        client.default_params = {\"model\": \"Fanar-C-1-8.7B\"}\n",
    "        return client    \n",
    "    elif model_type=='gemini':\n",
    "        pass\n",
    "    return client\n",
    "\n",
    "# Load environment variables\n",
    "model_type=\"fanar\"\n",
    "deployment = APIKeyManager(model_type, \"./azure.env\")\n",
    "model = \"Fanar-C-1-8.7B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08a84582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class TopicClassifier:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def classify_topic(self, topic, information):\n",
    "        \"\"\"\n",
    "        Classify podcast topic and determine optimal approach\n",
    "        \n",
    "        Args:\n",
    "            topic: Main topic of the podcast episode\n",
    "            information: Background information about the topic\n",
    "            \n",
    "        Returns:\n",
    "            JSON with classification results\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in analyzing and classifying topics for Arabic podcast production.\n",
    "\n",
    "Task: Analyze the following topic and determine the best approach for an Arabic podcast.\n",
    "\n",
    "Topic: {topic}\n",
    "Background Information: {information}\n",
    "\n",
    "Analyze the topic and return the result in JSON format with these exact keys:\n",
    "\n",
    "{{\n",
    "    \"primary_category\": \"Main category from the available options\",\n",
    "    \"category_justification\": \"Reason for choosing this category\",\n",
    "    \"optimal_style\": \"Best discussion style from available options\",\n",
    "    \"discourse_pattern\": \"Appropriate discourse pattern\",\n",
    "    \"audience_engagement_goal\": \"Audience engagement objective\",\n",
    "    \"cultural_sensitivity_level\": \"Cultural sensitivity level\",\n",
    "    \"controversy_potential\": \"Controversy potential level\",\n",
    "    \"key_discussion_angles\": [\n",
    "        \"First main discussion angle\",\n",
    "        \"Second point of interest for Arabic audiences\"\n",
    "    ],\n",
    "    \"natural_tension_points\": [\n",
    "        \"First natural tension point in the topic\",\n",
    "        \"Second aspect that might generate healthy debate\"\n",
    "    ],\n",
    "    \"cultural_connection_opportunities\": [\n",
    "        \"First opportunity to connect with Arabic culture\",\n",
    "        \"Second relevant local or regional reference\"\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Available Categories (choose one):\n",
    "1. \"العلوم والتكنولوجيا\" - For technical, scientific topics and innovations\n",
    "2. \"السياسة والشؤون العامة\" - For political topics, current events, public affairs\n",
    "3. \"القضايا الاجتماعية\" - For social topics, relationships, values, social challenges\n",
    "4. \"الرياضة والترفيه\" - For sports, arts, entertainment topics\n",
    "5. \"التاريخ والثقافة\" - For historical, heritage, cultural topics\n",
    "\n",
    "Available Styles (choose one):\n",
    "- \"حواري\" - Natural friendly dialogue between host and guest\n",
    "- \"تعليمي\" - Focus on explanation and education in entertaining way\n",
    "- \"ترفيهي\" - Fun and light with humorous touches\n",
    "- \"تحليلي\" - Deep, specialized analytical discussion\n",
    "\n",
    "Discourse Patterns (choose one):\n",
    "- \"رسمي\" - Formal and respectful language\n",
    "- \"ودي\" - Warm and familiar language\n",
    "- \"جدلي\" - Lively discussion with multiple viewpoints\n",
    "- \"سردي\" - Storytelling and narrative style\n",
    "\n",
    "Cultural Sensitivity Levels (choose one):\n",
    "- \"عالي\" - Requires extreme caution in handling\n",
    "- \"متوسط\" - Needs moderate cultural consideration\n",
    "- \"منخفض\" - Generally acceptable topic\n",
    "\n",
    "Controversy Potential (choose one):\n",
    "- \"عالية\" - Inherently controversial topic\n",
    "- \"متوسطة\" - May generate some disagreements\n",
    "- \"منخفضة\" - Generally acceptable topic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All JSON values must be in Modern Standard Arabic (MSA)\n",
    "- JSON keys must be in English\n",
    "- Use ONLY English commas (,) - NEVER Arabic commas (،)\n",
    "- Use ONLY standard double quotes (\") - NEVER Arabic quotes\n",
    "- Do NOT include any explanatory text before or after JSON\n",
    "- Do NOT include confidence scores like \"الثقة: 95%\"\n",
    "- Do NOT include ```json markers\n",
    "- Return ONLY valid JSON that can be parsed by json.loads()\n",
    "- Analyze the topic deeply considering Arabic cultural context\n",
    "- Focus on what makes the topic appealing to Arabic audiences\n",
    "- Optimal episode duration is 10 minutes\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in analyzing topics for Arabic podcasts. Return ONLY valid JSON with English punctuation. No explanatory text. No confidence scores. No Arabic commas.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3  # Lower temperature for more consistent classification\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Clean the JSON response to ensure it's parseable\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before the first { and after the last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('،', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove confidence scores and meta text\n",
    "        meta_patterns = [\n",
    "            r'الثقة:\\s*\\d+%',\n",
    "            r'الدقة:\\s*\\d+%',\n",
    "            r'معدل الثقة:\\s*\\d+%',\n",
    "            r'\\n.*الثقة.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*'\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def classify_with_validation(self, topic, information):\n",
    "        \"\"\"\n",
    "        Classify topic with automatic validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get classification\n",
    "                classification_result = self.classify_topic(topic, information)\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(classification_result)\n",
    "                \n",
    "                # Validate required fields\n",
    "                required_fields = [\n",
    "                    \"primary_category\", \"category_justification\", \"optimal_style\",\n",
    "                    \"discourse_pattern\", \"audience_engagement_goal\", \n",
    "                    \"cultural_sensitivity_level\", \"controversy_potential\",\n",
    "                    \"key_discussion_angles\", \"natural_tension_points\",\n",
    "                    \"cultural_connection_opportunities\"\n",
    "                ]\n",
    "                \n",
    "                missing_fields = [field for field in required_fields if field not in parsed_result]\n",
    "                \n",
    "                if not missing_fields:\n",
    "                    print(f\"✅ Classification successful on attempt {attempt + 1}\")\n",
    "                    return classification_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1}: Missing fields: {missing_fields}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(classification_result[:500])\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"📝 Using fallback classification...\")\n",
    "        fallback_result = self._get_fallback_classification(topic)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _get_fallback_classification(self, topic):\n",
    "        \"\"\"Provide fallback classification if all attempts fail\"\"\"\n",
    "        return {\n",
    "            \"primary_category\": \"القضايا الاجتماعية\",\n",
    "            \"category_justification\": \"تصنيف افتراضي للموضوع المطروح\",\n",
    "            \"optimal_style\": \"حواري\",\n",
    "            \"discourse_pattern\": \"ودي\",\n",
    "            \"audience_engagement_goal\": \"زيادة الوعي والفهم حول الموضوع\",\n",
    "            \"cultural_sensitivity_level\": \"متوسط\",\n",
    "            \"controversy_potential\": \"متوسطة\",\n",
    "            \"key_discussion_angles\": [\n",
    "                \"الجوانب الأساسية للموضوع\",\n",
    "                \"التأثيرات على المجتمع العربي\"\n",
    "            ],\n",
    "            \"natural_tension_points\": [\n",
    "                \"وجهات النظر المختلفة حول الموضوع\",\n",
    "                \"التحديات والفرص المرتبطة\"\n",
    "            ],\n",
    "            \"cultural_connection_opportunities\": [\n",
    "                \"الربط بالقيم العربية التقليدية\",\n",
    "                \"التجارب المحلية ذات الصلة\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def analyze_classification_quality(self, parsed_result):\n",
    "        \"\"\"Analyze the quality of the classification result\"\"\"\n",
    "        analysis = {\n",
    "            \"category_appropriateness\": self._assess_category_fit(parsed_result.get(\"primary_category\", \"\")),\n",
    "            \"style_consistency\": self._assess_style_choice(parsed_result.get(\"optimal_style\", \"\")),\n",
    "            \"cultural_awareness\": len(parsed_result.get(\"cultural_connection_opportunities\", [])),\n",
    "            \"discussion_depth\": len(parsed_result.get(\"key_discussion_angles\", [])),\n",
    "            \"sensitivity_awareness\": parsed_result.get(\"cultural_sensitivity_level\", \"\") != \"\",\n",
    "            \"engagement_focus\": parsed_result.get(\"audience_engagement_goal\", \"\") != \"\"\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        score_factors = [\n",
    "            analysis[\"category_appropriateness\"] > 0,\n",
    "            analysis[\"style_consistency\"],\n",
    "            analysis[\"cultural_awareness\"] >= 2,\n",
    "            analysis[\"discussion_depth\"] >= 2,\n",
    "            analysis[\"sensitivity_awareness\"],\n",
    "            analysis[\"engagement_focus\"]\n",
    "        ]\n",
    "        \n",
    "        analysis[\"overall_score\"] = sum(score_factors) * 100 // len(score_factors)\n",
    "        analysis[\"quality_grade\"] = (\n",
    "            \"ممتاز\" if analysis[\"overall_score\"] >= 85 else\n",
    "            \"جيد\" if analysis[\"overall_score\"] >= 70 else\n",
    "            \"مقبول\" if analysis[\"overall_score\"] >= 55 else\n",
    "            \"يحتاج تحسين\"\n",
    "        )\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _assess_category_fit(self, category):\n",
    "        \"\"\"Assess if the category choice seems appropriate\"\"\"\n",
    "        valid_categories = [\n",
    "            \"العلوم والتكنولوجيا\", \"السياسة والشؤون العامة\", \n",
    "            \"القضايا الاجتماعية\", \"الرياضة والترفيه\", \"التاريخ والثقافة\"\n",
    "        ]\n",
    "        return 1 if category in valid_categories else 0\n",
    "\n",
    "    def _assess_style_choice(self, style):\n",
    "        \"\"\"Assess if the style choice is valid\"\"\"\n",
    "        valid_styles = [\"حواري\", \"تعليمي\", \"ترفيهي\", \"تحليلي\"]\n",
    "        return style in valid_styles\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_topic_classifier(deployment, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the topic classifier with comprehensive validation\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Topic Classifier with Enhanced Validation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    classifier = TopicClassifier(deployment, model_name)\n",
    "    \n",
    "    # Test topic\n",
    "    topic = \"الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\"\n",
    "    \n",
    "    information = '''\n",
    "مع انتشار تقنيات الذكاء الاصطناعي بسرعة في العالم العربي، تزداد المخاوف حول تأثيرها على الهوية الثقافية واللغة العربية. \n",
    "تشير الدراسات إلى أن 78% من المحتوى الرقمي باللغة الإنجليزية، بينما المحتوى العربي لا يتجاوز 3%. \n",
    "معظم نماذج الذكاء الاصطناعي الحالية مدربة على بيانات غربية، مما يثير تساؤلات حول قدرتها على فهم السياق الثقافي العربي.\n",
    "في المقابل، تسعى دول مثل الإمارات والسعودية لتطوير نماذج ذكاء اصطناعي عربية مثل \"جايس\" و\"الحوراء\" لمواجهة هذا التحدي.\n",
    "التحدي الأكبر يكمن في كيفية الاستفادة من هذه التقنيات لتعزيز الثقافة العربية بدلاً من تهميشها، وضمان أن تخدم الذكاء الاصطناعي قيمنا ومبادئنا.\n",
    "'''\n",
    "    \n",
    "    # Run classification with validation\n",
    "    classification_result, parsed_result = classifier.classify_with_validation(topic, information)\n",
    "    \n",
    "    print(\"📊 Classification Results:\")\n",
    "    print(f\"Primary Category: {parsed_result.get('primary_category', 'N/A')}\")\n",
    "    print(f\"Optimal Style: {parsed_result.get('optimal_style', 'N/A')}\")\n",
    "    print(f\"Discourse Pattern: {parsed_result.get('discourse_pattern', 'N/A')}\")\n",
    "    print(f\"Cultural Sensitivity: {parsed_result.get('cultural_sensitivity_level', 'N/A')}\")\n",
    "    print(f\"Controversy Potential: {parsed_result.get('controversy_potential', 'N/A')}\")\n",
    "    \n",
    "    # Analyze quality\n",
    "    quality_analysis = classifier.analyze_classification_quality(parsed_result)\n",
    "    print(f\"\\n📈 Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Cultural Awareness: {quality_analysis['cultural_awareness']} connections\")\n",
    "    print(f\"Discussion Angles: {quality_analysis['discussion_depth']} angles\")\n",
    "    \n",
    "    # Show key discussion points\n",
    "    print(f\"\\n🎯 Key Discussion Angles:\")\n",
    "    for i, angle in enumerate(parsed_result.get('key_discussion_angles', []), 1):\n",
    "        print(f\"  {i}. {angle}\")\n",
    "    \n",
    "    print(f\"\\n🌍 Cultural Connections:\")\n",
    "    for i, connection in enumerate(parsed_result.get('cultural_connection_opportunities', []), 1):\n",
    "        print(f\"  {i}. {connection}\")\n",
    "    \n",
    "    return classification_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# classifier = TopicClassifier(deployment, \"Fanar-C-1-8.7B\")\n",
    "# classification_result, parsed_result = classifier.classify_with_validation(topic, information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bde0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Result:\n",
      "{\n",
      "  \"primary_category\": \"العلوم والتكنولوجيا\",\n",
      "  \"category_justification\": \"تتناول الموضوع تطورات تكنولوجية وتأثيراتها على المجتمع العربي\",\n",
      "  \"optimal_style\": \"تعليمي\",\n",
      "  \"discourse_pattern\": \"رسمي\",\n",
      "  \"audience_engagement_goal\": \"إثارة الوعي والفهم بين المستمعين العرب\",\n",
      "  \"cultural_sensitivity_level\": \"متوسط\",\n",
      "  \"controversy_potential\": \"متوسطة\",\n",
      "  \"key_discussion_angles\": [\"تأثير الذكاء الاصطناعي على اللغة العربية\", \"استخدام الذكاء الاصطناعي لحماية الثقافة العربية\"],\n",
      "  \"natural_tension_points\": [\"مخاطر الاعتماد الزائد على الذكاء الاصطناعي الغربي\", \"فرص استخدام الذكاء الاصطناعي المحلي\"],\n",
      "  \"cultural_connection_opportunities\": [\"دور الإسلام في التعامل مع التقدم التكنولوجي\", \"قصص نجاح الذكاء الاصطناعي العربي\"]\n",
      "}\n",
      "✅ Category: العلوم والتكنولوجيا\n",
      "✅ Style: تعليمي\n"
     ]
    }
   ],
   "source": [
    "# Testing Instructions:\n",
    "\n",
    "# To test Step 1, add this to a new cell in your notebook:\n",
    "\n",
    "# Test Step 1: Topic Classification\n",
    "\n",
    "# Test with the singlehood topic\n",
    "topic = \"الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\"\n",
    "\n",
    "information = '''\n",
    "مع انتشار تقنيات الذكاء الاصطناعي بسرعة في العالم العربي، تزداد المخاوف حول تأثيرها على الهوية الثقافية واللغة العربية. \n",
    "تشير الدراسات إلى أن 78% من المحتوى الرقمي باللغة الإنجليزية، بينما المحتوى العربي لا يتجاوز 3%. \n",
    "معظم نماذج الذكاء الاصطناعي الحالية مدربة على بيانات غربية، مما يثير تساؤلات حول قدرتها على فهم السياق الثقافي العربي.\n",
    "في المقابل، تسعى دول مثل الإمارات والسعودية لتطوير نماذج ذكاء اصطناعي عربية مثل \"جايس\" و\"الحوراء\" لمواجهة هذا التحدي.\n",
    "التحدي الأكبر يكمن في كيفية الاستفادة من هذه التقنيات لتعزيز الثقافة العربية بدلاً من تهميشها، وضمان أن تخدم الذكاء الاصطناعي قيمنا ومبادئنا.\n",
    "'''\n",
    "\n",
    "classifier = TopicClassifier(deployment, model)\n",
    "classification_result = classifier.classify_topic(topic, information)\n",
    "print(\"Classification Result:\")\n",
    "print(classification_result)\n",
    "\n",
    "try:\n",
    "    parsed_result = json.loads(classification_result)\n",
    "    print(f\"✅ Category: {parsed_result['primary_category']}\")\n",
    "    print(f\"✅ Style: {parsed_result['optimal_style']}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"❌ JSON parsing failed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc9ee26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class SimplePersonaGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def generate_personas(self, topic, information, classification_result):\n",
    "        \"\"\"\n",
    "        Generate simple but effective host and guest personas\n",
    "        \n",
    "        Args:\n",
    "            topic: Main topic of the podcast episode\n",
    "            information: Background information about the topic\n",
    "            classification_result: JSON string from Step 1 classification\n",
    "            \n",
    "        Returns:\n",
    "            JSON with simple host and guest personas\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse classification to understand the requirements\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid classification JSON provided\")\n",
    "        \n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        discourse_pattern = classification.get(\"discourse_pattern\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in designing Arabic podcast personas.\n",
    "\n",
    "Task: Create simple and suitable host and guest personas for this topic.\n",
    "\n",
    "Topic: {topic}\n",
    "Information: {information}\n",
    "Category: {primary_category}\n",
    "Required Style: {optimal_style}\n",
    "Discourse Pattern: {discourse_pattern}\n",
    "Cultural Sensitivity: {cultural_sensitivity}\n",
    "\n",
    "Return the result in this exact JSON format:\n",
    "\n",
    "{{\n",
    "    \"host\": {{\n",
    "        \"name\": \"Host's Arabic name\",\n",
    "        \"age\": numeric_age,\n",
    "        \"background\": \"Brief background in one sentence\",\n",
    "        \"personality\": \"Personality description in one sentence\",\n",
    "        \"speaking_style\": \"Speaking style in one sentence\"\n",
    "    }},\n",
    "    \"guest\": {{\n",
    "        \"name\": \"Guest's Arabic name\", \n",
    "        \"age\": numeric_age,\n",
    "        \"background\": \"Brief background in one sentence\",\n",
    "        \"expertise\": \"Area of expertise in one sentence\",\n",
    "        \"personality\": \"Personality description in one sentence\",\n",
    "        \"speaking_style\": \"Speaking style in one sentence\"\n",
    "    }},\n",
    "    \"why_good_match\": \"Why this host and guest are suitable for this topic - one sentence\"\n",
    "}}\n",
    "\n",
    "Requirements:\n",
    "- Use familiar Arabic names (like أحمد، محمد، فاطمة، نور، علي، لمى، سارة، خالد)\n",
    "- Simple and believable personas\n",
    "- Suitable for the topic and required style: {optimal_style}\n",
    "- Host should be curious and guest should be expert or have experience\n",
    "- All JSON values must be in Modern Standard Arabic (MSA)\n",
    "- JSON keys should be in English\n",
    "- Use ONLY English commas (,) - NEVER Arabic commas (،)\n",
    "- Use ONLY standard double quotes (\") - NEVER Arabic quotes\n",
    "- Age should be realistic numbers (25-55 range)\n",
    "- Do NOT include ```json markers\n",
    "- Do NOT include confidence scores or extra text\n",
    "- Return only valid JSON\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Host personality should match {optimal_style} style\n",
    "- Guest expertise should be relevant to: {topic}\n",
    "- Consider cultural sensitivity level: {cultural_sensitivity}\n",
    "- Make personas realistic and relatable to Arabic audiences\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are an expert in designing simple and effective podcast personas. Style: {optimal_style}. Always provide JSON values in Modern Standard Arabic while keeping keys in English. No extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Clean the JSON response to ensure it's parseable\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before the first { and after the last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('،', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove confidence scores and meta text\n",
    "        meta_patterns = [\n",
    "            r'الثقة:\\s*\\d+%',\n",
    "            r'الدقة:\\s*\\d+%',\n",
    "            r'معدل الثقة:\\s*\\d+%',\n",
    "            r'\\n.*الثقة.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*',\n",
    "            r'ملاحظة:.*',\n",
    "            r'تعليق:.*'\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def generate_personas_with_validation(self, topic, information, classification_result):\n",
    "        \"\"\"\n",
    "        Generate personas with automatic validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get personas\n",
    "                personas_result = self.generate_personas(topic, information, classification_result)\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(personas_result)\n",
    "                \n",
    "                # Validate required fields\n",
    "                validation_result = self._validate_personas(parsed_result)\n",
    "                \n",
    "                if validation_result[\"is_valid\"]:\n",
    "                    print(f\"✅ Persona generation successful on attempt {attempt + 1}\")\n",
    "                    return personas_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1}: Validation issues: {validation_result['issues']}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(personas_result[:500])\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"📝 Using fallback personas...\")\n",
    "        fallback_result = self._get_fallback_personas(topic, classification_result)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _validate_personas(self, parsed_result):\n",
    "        \"\"\"Validate the generated personas\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check main structure\n",
    "        if \"host\" not in parsed_result:\n",
    "            issues.append(\"Missing host section\")\n",
    "        if \"guest\" not in parsed_result:\n",
    "            issues.append(\"Missing guest section\")\n",
    "        if \"why_good_match\" not in parsed_result:\n",
    "            issues.append(\"Missing why_good_match section\")\n",
    "        \n",
    "        # Check host fields\n",
    "        host = parsed_result.get(\"host\", {})\n",
    "        required_host_fields = [\"name\", \"age\", \"background\", \"personality\", \"speaking_style\"]\n",
    "        for field in required_host_fields:\n",
    "            if field not in host or not host[field]:\n",
    "                issues.append(f\"Missing or empty host.{field}\")\n",
    "        \n",
    "        # Check guest fields\n",
    "        guest = parsed_result.get(\"guest\", {})\n",
    "        required_guest_fields = [\"name\", \"age\", \"background\", \"expertise\", \"personality\", \"speaking_style\"]\n",
    "        for field in required_guest_fields:\n",
    "            if field not in guest or not guest[field]:\n",
    "                issues.append(f\"Missing or empty guest.{field}\")\n",
    "        \n",
    "        # Check age validity\n",
    "        if \"age\" in host:\n",
    "            try:\n",
    "                age = int(host[\"age\"])\n",
    "                if age < 20 or age > 70:\n",
    "                    issues.append(f\"Host age {age} unrealistic (should be 20-70)\")\n",
    "            except (ValueError, TypeError):\n",
    "                issues.append(\"Host age should be a number\")\n",
    "        \n",
    "        if \"age\" in guest:\n",
    "            try:\n",
    "                age = int(guest[\"age\"])\n",
    "                if age < 20 or age > 70:\n",
    "                    issues.append(f\"Guest age {age} unrealistic (should be 20-70)\")\n",
    "            except (ValueError, TypeError):\n",
    "                issues.append(\"Guest age should be a number\")\n",
    "        \n",
    "        # Check for Arabic content\n",
    "        all_text = \" \".join([\n",
    "            str(host.get(\"name\", \"\")), str(host.get(\"background\", \"\")),\n",
    "            str(guest.get(\"name\", \"\")), str(guest.get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"why_good_match\", \"\"))\n",
    "        ])\n",
    "        \n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', all_text))\n",
    "        if arabic_chars < 10:\n",
    "            issues.append(\"Insufficient Arabic content\")\n",
    "        \n",
    "        return {\n",
    "            \"is_valid\": len(issues) == 0,\n",
    "            \"issues\": issues,\n",
    "            \"score\": max(0, 100 - len(issues) * 15)\n",
    "        }\n",
    "\n",
    "    def _get_fallback_personas(self, topic, classification_result):\n",
    "        \"\"\"Provide fallback personas if generation fails\"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            optimal_style = classification.get(\"optimal_style\", \"حواري\")\n",
    "        except:\n",
    "            optimal_style = \"حواري\"\n",
    "        \n",
    "        # Determine appropriate personas based on style\n",
    "        if optimal_style == \"تعليمي\":\n",
    "            host_personality = \"مقدم متحمس للتعلم ويطرح أسئلة واضحة\"\n",
    "            guest_personality = \"خبير صبور يشرح المعلومات بطريقة مبسطة\"\n",
    "        elif optimal_style == \"تحليلي\":\n",
    "            host_personality = \"مقدم مفكر يطرح أسئلة عميقة ومدروسة\"\n",
    "            guest_personality = \"محلل خبير يقدم رؤى متخصصة ومعمقة\"\n",
    "        elif optimal_style == \"ترفيهي\":\n",
    "            host_personality = \"مقدم مرح ومتفاعل يضيف روح الدعابة\"\n",
    "            guest_personality = \"ضيف ودود وطريف يشارك تجاربه بمرح\"\n",
    "        else:  # حواري\n",
    "            host_personality = \"مقدم ودود وفضولي يحب الحوار الطبيعي\"\n",
    "            guest_personality = \"ضيف متفتح ومتعاون يشارك خبراته بصراحة\"\n",
    "        \n",
    "        return {\n",
    "            \"host\": {\n",
    "                \"name\": \"أحمد السالم\",\n",
    "                \"age\": 35,\n",
    "                \"background\": \"مقدم برامج إذاعية مع خبرة في المواضيع المتنوعة\",\n",
    "                \"personality\": host_personality,\n",
    "                \"speaking_style\": \"يتحدث بوضوح ويطرح أسئلة مفتوحة لإثراء الحوار\"\n",
    "            },\n",
    "            \"guest\": {\n",
    "                \"name\": \"نور العلي\",\n",
    "                \"age\": 40,\n",
    "                \"background\": \"خبير ومختص في مجال الموضوع المطروح\",\n",
    "                \"expertise\": \"لديه معرفة عميقة وتجربة عملية في مجال النقاش\",\n",
    "                \"personality\": guest_personality,\n",
    "                \"speaking_style\": \"يعبر عن أفكاره بوضوح ويستخدم أمثلة من الواقع\"\n",
    "            },\n",
    "            \"why_good_match\": \"المقدم يجيد طرح الأسئلة والضيف لديه الخبرة للإجابة بفعالية\"\n",
    "        }\n",
    "\n",
    "    def analyze_persona_quality(self, parsed_result):\n",
    "        \"\"\"Analyze the quality of generated personas\"\"\"\n",
    "        analysis = {\n",
    "            \"name_authenticity\": self._assess_arabic_names(parsed_result),\n",
    "            \"age_realism\": self._assess_age_realism(parsed_result),\n",
    "            \"background_relevance\": self._assess_background_relevance(parsed_result),\n",
    "            \"personality_distinctiveness\": self._assess_personality_distinctiveness(parsed_result),\n",
    "            \"style_alignment\": self._assess_style_alignment(parsed_result),\n",
    "            \"cultural_appropriateness\": self._assess_cultural_appropriateness(parsed_result)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        score_factors = [\n",
    "            analysis[\"name_authenticity\"],\n",
    "            analysis[\"age_realism\"],\n",
    "            analysis[\"background_relevance\"] > 0,\n",
    "            analysis[\"personality_distinctiveness\"],\n",
    "            analysis[\"style_alignment\"] > 0,\n",
    "            analysis[\"cultural_appropriateness\"]\n",
    "        ]\n",
    "        \n",
    "        analysis[\"overall_score\"] = sum(score_factors) * 100 // len(score_factors)\n",
    "        analysis[\"quality_grade\"] = (\n",
    "            \"ممتاز\" if analysis[\"overall_score\"] >= 85 else\n",
    "            \"جيد\" if analysis[\"overall_score\"] >= 70 else\n",
    "            \"مقبول\" if analysis[\"overall_score\"] >= 55 else\n",
    "            \"يحتاج تحسين\"\n",
    "        )\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _assess_arabic_names(self, parsed_result):\n",
    "        \"\"\"Check if names are authentic Arabic names\"\"\"\n",
    "        host_name = parsed_result.get(\"host\", {}).get(\"name\", \"\")\n",
    "        guest_name = parsed_result.get(\"guest\", {}).get(\"name\", \"\")\n",
    "        \n",
    "        common_names = [\n",
    "            \"أحمد\", \"محمد\", \"علي\", \"خالد\", \"عمر\", \"يوسف\", \"حسن\", \"كريم\",\n",
    "            \"فاطمة\", \"عائشة\", \"نور\", \"لمى\", \"سارة\", \"مريم\", \"زينب\", \"رقية\"\n",
    "        ]\n",
    "        \n",
    "        host_authentic = any(name in host_name for name in common_names)\n",
    "        guest_authentic = any(name in guest_name for name in common_names)\n",
    "        \n",
    "        return host_authentic and guest_authentic\n",
    "\n",
    "    def _assess_age_realism(self, parsed_result):\n",
    "        \"\"\"Check if ages are realistic\"\"\"\n",
    "        try:\n",
    "            host_age = int(parsed_result.get(\"host\", {}).get(\"age\", 0))\n",
    "            guest_age = int(parsed_result.get(\"guest\", {}).get(\"age\", 0))\n",
    "            return 25 <= host_age <= 55 and 25 <= guest_age <= 65\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _assess_background_relevance(self, parsed_result):\n",
    "        \"\"\"Assess if backgrounds are relevant and detailed\"\"\"\n",
    "        host_bg = parsed_result.get(\"host\", {}).get(\"background\", \"\")\n",
    "        guest_bg = parsed_result.get(\"guest\", {}).get(\"background\", \"\")\n",
    "        guest_exp = parsed_result.get(\"guest\", {}).get(\"expertise\", \"\")\n",
    "        \n",
    "        relevance_score = 0\n",
    "        if len(host_bg) > 20:\n",
    "            relevance_score += 1\n",
    "        if len(guest_bg) > 20:\n",
    "            relevance_score += 1\n",
    "        if len(guest_exp) > 20:\n",
    "            relevance_score += 1\n",
    "        \n",
    "        return relevance_score\n",
    "\n",
    "    def _assess_personality_distinctiveness(self, parsed_result):\n",
    "        \"\"\"Check if host and guest have distinct personalities\"\"\"\n",
    "        host_personality = parsed_result.get(\"host\", {}).get(\"personality\", \"\")\n",
    "        guest_personality = parsed_result.get(\"guest\", {}).get(\"personality\", \"\")\n",
    "        \n",
    "        # Simple check: personalities should be different\n",
    "        similarity = len(set(host_personality.split()) & set(guest_personality.split()))\n",
    "        total_words = len(set(host_personality.split()) | set(guest_personality.split()))\n",
    "        \n",
    "        return similarity / total_words < 0.5 if total_words > 0 else False\n",
    "\n",
    "    def _assess_style_alignment(self, parsed_result):\n",
    "        \"\"\"Assess if personas align with the intended style\"\"\"\n",
    "        host_style = parsed_result.get(\"host\", {}).get(\"speaking_style\", \"\")\n",
    "        guest_style = parsed_result.get(\"guest\", {}).get(\"speaking_style\", \"\")\n",
    "        \n",
    "        style_indicators = {\n",
    "            \"حواري\": [\"ودود\", \"طبيعي\", \"تفاعل\", \"حوار\"],\n",
    "            \"تعليمي\": [\"تعليم\", \"شرح\", \"توضيح\", \"تبسيط\"],\n",
    "            \"تحليلي\": [\"تحليل\", \"عمق\", \"تخصص\", \"دقة\"],\n",
    "            \"ترفيهي\": [\"مرح\", \"دعابة\", \"ترفيه\", \"خفة\"]\n",
    "        }\n",
    "        \n",
    "        # This is a simplified assessment\n",
    "        return len(host_style) > 15 and len(guest_style) > 15\n",
    "\n",
    "    def _assess_cultural_appropriateness(self, parsed_result):\n",
    "        \"\"\"Check if personas are culturally appropriate\"\"\"\n",
    "        all_content = \" \".join([\n",
    "            str(parsed_result.get(\"host\", {}).get(\"name\", \"\")),\n",
    "            str(parsed_result.get(\"host\", {}).get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"guest\", {}).get(\"name\", \"\")),\n",
    "            str(parsed_result.get(\"guest\", {}).get(\"background\", \"\")),\n",
    "            str(parsed_result.get(\"why_good_match\", \"\"))\n",
    "        ])\n",
    "        \n",
    "        # Check for Arabic content\n",
    "        arabic_ratio = len(re.findall(r'[\\u0600-\\u06FF]', all_content)) / len(all_content) if all_content else 0\n",
    "        return arabic_ratio > 0.3\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_persona_generator(deployment, topic, information, classification_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the persona generator with comprehensive validation\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Persona Generator with Enhanced Validation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = SimplePersonaGenerator(deployment, model_name)\n",
    "    \n",
    "    # Run persona generation with validation\n",
    "    personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)\n",
    "    \n",
    "    print(\"👥 Generated Personas:\")\n",
    "    host = parsed_result.get(\"host\", {})\n",
    "    guest = parsed_result.get(\"guest\", {})\n",
    "    \n",
    "    print(f\"\\n🎤 Host: {host.get('name', 'N/A')} (عمر {host.get('age', 'N/A')})\")\n",
    "    print(f\"   الخلفية: {host.get('background', 'N/A')}\")\n",
    "    print(f\"   الشخصية: {host.get('personality', 'N/A')}\")\n",
    "    print(f\"   أسلوب الحديث: {host.get('speaking_style', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Guest: {guest.get('name', 'N/A')} (عمر {guest.get('age', 'N/A')})\")\n",
    "    print(f\"   الخلفية: {guest.get('background', 'N/A')}\")\n",
    "    print(f\"   الخبرة: {guest.get('expertise', 'N/A')}\")\n",
    "    print(f\"   الشخصية: {guest.get('personality', 'N/A')}\")\n",
    "    print(f\"   أسلوب الحديث: {guest.get('speaking_style', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n🤝 Why Good Match: {parsed_result.get('why_good_match', 'N/A')}\")\n",
    "    \n",
    "    # Analyze quality\n",
    "    quality_analysis = generator.analyze_persona_quality(parsed_result)\n",
    "    print(f\"\\n📈 Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Name Authenticity: {'✅' if quality_analysis['name_authenticity'] else '❌'}\")\n",
    "    print(f\"Age Realism: {'✅' if quality_analysis['age_realism'] else '❌'}\")\n",
    "    print(f\"Background Relevance: {quality_analysis['background_relevance']}/3\")\n",
    "    print(f\"Personality Distinctiveness: {'✅' if quality_analysis['personality_distinctiveness'] else '❌'}\")\n",
    "    \n",
    "    return personas_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# generator = SimplePersonaGenerator(deployment, \"Fanar-C-1-8.7B\")\n",
    "# personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f699140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Persona generation successful on attempt 1\n",
      "Personas Result:\n",
      "{\n",
      "  \"host\": {\n",
      "    \"name\": \"د. فادي حسن\",\n",
      "    \"age\": 40,\n",
      "    \"background\": \"أستاذ جامعي متخصص في اللغويات والحوسبة\",\n",
      "    \"personality\": \"مبتكر فضولي يبحث عن حلول مبتكرة لتحقيق توازن بين التكنولوجيا والثقافة الإسلامية والعربية.\",\n",
      "    \"speaking_style\": \"متحدث واضح وصريح مع تركيز على تقديم المعلومات بتوضيح مفصل.\"\n",
      "  },\n",
      "  \"guest\": {\n",
      "    \"name\": \"مهندسة رانيا الصغير\",\n",
      "    \"age\": 35,\n",
      "    \"background\": \"باحثة محترفة تعمل على تطوير نماذج الذكاء الاصطناعي باللغة العربية.\",\n",
      "    \"expertise\": \"تخصصها الرئيسي هو التعلم الآلي وسياقات اللغة العربية.\",\n",
      "    \"personality\": \"حماسية ومتفانية في البحث عن طرق جديدة لاستخدام الذكاء الاصطناعي بما يتماشى مع القيم الإسلامية.\",\n",
      "    \"speaking_style\": \"متحدثة شغوفة تحب مشاركة أفكارها وتجاربها العملية.\"\n",
      "  },\n",
      "  \"why_good_match\": \"هذه الشراكة مناسبة لأن د.فادي حسن يجلب منظورًا أكاديميًا وفكريًا للنقاش, بينما تقدم مهندسة رانيا الصغير وجهة نظر عملية ومعرفة مباشرة بأبحاث الذكاء الاصطناعي.\"\n",
      "}\n",
      "Host: د. فادي حسن (40 years)\n",
      "Guest: مهندسة رانيا الصغير (35 years)\n"
     ]
    }
   ],
   "source": [
    "generator = SimplePersonaGenerator(deployment, model)\n",
    "personas_result, parsed_result = generator.generate_personas_with_validation(topic, information, classification_result)\n",
    "print(\"Personas Result:\")\n",
    "print(personas_result)\n",
    "    # Results are guaranteed to be valid JSON with all required fields\n",
    "host = parsed_result['host']\n",
    "guest = parsed_result['guest']\n",
    "print(f\"Host: {host['name']} ({host['age']} years)\")\n",
    "print(f\"Guest: {guest['name']} ({guest['age']} years)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab52d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class FixedConversationStructureGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def generate_conversation_structure(self, topic, information, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Step 3: Generate core conversation structure with Arabic-only content\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse inputs\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided for classification or personas\")\n",
    "        \n",
    "        # Extract key info\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        discourse_pattern = classification.get(\"discourse_pattern\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "        host_background = host.get('background', '')\n",
    "        guest_background = guest.get('background', '')\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in designing conversation structures for Arabic podcasts.\n",
    "\n",
    "CRITICAL LANGUAGE REQUIREMENTS:\n",
    "- Use ONLY Arabic language (Modern Standard Arabic)\n",
    "- NO English words, phrases, or sentences\n",
    "- NO Chinese, Japanese, or any other foreign languages\n",
    "- NO foreign characters, symbols, or punctuation\n",
    "- Arabic text ONLY with standard JSON punctuation (, : \" {{ }})\n",
    "\n",
    "Task: Create a conversation structure for this Arabic podcast episode.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "\n",
    "Generate ONLY this JSON structure with Arabic content:\n",
    "\n",
    "{{\n",
    "    \"episode_topic\": \"Arabic episode topic here\",\n",
    "    \"personas\": {{\n",
    "        \"host\": {{\n",
    "            \"name\": \"{host_name}\",\n",
    "            \"background\": \"Arabic background description\",\n",
    "            \"speaking_style\": \"Arabic speaking style description\"\n",
    "        }},\n",
    "        \"guest\": {{\n",
    "            \"name\": \"{guest_name}\",\n",
    "            \"background\": \"Arabic background description\",\n",
    "            \"speaking_style\": \"Arabic speaking style description\"\n",
    "        }}\n",
    "    }},\n",
    "    \"conversation_flow\": {{\n",
    "        \"intro1\": {{\n",
    "            \"opening_line\": \"Arabic opening line for host\",\n",
    "            \"podcast_introduction\": \"Arabic podcast introduction\",\n",
    "            \"episode_hook\": \"Arabic engaging hook about topic\"\n",
    "        }},\n",
    "        \"intro2\": {{\n",
    "            \"topic_introduction\": \"Arabic topic introduction\",\n",
    "            \"guest_welcome\": \"Arabic welcome message for guest\",\n",
    "            \"guest_bio_highlight\": \"Arabic guest background highlight\"\n",
    "        }},\n",
    "        \"main_discussion\": [\n",
    "            {{\n",
    "                \"point_title\": \"Arabic first discussion point\",\n",
    "                \"personal_angle\": \"Arabic personal connection\"\n",
    "            }},\n",
    "            {{\n",
    "                \"point_title\": \"Arabic second discussion point\", \n",
    "                \"personal_angle\": \"Arabic personal angle\"\n",
    "            }},\n",
    "            {{\n",
    "                \"point_title\": \"Arabic third discussion point\",\n",
    "                \"personal_angle\": \"Arabic concluding angle\"\n",
    "            }}\n",
    "        ],\n",
    "        \"closing\": {{\n",
    "            \"conclusion\": {{\n",
    "                \"main_takeaways\": \"Arabic main takeaways\",\n",
    "                \"guest_final_message\": \"Arabic guest final message\",\n",
    "                \"host_closing_thoughts\": \"Arabic host closing thoughts\"\n",
    "            }},\n",
    "            \"outro\": {{\n",
    "                \"guest_appreciation\": \"Arabic thank guest message\",\n",
    "                \"audience_thanks\": \"Arabic thank audience message\",\n",
    "                \"call_to_action\": \"Arabic call for engagement\",\n",
    "                \"final_goodbye\": \"Arabic final goodbye\"\n",
    "            }}\n",
    "        }}\n",
    "    }},\n",
    "    \"cultural_context\": {{\n",
    "        \"proverbs_sayings\": [\n",
    "            \"Arabic proverb related to topic\",\n",
    "            \"Arabic wisdom saying\"\n",
    "        ],\n",
    "        \"regional_references\": [\n",
    "            \"Arabic local reference related to topic\",\n",
    "            \"Arabic regional experience\"\n",
    "        ]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENTS:\n",
    "- Use ONLY English commas (,) not Arabic commas (،)\n",
    "- Use ONLY standard double quotes (\") not Arabic quotes\n",
    "- Return ONLY the JSON structure above\n",
    "- NO explanatory text before or after JSON\n",
    "- NO confidence scores or meta-text\n",
    "- NO ```json markers or code blocks\n",
    "- All Arabic text must be grammatically correct MSA\n",
    "\n",
    "Replace all placeholder text with actual Arabic content specific to the topic: {topic}\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Generate Arabic podcast conversation structure. Return ONLY valid JSON with Arabic content. NO foreign languages. Use English JSON punctuation only.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3  # Lower temperature for more predictable output\n",
    "        )\n",
    "        \n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Enhanced Arabic-only JSON cleaning method\"\"\"\n",
    "        if not response:\n",
    "            return \"{}\"\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents for JSON\n",
    "        clean_json = clean_json.replace('،', ',')  # Arabic comma to English comma\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace('\"', '\"')  # Arabic quote to English quote\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        clean_json = clean_json.replace(''', \"'\")  # Arabic apostrophe\n",
    "        \n",
    "        # Remove foreign language characters (Chinese, Japanese, etc.)\n",
    "        # Keep only Arabic Unicode ranges + basic JSON syntax\n",
    "        foreign_patterns = [\n",
    "            r'[\\u4e00-\\u9fff]',  # Chinese characters\n",
    "            r'[\\u3040-\\u309f]',  # Hiragana\n",
    "            r'[\\u30a0-\\u30ff]',  # Katakana\n",
    "            r'[\\u3000-\\u303f]',  # Japanese punctuation\n",
    "            r'[\\uff00-\\uffef]',  # Fullwidth characters\n",
    "            r'[\\u2000-\\u206f]',  # General punctuation (some problematic ones)\n",
    "        ]\n",
    "        \n",
    "        for pattern in foreign_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json)\n",
    "        \n",
    "        # Remove specific problematic characters we've seen\n",
    "        problematic_chars = ['千', '浮', '起', '提', '您', '足', '于', '、', '！', 'Indeed', 'how']\n",
    "        for char in problematic_chars:\n",
    "            clean_json = clean_json.replace(char, '')\n",
    "        \n",
    "        # Remove English words mixed in Arabic text (basic detection)\n",
    "        # This is a simple approach - remove common English words found in previous outputs\n",
    "        english_words = [\n",
    "            'Translation:', 'Hello', 'everyone', 'welcome', 'back', 'Indeed',\n",
    "            'how', 'preserving', 'culture', 'and', 'identity', 'in', 'digital', 'age',\n",
    "            'NLP', 'để', 'tweaking', 'idees', 'AbdulRahman', 'Fatima'\n",
    "        ]\n",
    "        \n",
    "        for word in english_words:\n",
    "            clean_json = clean_json.replace(word, '')\n",
    "        \n",
    "        # Remove meta-text patterns\n",
    "        meta_patterns = [\n",
    "            r'الثقة:\\s*\\d+%',\n",
    "            r'الدقة:\\s*\\d+%',\n",
    "            r'معدل الثقة:\\s*\\d+%',\n",
    "            r'\\n.*الثقة.*',\n",
    "            r'\\n.*confidence.*',\n",
    "            r'\\n.*accuracy.*',\n",
    "            r'ملاحظة:.*',\n",
    "            r'تعليق:.*',\n",
    "            r'\\(Translation:.*?\\)',\n",
    "            r'\\*\\*.*?\\*\\*',  # Remove markdown bold\n",
    "        ]\n",
    "        \n",
    "        for pattern in meta_patterns:\n",
    "            clean_json = re.sub(pattern, '', clean_json, flags=re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Fix missing commas between properties\n",
    "        clean_json = re.sub(r'\"\\s*\\n\\s*\"', '\",\\n\"', clean_json)\n",
    "        \n",
    "        # Remove multiple spaces\n",
    "        clean_json = re.sub(r'\\s+', ' ', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json.strip()\n",
    "\n",
    "    def _validate_arabic_only(self, text):\n",
    "        \"\"\"Validate that text contains only Arabic and basic punctuation\"\"\"\n",
    "        if not text:\n",
    "            return False, \"Empty text\"\n",
    "        \n",
    "        # Check for foreign language characters\n",
    "        foreign_patterns = [\n",
    "            (r'[\\u4e00-\\u9fff]', \"Chinese characters detected\"),\n",
    "            (r'[\\u3040-\\u309f\\u30a0-\\u30ff]', \"Japanese characters detected\"),\n",
    "            (r'\\b[a-zA-Z]{2,}\\b', \"English words detected\"),\n",
    "            (r'[千浮起提您足于、！]', \"Specific foreign characters detected\")\n",
    "        ]\n",
    "        \n",
    "        for pattern, message in foreign_patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return False, message\n",
    "        \n",
    "        # Check for minimum Arabic content\n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "        total_chars = len(re.sub(r'[\\s\\{\\}\",:\\[\\]]', '', text))  # Exclude JSON syntax\n",
    "        \n",
    "        if total_chars > 0:\n",
    "            arabic_ratio = arabic_chars / total_chars\n",
    "            if arabic_ratio < 0.8:  # At least 80% Arabic\n",
    "                return False, f\"Insufficient Arabic content: {arabic_ratio:.2%}\"\n",
    "        \n",
    "        return True, \"Arabic validation successful\"\n",
    "\n",
    "    def generate_conversation_structure_with_validation(self, topic, information, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Generate conversation structure with Arabic-only validation and retry\n",
    "        \"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Get structure\n",
    "                structure_result = self.generate_conversation_structure(topic, information, classification_result, personas_result)\n",
    "                \n",
    "                # Validate Arabic-only content\n",
    "                is_arabic_valid, arabic_message = self._validate_arabic_only(structure_result)\n",
    "                if not is_arabic_valid:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1}: Arabic validation failed: {arabic_message}\")\n",
    "                    continue\n",
    "                \n",
    "                # Try to parse JSON\n",
    "                parsed_result = json.loads(structure_result)\n",
    "                \n",
    "                # Validate structure completeness\n",
    "                is_structure_valid, structure_message = self.validate_conversation_structure(structure_result)\n",
    "                \n",
    "                if is_structure_valid:\n",
    "                    print(f\"✅ Conversation structure generation successful on attempt {attempt + 1}\")\n",
    "                    return structure_result, parsed_result\n",
    "                else:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1}: Structure validation failed: {structure_message}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: JSON parsing error: {e}\")\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(\"Raw response for debugging:\")\n",
    "                    print(structure_result[:300] + \"...\" if len(structure_result) > 300 else structure_result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Attempt {attempt + 1}: General error: {e}\")\n",
    "        \n",
    "        # If all attempts fail, return fallback\n",
    "        print(\"📝 Using fallback conversation structure...\")\n",
    "        fallback_result = self._get_fallback_structure(topic, classification_result, personas_result)\n",
    "        return json.dumps(fallback_result, ensure_ascii=False, indent=2), fallback_result\n",
    "\n",
    "    def _get_fallback_structure(self, topic, classification_result, personas_result):\n",
    "        \"\"\"Provide Arabic-only fallback conversation structure\"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "            optimal_style = classification.get(\"optimal_style\", \"حواري\")\n",
    "        except:\n",
    "            optimal_style = \"حواري\"\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "        \n",
    "        return {\n",
    "            \"episode_topic\": f\"نقاش حول {topic}\",\n",
    "            \"personas\": {\n",
    "                \"host\": {\n",
    "                    \"name\": host_name,\n",
    "                    \"background\": host.get('background', 'مقدم برامج إذاعية متخصص'),\n",
    "                    \"speaking_style\": host.get('speaking_style', 'يتحدث بوضوح ويطرح أسئلة مدروسة')\n",
    "                },\n",
    "                \"guest\": {\n",
    "                    \"name\": guest_name,\n",
    "                    \"background\": guest.get('background', 'خبير متخصص في الموضوع'),\n",
    "                    \"speaking_style\": guest.get('speaking_style', 'يشرح بوضوح ويقدم أمثلة عملية')\n",
    "                }\n",
    "            },\n",
    "            \"conversation_flow\": {\n",
    "                \"intro1\": {\n",
    "                    \"opening_line\": f\"مرحباً بكم مستمعينا الكرام، معكم {host_name} في حلقة جديدة\",\n",
    "                    \"podcast_introduction\": \"نناقش اليوم موضوعاً مهماً يهم الجميع ويستحق التأمل\",\n",
    "                    \"episode_hook\": f\"موضوع حلقتنا اليوم هو {topic} وأثره على حياتنا\"\n",
    "                },\n",
    "                \"intro2\": {\n",
    "                    \"topic_introduction\": f\"سنتحدث اليوم عن {topic} وجوانبه المختلفة والمهمة\",\n",
    "                    \"guest_welcome\": f\"معي اليوم الضيف المتميز {guest_name}، أهلاً وسهلاً بك\",\n",
    "                    \"guest_bio_highlight\": f\"{guest_name} خبير متخصص في هذا المجال ولديه خبرة واسعة\"\n",
    "                },\n",
    "                \"main_discussion\": [\n",
    "                    {\n",
    "                        \"point_title\": \"الجانب الأول والأساسي للموضوع\",\n",
    "                        \"personal_angle\": \"كيف يؤثر هذا الموضوع على حياتنا اليومية وتجاربنا\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"point_title\": \"الجانب الثاني والتحديات المرتبطة\",\n",
    "                        \"personal_angle\": \"التحديات والفرص المتاحة في هذا المجال\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"point_title\": \"الجانب الثالث والحلول المقترحة\",\n",
    "                        \"personal_angle\": \"النصائح والتوجيهات العملية للمستقبل\"\n",
    "                    }\n",
    "                ],\n",
    "                \"closing\": {\n",
    "                    \"conclusion\": {\n",
    "                        \"main_takeaways\": \"الخلاصات المهمة والنقاط الأساسية من نقاشنا اليوم\",\n",
    "                        \"guest_final_message\": \"رسالة أخيرة ومهمة من الضيف لجمهور المستمعين\",\n",
    "                        \"host_closing_thoughts\": \"أفكار ختامية وتأملات من المقدم حول الموضوع\"\n",
    "                    },\n",
    "                    \"outro\": {\n",
    "                        \"guest_appreciation\": f\"شكراً جزيلاً {guest_name} على هذا النقاش المفيد والثري\",\n",
    "                        \"audience_thanks\": \"شكراً لكم مستمعينا الكرام على متابعتكم واهتمامكم\",\n",
    "                        \"call_to_action\": \"تفاعلوا معنا وشاركونا آراءكم عبر وسائل التواصل الاجتماعي\",\n",
    "                        \"final_goodbye\": \"إلى اللقاء في حلقة قادمة بإذن الله\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"cultural_context\": {\n",
    "                \"proverbs_sayings\": [\n",
    "                    \"العلم نور والجهل ظلام\",\n",
    "                    \"في التأني السلامة وفي العجلة الندامة\"\n",
    "                ],\n",
    "                \"regional_references\": [\n",
    "                    \"التجربة العربية الغنية في هذا المجال\",\n",
    "                    \"الخبرات المحلية والإقليمية ذات الصلة بالموضوع\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def validate_conversation_structure(self, structure_json):\n",
    "        \"\"\"Enhanced validation for conversation structure\"\"\"\n",
    "        required_keys = [\"episode_topic\", \"personas\", \"conversation_flow\", \"cultural_context\"]\n",
    "        \n",
    "        conversation_flow_required = [\"intro1\", \"intro2\", \"main_discussion\", \"closing\"]\n",
    "        intro1_required = [\"opening_line\", \"podcast_introduction\", \"episode_hook\"]\n",
    "        intro2_required = [\"topic_introduction\", \"guest_welcome\", \"guest_bio_highlight\"]\n",
    "        \n",
    "        try:\n",
    "            structure = json.loads(structure_json)\n",
    "            missing_keys = []\n",
    "            \n",
    "            # Check main structure\n",
    "            for key in required_keys:\n",
    "                if key not in structure:\n",
    "                    missing_keys.append(key)\n",
    "            \n",
    "            # Check conversation flow\n",
    "            if \"conversation_flow\" in structure:\n",
    "                conv_flow = structure[\"conversation_flow\"]\n",
    "                for key in conversation_flow_required:\n",
    "                    if key not in conv_flow:\n",
    "                        missing_keys.append(f\"conversation_flow.{key}\")\n",
    "                \n",
    "                # Check intro1\n",
    "                if \"intro1\" in conv_flow:\n",
    "                    intro1 = conv_flow[\"intro1\"]\n",
    "                    for key in intro1_required:\n",
    "                        if key not in intro1:\n",
    "                            missing_keys.append(f\"intro1.{key}\")\n",
    "                \n",
    "                # Check intro2\n",
    "                if \"intro2\" in conv_flow:\n",
    "                    intro2 = conv_flow[\"intro2\"]\n",
    "                    for key in intro2_required:\n",
    "                        if key not in intro2:\n",
    "                            missing_keys.append(f\"intro2.{key}\")\n",
    "                \n",
    "                # Check main discussion\n",
    "                if \"main_discussion\" in conv_flow:\n",
    "                    main_disc = conv_flow[\"main_discussion\"]\n",
    "                    if not isinstance(main_disc, list) or len(main_disc) < 3:\n",
    "                        missing_keys.append(\"main_discussion (need at least 3 points)\")\n",
    "                    else:\n",
    "                        for i, point in enumerate(main_disc):\n",
    "                            if \"point_title\" not in point:\n",
    "                                missing_keys.append(f\"main_discussion[{i}].point_title\")\n",
    "                            if \"personal_angle\" not in point:\n",
    "                                missing_keys.append(f\"main_discussion[{i}].personal_angle\")\n",
    "            \n",
    "            if missing_keys:\n",
    "                return False, f\"Missing required keys: {missing_keys}\"\n",
    "            \n",
    "            return True, \"Conversation structure validation successful\"\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            return False, f\"Invalid JSON format: {e}\"\n",
    "\n",
    "    def analyze_structure_quality(self, structure_json):\n",
    "        \"\"\"Enhanced quality analysis with Arabic-only validation\"\"\"\n",
    "        try:\n",
    "            structure = json.loads(structure_json)\n",
    "            \n",
    "            analysis = {\n",
    "                \"structure_completeness\": 0,\n",
    "                \"content_quality\": 0,\n",
    "                \"cultural_integration\": 0,\n",
    "                \"arabic_purity\": 0\n",
    "            }\n",
    "            \n",
    "            # Check completeness\n",
    "            conv_flow = structure.get(\"conversation_flow\", {})\n",
    "            completeness_indicators = [\n",
    "                bool(conv_flow.get(\"intro1\")),\n",
    "                bool(conv_flow.get(\"intro2\")),\n",
    "                bool(conv_flow.get(\"main_discussion\")),\n",
    "                bool(conv_flow.get(\"closing\")),\n",
    "                len(conv_flow.get(\"main_discussion\", [])) >= 3\n",
    "            ]\n",
    "            analysis[\"structure_completeness\"] = sum(completeness_indicators) * 20\n",
    "            \n",
    "            # Check content quality\n",
    "            intro1 = conv_flow.get(\"intro1\", {})\n",
    "            intro2 = conv_flow.get(\"intro2\", {})\n",
    "            quality_indicators = [\n",
    "                len(intro1.get(\"opening_line\", \"\")) > 15,\n",
    "                len(intro1.get(\"episode_hook\", \"\")) > 15,\n",
    "                len(intro2.get(\"topic_introduction\", \"\")) > 15,\n",
    "                len(intro2.get(\"guest_welcome\", \"\")) > 10\n",
    "            ]\n",
    "            analysis[\"content_quality\"] = sum(quality_indicators) * 25\n",
    "            \n",
    "            # Check cultural integration\n",
    "            cultural = structure.get(\"cultural_context\", {})\n",
    "            cultural_indicators = [\n",
    "                len(cultural.get(\"proverbs_sayings\", [])) >= 1,\n",
    "                len(cultural.get(\"regional_references\", [])) >= 1\n",
    "            ]\n",
    "            analysis[\"cultural_integration\"] = sum(cultural_indicators) * 50\n",
    "            \n",
    "            # Check Arabic purity\n",
    "            full_text = json.dumps(structure, ensure_ascii=False)\n",
    "            is_arabic_pure, _ = self._validate_arabic_only(full_text)\n",
    "            analysis[\"arabic_purity\"] = 100 if is_arabic_pure else 0\n",
    "            \n",
    "            # Calculate overall score\n",
    "            analysis[\"overall_score\"] = min(100, sum([\n",
    "                analysis[\"structure_completeness\"],\n",
    "                analysis[\"content_quality\"],\n",
    "                analysis[\"cultural_integration\"],\n",
    "                analysis[\"arabic_purity\"]\n",
    "            ]) // 4)\n",
    "            \n",
    "            analysis[\"quality_grade\"] = (\n",
    "                \"ممتاز\" if analysis[\"overall_score\"] >= 90 else\n",
    "                \"جيد جداً\" if analysis[\"overall_score\"] >= 80 else\n",
    "                \"جيد\" if analysis[\"overall_score\"] >= 70 else\n",
    "                \"مقبول\" if analysis[\"overall_score\"] >= 60 else\n",
    "                \"يحتاج تحسين\"\n",
    "            )\n",
    "            \n",
    "            analysis[\"ready_for_next_step\"] = analysis[\"overall_score\"] >= 75\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except:\n",
    "            return {\"error\": \"Could not analyze structure quality\"}\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_fixed_conversation_structure_generator(deployment, topic, information, classification_result, personas_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the fixed conversation structure generator with Arabic-only validation\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Fixed Conversation Structure Generator...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = FixedConversationStructureGenerator(deployment, model_name)\n",
    "    \n",
    "    # Run generation with validation\n",
    "    structure_result, parsed_result = generator.generate_conversation_structure_with_validation(\n",
    "        topic, information, classification_result, personas_result\n",
    "    )\n",
    "    \n",
    "    print(\"📋 Generated Structure:\")\n",
    "    print(f\"Episode Topic: {parsed_result.get('episode_topic', 'N/A')}\")\n",
    "    \n",
    "    # Show conversation flow\n",
    "    conv_flow = parsed_result.get(\"conversation_flow\", {})\n",
    "    intro1 = conv_flow.get(\"intro1\", {})\n",
    "    main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "    \n",
    "    print(f\"\\n🎬 Intro1 Opening: {intro1.get('opening_line', 'N/A')[:80]}...\")\n",
    "    print(f\"📝 Discussion Points: {len(main_discussion)}\")\n",
    "    for i, point in enumerate(main_discussion, 1):\n",
    "        print(f\"  {i}. {point.get('point_title', 'N/A')[:60]}...\")\n",
    "    \n",
    "    # Show cultural context\n",
    "    cultural = parsed_result.get(\"cultural_context\", {})\n",
    "    proverbs = cultural.get(\"proverbs_sayings\", [])\n",
    "    print(f\"\\n🏛️ Cultural Proverbs: {len(proverbs)}\")\n",
    "    for proverb in proverbs:\n",
    "        print(f\"  • {proverb}\")\n",
    "    \n",
    "    # Enhanced quality analysis\n",
    "    quality_analysis = generator.analyze_structure_quality(structure_result)\n",
    "    print(f\"\\n📈 Quality Analysis:\")\n",
    "    print(f\"Overall Score: {quality_analysis['overall_score']}/100\")\n",
    "    print(f\"Quality Grade: {quality_analysis['quality_grade']}\")\n",
    "    print(f\"Structure Completeness: {quality_analysis['structure_completeness']}/100\")\n",
    "    print(f\"Content Quality: {quality_analysis['content_quality']}/100\")\n",
    "    print(f\"Cultural Integration: {quality_analysis['cultural_integration']}/100\")\n",
    "    print(f\"Arabic Purity: {quality_analysis['arabic_purity']}/100\")\n",
    "    print(f\"Ready for Next Step: {'✅' if quality_analysis['ready_for_next_step'] else '❌'}\")\n",
    "    \n",
    "    # Arabic validation check\n",
    "    is_arabic_valid, arabic_message = generator._validate_arabic_only(structure_result)\n",
    "    print(f\"\\n🔍 Arabic Validation: {'✅' if is_arabic_valid else '❌'} {arabic_message}\")\n",
    "    \n",
    "    return structure_result, parsed_result\n",
    "\n",
    "# Usage:\n",
    "# generator = FixedConversationStructureGenerator(deployment, \"Fanar-C-1-8.7B\")\n",
    "# structure_result, parsed_result = generator.generate_conversation_structure_with_validation(topic, information, classification_result, personas_result)\n",
    "\n",
    "# Test the fixed generator\n",
    "# test_result = test_fixed_conversation_structure_generator(deployment, topic, information, classification_result, personas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09091744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Attempt 1: Arabic validation failed: English words detected\n",
      "⚠️ Attempt 2: Arabic validation failed: English words detected\n",
      "⚠️ Attempt 3: Arabic validation failed: English words detected\n",
      "📝 Using fallback conversation structure...\n",
      "Conversation Structure Result:\n",
      "{\n",
      "  \"episode_topic\": \"نقاش حول الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"د. فادي حسن\",\n",
      "      \"background\": \"أستاذ جامعي متخصص في اللغويات والحوسبة\",\n",
      "      \"speaking_style\": \"متحدث واضح وصريح مع تركيز على تقديم المعلومات بتوضيح مفصل.\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"مهندسة رانيا الصغير\",\n",
      "      \"background\": \"باحثة محترفة تعمل على تطوير نماذج الذكاء الاصطناعي باللغة العربية.\",\n",
      "      \"speaking_style\": \"متحدثة شغوفة تحب مشاركة أفكارها وتجاربها العملية.\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"مرحباً بكم مستمعينا الكرام، معكم د. فادي حسن في حلقة جديدة\",\n",
      "      \"podcast_introduction\": \"نناقش اليوم موضوعاً مهماً يهم الجميع ويستحق التأمل\",\n",
      "      \"episode_hook\": \"موضوع حلقتنا اليوم هو الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي وأثره على حياتنا\"\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"سنتحدث اليوم عن الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي وجوانبه المختلفة والمهمة\",\n",
      "      \"guest_welcome\": \"معي اليوم الضيف المتميز مهندسة رانيا الصغير، أهلاً وسهلاً بك\",\n",
      "      \"guest_bio_highlight\": \"مهندسة رانيا الصغير خبير متخصص في هذا المجال ولديه خبرة واسعة\"\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"الجانب الأول والأساسي للموضوع\",\n",
      "        \"personal_angle\": \"كيف يؤثر هذا الموضوع على حياتنا اليومية وتجاربنا\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثاني والتحديات المرتبطة\",\n",
      "        \"personal_angle\": \"التحديات والفرص المتاحة في هذا المجال\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثالث والحلول المقترحة\",\n",
      "        \"personal_angle\": \"النصائح والتوجيهات العملية للمستقبل\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"الخلاصات المهمة والنقاط الأساسية من نقاشنا اليوم\",\n",
      "        \"guest_final_message\": \"رسالة أخيرة ومهمة من الضيف لجمهور المستمعين\",\n",
      "        \"host_closing_thoughts\": \"أفكار ختامية وتأملات من المقدم حول الموضوع\"\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"شكراً جزيلاً مهندسة رانيا الصغير على هذا النقاش المفيد والثري\",\n",
      "        \"audience_thanks\": \"شكراً لكم مستمعينا الكرام على متابعتكم واهتمامكم\",\n",
      "        \"call_to_action\": \"تفاعلوا معنا وشاركونا آراءكم عبر وسائل التواصل الاجتماعي\",\n",
      "        \"final_goodbye\": \"إلى اللقاء في حلقة قادمة بإذن الله\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"العلم نور والجهل ظلام\",\n",
      "      \"في التأني السلامة وفي العجلة الندامة\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"التجربة العربية الغنية في هذا المجال\",\n",
      "      \"الخبرات المحلية والإقليمية ذات الصلة بالموضوع\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "🧪 Testing Fixed Conversation Structure Generator...\n",
      "============================================================\n",
      "⚠️ Attempt 1: Arabic validation failed: English words detected\n",
      "⚠️ Attempt 2: Arabic validation failed: English words detected\n",
      "⚠️ Attempt 3: Arabic validation failed: English words detected\n",
      "📝 Using fallback conversation structure...\n",
      "📋 Generated Structure:\n",
      "Episode Topic: نقاش حول الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\n",
      "\n",
      "🎬 Intro1 Opening: مرحباً بكم مستمعينا الكرام، معكم د. فادي حسن في حلقة جديدة...\n",
      "📝 Discussion Points: 3\n",
      "  1. الجانب الأول والأساسي للموضوع...\n",
      "  2. الجانب الثاني والتحديات المرتبطة...\n",
      "  3. الجانب الثالث والحلول المقترحة...\n",
      "\n",
      "🏛️ Cultural Proverbs: 2\n",
      "  • العلم نور والجهل ظلام\n",
      "  • في التأني السلامة وفي العجلة الندامة\n",
      "\n",
      "📈 Quality Analysis:\n",
      "Overall Score: 75/100\n",
      "Quality Grade: جيد\n",
      "Structure Completeness: 100/100\n",
      "Content Quality: 100/100\n",
      "Cultural Integration: 100/100\n",
      "Arabic Purity: 0/100\n",
      "Ready for Next Step: ✅\n",
      "\n",
      "🔍 Arabic Validation: ❌ English words detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize with Arabic-only focus\n",
    "generator = FixedConversationStructureGenerator(deployment, model)\n",
    "\n",
    "# Generate with validation\n",
    "structure_result, parsed_result = generator.generate_conversation_structure_with_validation(\n",
    "    topic, information, classification_result, personas_result\n",
    ")\n",
    "print(\"Conversation Structure Result:\")\n",
    "print(structure_result)\n",
    "# Test with comprehensive checks\n",
    "test_result = test_fixed_conversation_structure_generator(\n",
    "    deployment, topic, information, classification_result, personas_result\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c22de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Structure Result:\n",
      "{\n",
      "  \"episode_topic\": \"الذكاء الاصطناعي والهوية العربية: تحديات الحفاظ على تراثنا الثقافي في عالم رقمي متنامٍ\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"د. فادي حسن\",\n",
      "      \"background\": \"أستاذ جامعي ذو خلفية لغوية وحسابية غنية\",\n",
      "      \"speaking_style\": \"متفائل ومحفز مع اهتمام بالتفاصيل العلمية\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"مهندسة رانيا الصغير\",\n",
      "      \"background\": \"باحثة رائدة في مجال الذكاء الاصطناعي وتنمية النماذج البرمجية بالعربية\",\n",
      "      \"speaking_style\": \"جادّة ودقيقة ولكنها تشارِكة أيضًا بأمثلة حيوية من تجاربها العملية\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"مرحبا بكم جميعا! اليوم نستضيف المهندسة رانيا الصغير لمناقشة موضوع مهم جدا يتعلق بكيفية توافق التطور السريع للتقنيات الحديثة مع هويتنا الهوية الإسلامية والعربية.\",\n",
      "      \"podcast_introduction\": \"في هذه الحلقة الجديدة, سنعبر سويا عن آرائنا حول دور الذكاء الاصطناعي وأثر ذلك على حفظ قيمنا التقليدية وثقافتنا الغنية المقترنة برسالتنا الروحية.\",\n",
      "      \"episode_hook\": \"سنطرح أسئلة مثيرة كالتالي: هل يمكن أن يساعد الذكاء الاصطناعي بالفعل في إظهار جمال اللغة العربية الأصيلة أم أنه يشكل تهديدا لها؟ وهل هناك طرق لاستخدام هذا المجال الناشئ بشكل يخدم مصالح شعبنا ويحترم هويته الفريدة?\",\n",
      "      \"tone_guidance\": \"دعونا نسعى لحفظ جو مفعم بالأمل والأصالة رغم التعامل مع مواضيع قد تبدو مرهقة أو صعبة.\"\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"لنبدأ إذن باستقبال ضيفتنا المبجلة, مهندسة رانيا التي ستقدم لنا رؤى عميقة وخبرات فريدة حول مساعيها الرائعة لتطوير تقنيات برمجية مبتكرة تحت مظلة الالتزام بقيمنا الخاصة.\",\n",
      "      \"guest_welcome\": \"مرحبًا يا رانيا! شكرا لاتساقك معنا لهذا الجدل الهام.\",\n",
      "      \"guest_bio_highlight\": \"بالطبع, قبل الدخول مباشرة إلى الموضوع الرئيسي, دعينا نتذكر أهمية جهودك الثابتة لدمج منطلقات حضارية اسلامية في ابتكار منتجات رقمية ذات تأثير اجتماعي ايجابي.\",\n",
      "      \"transition_to_discussion\": \"مع فهم أفضل لمنظورك المتنوع, فلنتحول الآن إلى نقاشنا المركزي حول كيفية توظيف قوة الذكاء الاصطناعي لتحقيق هدف مشترك وهو تشكيل المستقبل بما يحقق العدالة والإنسانية ويتوافق مع توجهاتنا الدينية والثقافية.\"\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"تأثير الذكاء الاصطناعي على التواصل العربي\",\n",
      "        \"personal_angle\": \"من وجهتي النظر كاستاذ لغة عربية وعلم كمبيوتر, من المحتمل أن تتسبب بيئات الذكاء الاصطناعي الشائعة حاليًا بتغيير طبيعة المحادثة الطبيعية بين الأصدقاء والجيران – مما يؤدي لفقدان بعض الطبقات المعقدة للعلاقة الاجتماعية المميزة لوطننا العربي.\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"إمكانات الذكاء الاصطناعي لإبراز جمالية اللغة العربية\",\n",
      "        \"personal_angle\": \"لكن يجب أيضا الاعتراف بأن تكنولوجيا مثل التحليل اللغوي الضخم NLP لديها القدرة الهائلة للحفاظ على حرفية الشعر القديم وإعادة عرض القصائد الكلاسيكية بطريقة جذابة وجديدة تماماً للأجيال القادمة!\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"المسؤولية الأخلاقية تجاه تطبيقات الذكاء الاصطناعي\",\n",
      "        \"personal_angle\": \"وفي النهاية, لن ينسى المجتمع أبدا المسؤولية المشروعة المُلقاة علی عواتق المصممين الذين يستعينون بتلك الآلات المدربة جيدا; فلا بد لهم من وضع حدود واضحة تحمي خصوصية المستخدم وتمكنه من استخلاص الفائدة دون تعريض نفسه لأخطار محتملة بسبب الانتحال غير المرغوب فيه للمعارف المقدمة عبر الشبكات العنكبوتية الواسعة.\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"ولذلك تلخص خلاصة حديثنا اليوم في ضرورة العمل الجماعي نحو مستقبل أكثر إنصافاً حيث لا تنفصل التطورات التكنولوجيةعن منظومتناالفكريةوالروحيةبلتكونجزءلافتقمشروعالإصلاحالثوري.\",\n",
      "        \"guest_final_message\": \"وأخيرا وليس آخرا, ما رأيكِ أخيرآرانيا فيما إذا كان بالإمكان تطبيق أفكار مشابهة هنا داخل دولة الخليج العربي تحديداً?\" ,\n",
      "        \"host_closing_thoughts\": \"شكراً لك مرة أخرى لجهودك المثمرة في هذا الحوار الإبداعي الذي أعطانا الكثير للمشاركة به مستقبلاً وللتفكيرفيه ملياً !\"\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"شكرا جزيلا لكمجميعالسامعينعلىانتباههملطرحاسبتتنااليوم!\",\n",
      "        \"audience_thanks\": \"نشكركمأيضا علي حضوركممستمرللوقوفمعناهذه الرحلةالعلميةالغنيةبالإكتشافاتجديدة!\",\n",
      "        \"call_to_action\": \"ولا تنسواالاكتتابفي خدمة البودكاست لدينا للتواصلفضائيامعالمزيدمنالحلقاتمثلهذهالقادمـة!\",\n",
      "        \"final_goodbye\": \"إلىلقاءٱخرسلام عليكم ورحمة الله وبركاته ! \"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\"قال رسول الإسلام صلى الله عليه وسلم:'إنَّ العُلَمَاءَ ورثةُ الأنبياء'\", \"كما قال أبو الطيب المتنبي:\"ليس كل من يزور بيتكم صديقا\"],\n",
      "    \"regional_references\": [\"تاريخ الرياضيات والفلسفة العربيين\",\"تجربتها الشخصية أثناء زيارة جامعة قطر\"]\n",
      "  },\n",
      "  \"language_style\": {\n",
      "    \"formality_level\": \"رسمي لكنه سهل الاستيعاب ومعبر عن شخصية الشخصيات \",\n",
      "    \"dialect_touches\": \"بعض المفردات العامية العربية خفيفة لمساعدة السياق\",\n",
      "    \"vocabulary_richness\": \"مواد شاعرية ولغة علمية متنوعة\"\n",
      "  },\n",
      "  \"technical_notes\": {\n",
      "    \"pacing_guidance\": \"تسارع الخطاب قليلا عند الوصول لعناصر الجدلية الرئيسية ,سرعته المعتادة خلال المواقف التمهيدية والنهائية\",\n",
      "    \"pause_points\": \"[ بعد نقاط العنوان الرئيسيه ]\",\n",
      "    \"emphasis_moments\": \"الكلمات المفتاحية مثل : 'الهوية' ,'الأصالة','جودة الحياة',\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cece3b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Minimal enhancement (fastest, most concise)\\nenhancer_minimal = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"minimal\")\\nresult_minimal = enhancer_minimal.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Standard enhancement (balanced)\\nenhancer_standard = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"standard\")\\nresult_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Full enhancement (comprehensive but largest)\\nenhancer_full = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"full\")\\nresult_full = enhancer_full.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\\n\\n# Test with specific level\\nenhanced_result = test_enhanced_dialogue_content_enhancer(\\n    deployment, topic, information, classification_result, personas_result, structure_result, \\n    model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"\\n)\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "class SectionalDialogueContentEnhancer:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\", enhancement_level=\"minimal\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "        self.enhancement_level = enhancement_level  # \"minimal\", \"standard\", \"full\"\n",
    "\n",
    "    def enhance_intro_sections(self, topic, classification_result, personas_result, intro1, intro2):\n",
    "        \"\"\"\n",
    "        Chunk 1: Enhance intro1 and intro2 sections (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "        \n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"2\"\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"3\"\n",
    "        else:  # full\n",
    "            intro1_fields = \"spontaneity_elements\"\n",
    "            intro2_fields = \"cultural_connections\"\n",
    "            item_count = \"3-4\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast introductions.\n",
    "\n",
    "Task: Enhance ONLY the intro sections with natural dialogue elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current intro1: {json.dumps(intro1, ensure_ascii=False)}\n",
    "Current intro2: {json.dumps(intro2, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "\n",
    "For intro1, ADD this field:\n",
    "- \"{intro1_fields}\": [{item_count} natural spontaneous phrases that the host might use when opening, in MSA]\n",
    "\n",
    "For intro2, ADD this field:  \n",
    "- \"{intro2_fields}\": [{item_count} ways to connect this topic to Arab culture/values, in MSA]\n",
    "\n",
    "Return the enhanced sections in this exact format:\n",
    "{{\n",
    "    \"intro1\": {{\n",
    "        [keep all existing intro1 fields],\n",
    "        \"{intro1_fields}\": [new content]\n",
    "    }},\n",
    "    \"intro2\": {{\n",
    "        [keep all existing intro2 fields],\n",
    "        \"{intro2_fields}\": [new content]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing content unchanged\n",
    "- Add only the specified new fields\n",
    "- All new values in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content specific to topic: {topic}\n",
    "- Match the {optimal_style} style\n",
    "- Keep arrays short and impactful\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast intros. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_main_discussion_point(self, topic, classification_result, personas_result, discussion_point, point_index):\n",
    "        \"\"\"\n",
    "        Chunk 2: Enhance individual main discussion points (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"]\"\"\"\n",
    "            field_count = \"2\"\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"],\n",
    "    \"natural_transitions\": \"transition phrase in MSA\\\"\"\"\"\n",
    "            field_count = \"3\"\n",
    "        else:  # full\n",
    "            enhancement_fields = \"\"\"\n",
    "    \"spontaneous_triggers\": [\"trigger 1 in MSA\", \"trigger 2 in MSA\"],\n",
    "    \"disagreement_points\": \"disagreement description in MSA\",\n",
    "    \"cultural_references\": [\"reference 1 in MSA\", \"reference 2 in MSA\"],\n",
    "    \"natural_transitions\": \"transition phrase in MSA\",\n",
    "    \"emotional_triggers\": \"emotional description in MSA\\\"\"\"\"\n",
    "            field_count = \"5\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast discussion points.\n",
    "\n",
    "Task: Enhance ONE discussion point with rich dialogue elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Point #{point_index + 1}\n",
    "\n",
    "Current discussion point: {json.dumps(discussion_point, ensure_ascii=False)}\n",
    "\n",
    "Add EXACTLY these {field_count} fields. Keep all existing fields unchanged:\n",
    "\n",
    "{{\n",
    "    [all existing fields from discussion_point],{enhancement_fields}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing fields exactly as they are\n",
    "- Add only the {field_count} new fields shown above\n",
    "- All new content in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation ONLY (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content relevant to topic: {topic}\n",
    "- Keep responses concise and actionable\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast discussion points. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_closing_sections(self, topic, classification_result, personas_result, closing_section):\n",
    "        \"\"\"\n",
    "        Chunk 3: Enhance closing (conclusion + outro) sections (REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            conclusion_fields = '\"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\"'\n",
    "            outro_fields = '\"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\"'\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            conclusion_fields = '''\n",
    "        \"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\",\n",
    "        \"key_insights\": [\"insight 1 in MSA\", \"insight 2 in MSA\"]'''\n",
    "            outro_fields = '\"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\"'\n",
    "        else:  # full\n",
    "            conclusion_fields = '''\n",
    "        \"emotional_closure\": \"how to create emotional satisfaction for listeners, in MSA\",\n",
    "        \"key_insights\": [2-3 key insights that should be highlighted in the wrap-up, in MSA]'''\n",
    "            outro_fields = '''\n",
    "        \"memorable_ending\": \"a memorable way to end that listeners will remember, in MSA\",\n",
    "        \"connection_building\": \"ways to build ongoing connection with the audience, in MSA\"'''\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast closings.\n",
    "\n",
    "Task: Enhance the closing section with natural wrap-up elements.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current closing: {json.dumps(closing_section, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "\n",
    "For conclusion subsection, ADD:\n",
    "{conclusion_fields}\n",
    "\n",
    "For outro subsection, ADD:\n",
    "{outro_fields}\n",
    "\n",
    "Return enhanced closing in this exact format:\n",
    "{{\n",
    "    \"conclusion\": {{\n",
    "        [keep all existing conclusion fields],\n",
    "        {conclusion_fields}\n",
    "    }},\n",
    "    \"outro\": {{\n",
    "        [keep all existing outro fields],\n",
    "        {outro_fields}\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Keep ALL existing content unchanged\n",
    "- Add only the specified new fields\n",
    "- All new values in Modern Standard Arabic (MSA)\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Make content feel conclusive and satisfying\n",
    "- Keep insights concise and actionable\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance Arabic podcast closings. Style: {optimal_style}. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def create_global_elements(self, topic, classification_result, personas_result):\n",
    "        \"\"\"\n",
    "        Chunk 4: Create global elements (SIMPLIFIED AND REDUCED)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        cultural_sensitivity = classification.get(\"cultural_sensitivity_level\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        # Determine enhancement scope based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\"\n",
    "        ],\n",
    "        \"personal_stories\": [\n",
    "            \"first personal story in MSA\",\n",
    "            \"second personal story in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\"\n",
    "        ],\n",
    "        \"storytelling_moments\": [\n",
    "            \"first storytelling moment in MSA\",\n",
    "            \"second storytelling moment in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "        else:  # full\n",
    "            structure = '''\n",
    "{\n",
    "    \"spontaneous_moments\": {\n",
    "        \"natural_interruptions\": [\n",
    "            \"first natural interruption in MSA\",\n",
    "            \"second natural interruption in MSA\",\n",
    "            \"third natural interruption in MSA\"\n",
    "        ],\n",
    "        \"emotional_reactions\": [\n",
    "            \"first emotional reaction in MSA\",\n",
    "            \"second emotional reaction in MSA\", \n",
    "            \"third emotional reaction in MSA\"\n",
    "        ],\n",
    "        \"personal_stories\": [\n",
    "            \"first personal story in MSA\",\n",
    "            \"second personal story in MSA\"\n",
    "        ],\n",
    "        \"humorous_moments\": [\n",
    "            \"first humorous moment in MSA\",\n",
    "            \"second humorous moment in MSA\"\n",
    "        ]\n",
    "    },\n",
    "    \"personality_interactions\": {\n",
    "        \"host_strengths\": \"host strengths description in MSA\",\n",
    "        \"guest_expertise\": \"guest expertise description in MSA\",\n",
    "        \"natural_chemistry\": \"chemistry description in MSA\",\n",
    "        \"tension_points\": \"tension points description in MSA\",\n",
    "        \"collaboration_moments\": \"collaboration description in MSA\"\n",
    "    },\n",
    "    \"dialogue_techniques\": {\n",
    "        \"questioning_styles\": [\n",
    "            \"first questioning style in MSA\",\n",
    "            \"second questioning style in MSA\",\n",
    "            \"third questioning style in MSA\"\n",
    "        ],\n",
    "        \"storytelling_moments\": [\n",
    "            \"first storytelling moment in MSA\",\n",
    "            \"second storytelling moment in MSA\"\n",
    "        ],\n",
    "        \"audience_engagement\": [\n",
    "            \"first engagement technique in MSA\",\n",
    "            \"second engagement technique in MSA\",\n",
    "            \"third engagement technique in MSA\"\n",
    "        ],\n",
    "        \"emotional_peaks\": [\n",
    "            \"first emotional peak in MSA\",\n",
    "            \"second emotional peak in MSA\"\n",
    "        ]\n",
    "    }\n",
    "}'''\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in creating global dialogue elements for Arabic podcasts.\n",
    "\n",
    "Task: Create global sections that enhance the overall conversation flow.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "Style: {optimal_style}\n",
    "Enhancement Level: {self.enhancement_level}\n",
    "\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Create EXACTLY this JSON structure with proper English punctuation:\n",
    "\n",
    "{structure}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Replace placeholder text with actual content in Modern Standard Arabic (MSA)\n",
    "- Use ONLY English commas (,) and standard quotes (\")\n",
    "- NO Arabic commas (،) or special punctuation\n",
    "- NO extra text before or after JSON\n",
    "- NO explanatory text\n",
    "- Make content specific to {host_name}, {guest_name}, and topic: {topic}\n",
    "- Follow the EXACT structure shown above\n",
    "- Keep content concise and actionable\n",
    "- Ensure all arrays have exactly the specified number of items\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You create global dialogue elements. Return ONLY valid JSON with English punctuation. No Arabic commas. No extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_dialogue_content(self, topic, information, classification_result, personas_result, structure_result):\n",
    "        \"\"\"\n",
    "        Main orchestration method: Coordinates all chunks with configurable enhancement levels\n",
    "        \"\"\"\n",
    "        print(f\"🔧 Starting sectional dialogue enhancement (Level: {self.enhancement_level})...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            structure = json.loads(structure_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid structure JSON provided\")\n",
    "        \n",
    "        # Extract sections\n",
    "        conv_flow = structure.get(\"conversation_flow\", {})\n",
    "        intro1 = conv_flow.get(\"intro1\", {})\n",
    "        intro2 = conv_flow.get(\"intro2\", {})\n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        closing = conv_flow.get(\"closing\", {})\n",
    "        \n",
    "        # Chunk 1: Enhance intro sections\n",
    "        print(\"📝 Chunk 1: Enhancing intro sections...\")\n",
    "        try:\n",
    "            enhanced_intros_json = self.enhance_intro_sections(\n",
    "                topic, classification_result, personas_result, intro1, intro2\n",
    "            )\n",
    "            enhanced_intros = json.loads(enhanced_intros_json)\n",
    "            \n",
    "            # Update structure\n",
    "            structure[\"conversation_flow\"][\"intro1\"].update(enhanced_intros.get(\"intro1\", {}))\n",
    "            structure[\"conversation_flow\"][\"intro2\"].update(enhanced_intros.get(\"intro2\", {}))\n",
    "            print(\"✅ Intro sections enhanced successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing intros: {e}\")\n",
    "        \n",
    "        # Small delay between chunks\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 2: Enhance main discussion points (one by one)\n",
    "        print(\"📝 Chunk 2: Enhancing main discussion points...\")\n",
    "        enhanced_discussion_points = []\n",
    "        \n",
    "        for i, point in enumerate(main_discussion):\n",
    "            print(f\"  Enhancing discussion point {i+1}/{len(main_discussion)}...\")\n",
    "            try:\n",
    "                enhanced_point_json = self.enhance_main_discussion_point(\n",
    "                    topic, classification_result, personas_result, point, i\n",
    "                )\n",
    "                enhanced_point = json.loads(enhanced_point_json)\n",
    "                enhanced_discussion_points.append(enhanced_point)\n",
    "                print(f\"  ✅ Point {i+1} enhanced successfully\")\n",
    "                \n",
    "                # Small delay between points\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Error enhancing point {i+1}: {e}\")\n",
    "                print(f\"  🔄 Using fallback enhancement for point {i+1}...\")\n",
    "                \n",
    "                # Try fallback enhancement for this point\n",
    "                enhanced_point = self._create_fallback_discussion_point(\n",
    "                    topic, classification_result, personas_result, point, i\n",
    "                )\n",
    "                enhanced_discussion_points.append(enhanced_point)\n",
    "                print(f\"  ✅ Point {i+1} enhanced with fallback method\")\n",
    "        \n",
    "        # Update structure with enhanced discussion points\n",
    "        structure[\"conversation_flow\"][\"main_discussion\"] = enhanced_discussion_points\n",
    "        print(\"✅ All main discussion points processed\")\n",
    "        \n",
    "        # Small delay between chunks\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 3: Enhance closing sections\n",
    "        print(\"📝 Chunk 3: Enhancing closing sections...\")\n",
    "        try:\n",
    "            enhanced_closing_json = self.enhance_closing_sections(\n",
    "                topic, classification_result, personas_result, closing\n",
    "            )\n",
    "            enhanced_closing = json.loads(enhanced_closing_json)\n",
    "            \n",
    "            # Update structure\n",
    "            structure[\"conversation_flow\"][\"closing\"].update(enhanced_closing)\n",
    "            print(\"✅ Closing sections enhanced successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing closing: {e}\")\n",
    "        \n",
    "        # Small delay between chunks  \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 4: Create global elements\n",
    "        print(\"📝 Chunk 4: Creating global elements...\")\n",
    "        try:\n",
    "            global_elements_json = self.create_global_elements(\n",
    "                topic, classification_result, personas_result\n",
    "            )\n",
    "            global_elements = json.loads(global_elements_json)\n",
    "            \n",
    "            # Add global elements to structure\n",
    "            structure.update(global_elements)\n",
    "            print(\"✅ Global elements created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error creating global elements: {e}\")\n",
    "            print(\"🔄 Attempting to create fallback global elements...\")\n",
    "            \n",
    "            # Create fallback global elements\n",
    "            try:\n",
    "                fallback_elements = self._create_fallback_global_elements(\n",
    "                    topic, classification_result, personas_result\n",
    "                )\n",
    "                structure.update(fallback_elements)\n",
    "                print(\"✅ Fallback global elements created successfully\")\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"⚠️ Fallback also failed: {fallback_error}\")\n",
    "                print(\"📝 Using minimal default global elements...\")\n",
    "                # Add minimal default elements so validation doesn't fail\n",
    "                structure.update(self._get_minimal_global_elements())\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(f\"🎉 Sectional dialogue enhancement completed! (Level: {self.enhancement_level})\")\n",
    "        \n",
    "        return json.dumps(structure, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Helper method to clean JSON response - Enhanced version\"\"\"\n",
    "        response = response.strip()\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Replace Arabic commas and punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('،', ',')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        # Remove trailing commas before closing braces/brackets\n",
    "        import re\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        # Ensure proper quote escaping\n",
    "        clean_json = clean_json.replace('\\\\\"', '\"')\n",
    "        \n",
    "        return clean_json\n",
    "\n",
    "    def validate_enhanced_content(self, enhanced_json):\n",
    "        \"\"\"Validate the enhanced dialogue content (adapted for different levels)\"\"\"\n",
    "        try:\n",
    "            enhanced = json.loads(enhanced_json)\n",
    "            \n",
    "            missing_elements = []\n",
    "            \n",
    "            # Check global sections (varies by level)\n",
    "            if self.enhancement_level == \"minimal\":\n",
    "                required_global = [\"spontaneous_moments\", \"dialogue_techniques\"]\n",
    "            elif self.enhancement_level == \"standard\":\n",
    "                required_global = [\"spontaneous_moments\", \"dialogue_techniques\"]\n",
    "            else:  # full\n",
    "                required_global = [\"spontaneous_moments\", \"personality_interactions\", \"dialogue_techniques\"]\n",
    "                \n",
    "            for element in required_global:\n",
    "                if element not in enhanced:\n",
    "                    missing_elements.append(element)\n",
    "            \n",
    "            # Check enhanced conversation flow\n",
    "            conv_flow = enhanced.get(\"conversation_flow\", {})\n",
    "            \n",
    "            # Check intro1 enhancements\n",
    "            intro1 = conv_flow.get(\"intro1\", {})\n",
    "            if \"spontaneity_elements\" not in intro1:\n",
    "                missing_elements.append(\"intro1.spontaneity_elements\")\n",
    "            \n",
    "            # Check intro2 enhancements  \n",
    "            intro2 = conv_flow.get(\"intro2\", {})\n",
    "            if \"cultural_connections\" not in intro2:\n",
    "                missing_elements.append(\"intro2.cultural_connections\")\n",
    "            \n",
    "            # Check main discussion enhancements (varies by level)\n",
    "            main_disc = conv_flow.get(\"main_discussion\", [])\n",
    "            if self.enhancement_level == \"minimal\":\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"cultural_references\"]\n",
    "            elif self.enhancement_level == \"standard\":\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"cultural_references\", \"natural_transitions\"]\n",
    "            else:  # full\n",
    "                required_point_fields = [\"spontaneous_triggers\", \"disagreement_points\", \"cultural_references\", \"natural_transitions\", \"emotional_triggers\"]\n",
    "            \n",
    "            for i, point in enumerate(main_disc):\n",
    "                for field in required_point_fields:\n",
    "                    if field not in point:\n",
    "                        missing_elements.append(f\"main_discussion[{i}].{field}\")\n",
    "            \n",
    "            # Check closing enhancements (varies by level)\n",
    "            closing = conv_flow.get(\"closing\", {})\n",
    "            conclusion = closing.get(\"conclusion\", {})\n",
    "            outro = closing.get(\"outro\", {})\n",
    "            \n",
    "            if \"emotional_closure\" not in conclusion:\n",
    "                missing_elements.append(\"closing.conclusion.emotional_closure\")\n",
    "            if \"memorable_ending\" not in outro:\n",
    "                missing_elements.append(\"closing.outro.memorable_ending\")\n",
    "                \n",
    "            # Additional checks for standard/full levels\n",
    "            if self.enhancement_level in [\"standard\", \"full\"]:\n",
    "                if \"key_insights\" not in conclusion:\n",
    "                    missing_elements.append(\"closing.conclusion.key_insights\")\n",
    "            if self.enhancement_level == \"full\":\n",
    "                if \"connection_building\" not in outro:\n",
    "                    missing_elements.append(\"closing.outro.connection_building\")\n",
    "            \n",
    "            if missing_elements:\n",
    "                return False, f\"Missing enhanced elements: {missing_elements}\"\n",
    "            \n",
    "            return True, f\"Sectional dialogue content enhancement validation successful (Level: {self.enhancement_level})\"\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            return False, \"Invalid JSON format in enhanced content\"\n",
    "\n",
    "    def _create_fallback_discussion_point(self, topic, classification_result, personas_result, discussion_point, point_index):\n",
    "        \"\"\"Create fallback enhancement for a single discussion point (level-aware)\"\"\"\n",
    "        enhanced_point = discussion_point.copy()\n",
    "        \n",
    "        # Add minimal enhancements based on level\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"هذا يثير تساؤلاً مهماً\",\n",
    "                    \"دعني أشارككم تجربة في هذا المجال\"\n",
    "                ],\n",
    "                \"cultural_references\": [\n",
    "                    \"كما يقول المثل: العلم نور\",\n",
    "                    \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
    "                ]\n",
    "            })\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"هذا يثير تساؤلاً مهماً\",\n",
    "                    \"دعني أشارككم تجربة في هذا المجال\"\n",
    "                ],\n",
    "                \"cultural_references\": [\n",
    "                    \"كما يقول المثل: العلم نور\",\n",
    "                    \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
    "                ],\n",
    "                \"natural_transitions\": \"هذا يقودنا إلى نقطة مهمة أخرى\"\n",
    "            })\n",
    "        else:  # full\n",
    "            enhanced_point.update({\n",
    "                \"spontaneous_triggers\": [\n",
    "                    \"هذا يثير تساؤلاً مهماً\",\n",
    "                    \"دعني أشارككم تجربة في هذا المجال\"\n",
    "                ],\n",
    "                \"disagreement_points\": \"قد تختلف وجهات النظر حول أفضل طريقة للتعامل مع هذه القضية\",\n",
    "                \"cultural_references\": [\n",
    "                    \"كما يقول المثل: العلم نور\",\n",
    "                    \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
    "                ],\n",
    "                \"natural_transitions\": \"هذا يقودنا إلى نقطة مهمة أخرى\",\n",
    "                \"emotional_triggers\": \"هذا الموضوع يلامس قلوب كل من يهتم بمستقبل ثقافتنا\"\n",
    "            })\n",
    "        \n",
    "        return enhanced_point\n",
    "\n",
    "    def _create_fallback_global_elements(self, topic, classification_result, personas_result):\n",
    "        \"\"\"Create fallback global elements based on enhancement level\"\"\"\n",
    "        try:\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            personas = {}\n",
    "        \n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "        \n",
    "        fallback_elements = {\n",
    "            \"spontaneous_moments\": {\n",
    "                \"natural_interruptions\": [\n",
    "                    \"اسمحوا لي أن أضيف نقطة هنا\",\n",
    "                    \"هذا يذكرني بموقف مشابه\"\n",
    "                ],\n",
    "                \"emotional_reactions\": [\n",
    "                    \"هذا مؤثر فعلاً\",\n",
    "                    \"لم أفكر في الأمر من هذه الزاوية\"\n",
    "                ]\n",
    "            },\n",
    "            \"dialogue_techniques\": {\n",
    "                \"questioning_styles\": [\n",
    "                    \"أسئلة مفتوحة لتعميق النقاش\",\n",
    "                    \"أسئلة تحليلية للوصول للجذور\"\n",
    "                ],\n",
    "                \"audience_engagement\": [\n",
    "                    \"طرح أسئلة يفكر فيها المستمع\",\n",
    "                    \"استخدام أمثلة من الواقع\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add more elements for standard/full levels\n",
    "        if self.enhancement_level in [\"standard\", \"full\"]:\n",
    "            fallback_elements[\"spontaneous_moments\"][\"personal_stories\"] = [\n",
    "                \"أتذكر موقفاً مشابهاً حدث معي\",\n",
    "                \"في تجربتي الشخصية وجدت أن\"\n",
    "            ]\n",
    "            fallback_elements[\"dialogue_techniques\"][\"storytelling_moments\"] = [\n",
    "                \"سرد تجارب شخصية ذات صلة\",\n",
    "                \"قصص نجاح ملهمة\"\n",
    "            ]\n",
    "        \n",
    "        if self.enhancement_level == \"full\":\n",
    "            fallback_elements[\"spontaneous_moments\"][\"humorous_moments\"] = [\n",
    "                \"هذا يذكرني بنكتة لطيفة\",\n",
    "                \"الموقف له جانب طريف\"\n",
    "            ]\n",
    "            fallback_elements[\"personality_interactions\"] = {\n",
    "                \"host_strengths\": f\"{host_name} ماهر في طرح الأسئلة المناسبة وتوجيه الحوار\",\n",
    "                \"guest_expertise\": f\"{guest_name} يقدم معرفة عميقة في مجال تخصصه\",\n",
    "                \"natural_chemistry\": \"يتفاعل المقدم والضيف بطريقة طبيعية ومريحة\",\n",
    "                \"tension_points\": \"قد يختلفان في بعض وجهات النظر مما يثري النقاش\",\n",
    "                \"collaboration_moments\": \"يبنيان على أفكار بعضهما البعض لإثراء المحتوى\"\n",
    "            }\n",
    "            fallback_elements[\"dialogue_techniques\"][\"emotional_peaks\"] = [\n",
    "                \"لحظات تأملية عميقة\",\n",
    "                \"قصص مؤثرة تلامس القلب\"\n",
    "            ]\n",
    "        \n",
    "        return fallback_elements\n",
    "\n",
    "    def _get_minimal_global_elements(self):\n",
    "        \"\"\"Return minimal default global elements\"\"\"\n",
    "        if self.enhancement_level == \"minimal\":\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"اسمحوا لي أن أضيف نقطة هنا\",\n",
    "                        \"هذا يذكرني بموقف مشابه\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"هذا مؤثر فعلاً\",\n",
    "                        \"لم أفكر في الأمر من هذه الزاوية\"\n",
    "                    ]\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"أسئلة مفتوحة لتعميق النقاش\",\n",
    "                        \"أسئلة تحليلية للوصول للجذور\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"طرح أسئلة يفكر فيها المستمع\",\n",
    "                        \"استخدام أمثلة من الواقع\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        elif self.enhancement_level == \"standard\":\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"اسمحوا لي أن أضيف نقطة هنا\",\n",
    "                        \"هذا يذكرني بموقف مشابه\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"هذا مؤثر فعلاً\",\n",
    "                        \"لم أفكر في الأمر من هذه الزاوية\"\n",
    "                    ],\n",
    "                    \"personal_stories\": [\n",
    "                        \"أتذكر موقفاً مشابهاً حدث معي\",\n",
    "                        \"في تجربتي الشخصية وجدت أن\"\n",
    "                    ]\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"أسئلة مفتوحة لتعميق النقاش\",\n",
    "                        \"أسئلة تحليلية للوصول للجذور\"\n",
    "                    ],\n",
    "                    \"storytelling_moments\": [\n",
    "                        \"سرد تجارب شخصية ذات صلة\",\n",
    "                        \"قصص نجاح ملهمة\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"طرح أسئلة يفكر فيها المستمع\",\n",
    "                        \"استخدام أمثلة من الواقع\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        else:  # full\n",
    "            return {\n",
    "                \"spontaneous_moments\": {\n",
    "                    \"natural_interruptions\": [\n",
    "                        \"اسمحوا لي أن أضيف نقطة هنا\",\n",
    "                        \"هذا يذكرني بموقف مشابه\",\n",
    "                        \"انتظر، هذا مهم جداً\"\n",
    "                    ],\n",
    "                    \"emotional_reactions\": [\n",
    "                        \"هذا مؤثر فعلاً\",\n",
    "                        \"لم أفكر في الأمر من هذه الزاوية\",\n",
    "                        \"أتفق معك تماماً\"\n",
    "                    ],\n",
    "                    \"personal_stories\": [\n",
    "                        \"أتذكر موقفاً مشابهاً حدث معي\",\n",
    "                        \"في تجربتي الشخصية وجدت أن\"\n",
    "                    ],\n",
    "                    \"humorous_moments\": [\n",
    "                        \"هذا يذكرني بنكتة لطيفة\",\n",
    "                        \"الموقف له جانب طريف\"\n",
    "                    ]\n",
    "                },\n",
    "                \"personality_interactions\": {\n",
    "                    \"host_strengths\": \"المقدم ماهر في طرح الأسئلة المناسبة وتوجيه الحوار\",\n",
    "                    \"guest_expertise\": \"الضيف يقدم معرفة عميقة في مجال تخصصه\",\n",
    "                    \"natural_chemistry\": \"يتفاعل المقدم والضيف بطريقة طبيعية ومريحة\",\n",
    "                    \"tension_points\": \"قد يختلفان في بعض وجهات النظر مما يثري النقاش\",\n",
    "                    \"collaboration_moments\": \"يبنيان على أفكار بعضهما البعض لإثراء المحتوى\"\n",
    "                },\n",
    "                \"dialogue_techniques\": {\n",
    "                    \"questioning_styles\": [\n",
    "                        \"أسئلة مفتوحة لتعميق النقاش\",\n",
    "                        \"أسئلة تحليلية للوصول للجذور\",\n",
    "                        \"أسئلة شخصية لإضافة البعد الإنساني\"\n",
    "                    ],\n",
    "                    \"storytelling_moments\": [\n",
    "                        \"سرد تجارب شخصية ذات صلة\",\n",
    "                        \"قصص نجاح ملهمة\"\n",
    "                    ],\n",
    "                    \"audience_engagement\": [\n",
    "                        \"طرح أسئلة يفكر فيها المستمع\",\n",
    "                        \"استخدام أمثلة من الواقع\",\n",
    "                        \"دعوة المستمعين للتفاعل\"\n",
    "                    ],\n",
    "                    \"emotional_peaks\": [\n",
    "                        \"لحظات تأملية عميقة\",\n",
    "                        \"قصص مؤثرة تلامس القلب\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Enhanced Testing Function with Level Selection\n",
    "def test_enhanced_dialogue_content_enhancer(deployment, topic, information, classification_result, personas_result, structure_result, model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"):\n",
    "    \"\"\"\n",
    "    Test the enhanced dialogue content enhancer with level selection\n",
    "    \"\"\"\n",
    "    print(f\"🧪 Testing Enhanced Dialogue Content Enhancer (Level: {enhancement_level})...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    enhancer = SectionalDialogueContentEnhancer(deployment, model_name, enhancement_level)\n",
    "    \n",
    "    # Run enhancement\n",
    "    enhanced_result = enhancer.enhance_dialogue_content(\n",
    "        topic, information, classification_result, personas_result, structure_result\n",
    "    )\n",
    "    \n",
    "    # Validate enhanced content\n",
    "    is_valid, validation_message = enhancer.validate_enhanced_content(enhanced_result)\n",
    "    \n",
    "    print(f\"\\n📊 Enhancement Results (Level: {enhancement_level}):\")\n",
    "    print(f\"Validation: {'✅ Valid' if is_valid else '❌ Invalid'}\")\n",
    "    print(f\"Message: {validation_message}\")\n",
    "    \n",
    "    # Quick content analysis\n",
    "    try:\n",
    "        enhanced_data = json.loads(enhanced_result)\n",
    "        \n",
    "        # Count enhancement fields\n",
    "        conv_flow = enhanced_data.get(\"conversation_flow\", {})\n",
    "        intro1_enhancements = len([k for k in conv_flow.get(\"intro1\", {}).keys() if k not in [\"opening_line\", \"podcast_introduction\", \"episode_hook\", \"tone_guidance\"]])\n",
    "        intro2_enhancements = len([k for k in conv_flow.get(\"intro2\", {}).keys() if k not in [\"topic_introduction\", \"guest_welcome\", \"guest_bio_highlight\", \"transition_to_discussion\"]])\n",
    "        \n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        discussion_enhancements = 0\n",
    "        for point in main_discussion:\n",
    "            discussion_enhancements += len([k for k in point.keys() if k not in [\"point_title\", \"personal_angle\"]])\n",
    "        \n",
    "        closing_enhancements = 0\n",
    "        closing = conv_flow.get(\"closing\", {})\n",
    "        conclusion = closing.get(\"conclusion\", {})\n",
    "        outro = closing.get(\"outro\", {})\n",
    "        closing_enhancements += len([k for k in conclusion.keys() if k not in [\"main_takeaways\", \"guest_final_message\", \"host_closing_thoughts\"]])\n",
    "        closing_enhancements += len([k for k in outro.keys() if k not in [\"guest_appreciation\", \"audience_thanks\", \"call_to_action\", \"final_goodbye\"]])\n",
    "        \n",
    "        global_sections = len([k for k in enhanced_data.keys() if k not in [\"episode_topic\", \"personas\", \"conversation_flow\", \"cultural_context\", \"language_style\", \"technical_notes\"]])\n",
    "        \n",
    "        print(f\"\\n📈 Enhancement Statistics:\")\n",
    "        print(f\"Intro1 Enhancements: {intro1_enhancements}\")\n",
    "        print(f\"Intro2 Enhancements: {intro2_enhancements}\")\n",
    "        print(f\"Discussion Enhancements: {discussion_enhancements}\")\n",
    "        print(f\"Closing Enhancements: {closing_enhancements}\")\n",
    "        print(f\"Global Sections Added: {global_sections}\")\n",
    "        \n",
    "        # Estimate size reduction vs original\n",
    "        total_enhancements = intro1_enhancements + intro2_enhancements + discussion_enhancements + closing_enhancements + global_sections\n",
    "        \n",
    "        if enhancement_level == \"minimal\":\n",
    "            expected_vs_full = \"~60% smaller than full enhancement\"\n",
    "        elif enhancement_level == \"standard\":\n",
    "            expected_vs_full = \"~40% smaller than full enhancement\"\n",
    "        else:\n",
    "            expected_vs_full = \"Full enhancement level\"\n",
    "        \n",
    "        print(f\"Total Enhancement Fields: {total_enhancements}\")\n",
    "        print(f\"Size vs Full: {expected_vs_full}\")\n",
    "        \n",
    "        # Show sample enhanced content\n",
    "        print(f\"\\n🎯 Sample Enhanced Content:\")\n",
    "        intro1 = conv_flow.get(\"intro1\", {})\n",
    "        if \"spontaneity_elements\" in intro1:\n",
    "            spont_elements = intro1[\"spontaneity_elements\"]\n",
    "            print(f\"Intro1 Spontaneity: {len(spont_elements)} elements\")\n",
    "            for i, element in enumerate(spont_elements, 1):\n",
    "                print(f\"  {i}. {element[:60]}...\")\n",
    "        \n",
    "        global_spont = enhanced_data.get(\"spontaneous_moments\", {})\n",
    "        if \"natural_interruptions\" in global_spont:\n",
    "            interruptions = global_spont[\"natural_interruptions\"]\n",
    "            print(f\"Natural Interruptions: {len(interruptions)} items\")\n",
    "            for i, interruption in enumerate(interruptions, 1):\n",
    "                print(f\"  {i}. {interruption[:60]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error analyzing enhanced content: {e}\")\n",
    "    \n",
    "    return enhanced_result\n",
    "\n",
    "# Usage examples for different enhancement levels:\n",
    "\"\"\"\n",
    "# Minimal enhancement (fastest, most concise)\n",
    "enhancer_minimal = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"minimal\")\n",
    "result_minimal = enhancer_minimal.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Standard enhancement (balanced)\n",
    "enhancer_standard = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"standard\")\n",
    "result_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Full enhancement (comprehensive but largest)\n",
    "enhancer_full = SectionalDialogueContentEnhancer(deployment, \"Fanar-C-1-8.7B\", \"full\")\n",
    "result_full = enhancer_full.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "\n",
    "# Test with specific level\n",
    "enhanced_result = test_enhanced_dialogue_content_enhancer(\n",
    "    deployment, topic, information, classification_result, personas_result, structure_result, \n",
    "    model_name=\"Fanar-C-1-8.7B\", enhancement_level=\"minimal\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a07febcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Starting sectional dialogue enhancement (Level: standard)...\n",
      "==================================================\n",
      "📝 Chunk 1: Enhancing intro sections...\n",
      "⚠️ Error enhancing intros: Expecting ',' delimiter: line 12 column 202 (char 1019)\n",
      "📝 Chunk 2: Enhancing main discussion points...\n",
      "  Enhancing discussion point 1/3...\n",
      "  ✅ Point 1 enhanced successfully\n",
      "  Enhancing discussion point 2/3...\n",
      "  ✅ Point 2 enhanced successfully\n",
      "  Enhancing discussion point 3/3...\n",
      "  ✅ Point 3 enhanced successfully\n",
      "✅ All main discussion points processed\n",
      "📝 Chunk 3: Enhancing closing sections...\n",
      "✅ Closing sections enhanced successfully\n",
      "📝 Chunk 4: Creating global elements...\n",
      "✅ Global elements created successfully\n",
      "==================================================\n",
      "🎉 Sectional dialogue enhancement completed! (Level: standard)\n",
      "Standard Enhancement Result:\n",
      "{\n",
      "  \"episode_topic\": \"نقاش حول الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"د. فادي حسن\",\n",
      "      \"background\": \"أستاذ جامعي متخصص في اللغويات والحوسبة\",\n",
      "      \"speaking_style\": \"متحدث واضح وصريح مع تركيز على تقديم المعلومات بتوضيح مفصل.\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"مهندسة رانيا الصغير\",\n",
      "      \"background\": \"باحثة محترفة تعمل على تطوير نماذج الذكاء الاصطناعي باللغة العربية.\",\n",
      "      \"speaking_style\": \"متحدثة شغوفة تحب مشاركة أفكارها وتجاربها العملية.\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"مرحباً بكم مستمعينا الكرام، معكم د. فادي حسن في حلقة جديدة\",\n",
      "      \"podcast_introduction\": \"نناقش اليوم موضوعاً مهماً يهم الجميع ويستحق التأمل\",\n",
      "      \"episode_hook\": \"موضوع حلقتنا اليوم هو الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي وأثره على حياتنا\"\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"سنتحدث اليوم عن الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي وجوانبه المختلفة والمهمة\",\n",
      "      \"guest_welcome\": \"معي اليوم الضيف المتميز مهندسة رانيا الصغير، أهلاً وسهلاً بك\",\n",
      "      \"guest_bio_highlight\": \"مهندسة رانيا الصغير خبير متخصص في هذا المجال ولديه خبرة واسعة\"\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"الجانب الأول والأساسي للموضوع\",\n",
      "        \"personal_angle\": \"كيف يؤثر هذا الموضوع على حياتنا اليومية وتجاربنا\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"ما إذا كنا نلاحظ تأثير الأنظمة الذكية في عاداتنا الغذائية اليومية\",\n",
      "          \"مناقشة مثال لبرنامج مطبخ ذكي يحاول تقديم وصفات تقليدية\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"إشارة إلى التقاليد الطهي العربية المعقدة\",\n",
      "          \"المقارنة مع استخدام أجدادنا للطهي دون أدوات رقمية\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"هذا يقودنا إلى سؤال مهم حول كيفية الجمع بين التقنية والتراث.\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثاني والتحديات المرتبطة\",\n",
      "        \"personal_angle\": \"التحديات والفرص المتاحة في هذا المجال\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"كيف يمكن أن يؤدي تطور AI إلى التأثير السلبي غير المقصود على القيم العربية؟\",\n",
      "          \"أين نرى بالفعل تأثيرات التكنولوجيا الحديثة على هُوية الشباب العربي اليوم?\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"تذكر المثل الشعبي 'العيب ليس في الدرع بل في من يرتديه' كاستعارة لتطبيقات AI التي قد تشوه المعرفة الحقيقية\",\n",
      "          \"يمكن أيضا الربط بين الاستخدام الحالي للذكاء الصناعي وأمثلة مِن التاريخ الإسلامي, مثل استخدام الأجهزة الآلية الواردة في كتب الخيال العلمي الإسلامية القديمة\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"من خلال فهم هذه النقاط, دعونا نتعمق أكثر في كيفية مواجهة تحديات الذكاء الاصطناعي مع الحفاظ على هويتنا الثقافية.\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثالث والحلول المقترحة\",\n",
      "        \"personal_angle\": \"النصائح والتوجيهات العملية للمستقبل\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"كيف يمكن لنا أن نضمن عدم غزو المحتوى غير الطابع العربي للفضاء الإلكتروني?\",\n",
      "          \"ما هي دوريات التفتيش الرقمية التي تراقب محتوى الإنترنت لحماية الهوية العربية؟\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"مبدأ 'أدب الكلمة' كما ورد في الشعر الجاهلي, يشدد على أهمية اللغة والمعنى.\",\n",
      "          \"التقاليد النحوية والفقه اللغوي من العصور القديمة قد تشكل دساتير قواعد ذكاء اصطناعي عربي إسلامي.\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"لنتحدث الآن عن بعض المبادرات المحلية والدولية لضمان بقاء هويتنا الثقافية بارزة عبر وسائل التواصل الاجتماعي والأجهزة الذكية.\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"الخلاصات المهمة والنقاط الأساسية من نقاشنا اليوم\",\n",
      "        \"guest_final_message\": \"رسالة أخيرة ومهمة من الضيف لجمهور المستمعين\",\n",
      "        \"host_closing_thoughts\": \"أفكار ختامية وتأملات من المقدم حول الموضوع\",\n",
      "        \"emotional_closure\": \"وهكذا, فإن تبني التكنولوجيا مع ضمان حماية قيمنا الثقافية هو مفتاح الحفاظ على الهوية العربية الفريدة.\",\n",
      "        \"key_insights\": [\n",
      "          \"التعرف على كيفية استخدام الذكاء الاصطناعي بطرق تخدم لغتنا وثقافتنا أمر حيوي.\",\n",
      "          \"تشجيع الجيل الشاب على تعلم البرمجة ودعم إبداعها باستخدام اللغة العربية يمكن أن يكون له تأثير كبير.\"\n",
      "        ]\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"شكراً جزيلاً مهندسة رانيا الصغير على هذا النقاش المفيد والثري.\",\n",
      "        \"audience_thanks\": \"شكراً لكم مستمعينا الكرام على متابعتكم واهتمامكم.\",\n",
      "        \"call_to_action\": \"تفاعلوا معنا وشاركونا آراءكم عبر وسائل التواصل الاجتماعي.\",\n",
      "        \"memorable_ending\": \"نذكر جميعاً بأن التكنولوجيا ليست تهديدًا؛ بل إنها فرصة للمساعدة في حفظ وإبراز تراثنا الغني!\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"العلم نور والجهل ظلام\",\n",
      "      \"في التأني السلامة وفي العجلة الندامة\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"التجربة العربية الغنية في هذا المجال\",\n",
      "      \"الخبرات المحلية والإقليمية ذات الصلة بالموضوع\"\n",
      "    ]\n",
      "  },\n",
      "  \"spontaneous_moments\": {\n",
      "    \"natural_interruptions\": [\n",
      "      \"توقُّفَة عفويّة: إنَّ هذا النقاش حول تفاعلِ الأداء اللغوي للذكاء الاصطناعي مع الهوية العربيّة ملهم بالفعل! \",\n",
      "      \"مقاطعة自然：كيف يمكن لبنية اللغة الدقيقة أن تشكل أساساً لفهم الثقافة العربية بشكل أفضل?\"\n",
      "    ],\n",
      "    \"emotional_reactions\": [\n",
      "      \"تعبيرات حماسية: يُثير عملكم حول التوائم بين الـAI والثقافة قلقي والإثارة أيضًا.\",\n",
      "      \"استجابة عاطفيّة: هذه الفرصة لإعادة اكتشاف تراثنا بالاعتماد على التقنيات الحديثة رائعة!\"\n",
      "    ],\n",
      "    \"personal_stories\": [\n",
      "      \"قصّة شخصية: أتذكر عندما كشفت تقنية NLP عن أفكار جديدة عند تحليل قصيدة شعراء عرب مخضرمين.\",\n",
      "      \"تجربة زمنيّة: لقد ساهم استخدام البرمجيات الآليّة في إحياء معرفتنا ببعض القيم القديمة.\"\n",
      "    ]\n",
      "  },\n",
      "  \"dialogue_techniques\": {\n",
      "    \"questioning_styles\": [\n",
      "      \"أسلوب الاستفسار: كم منتاظرًا قد تدعم آلات التعلم فهمنا العمیق للأدب الخلیجي؟ \",\n",
      "      \"نهج التحلیل: هل يمكن لنماذج الطور المعقدة الحفاظ علی الجوهر الأصيل للمسرحیات العربیة القدیمة?\"\n",
      "    ],\n",
      "    \"storytelling_moments\": [\n",
      "      \"لحظة روائية: تخیل ما إذا كان الذكاء الصناعی يستطيع إعادة إنتاج تجارب صحراویة سعودية بطابع مؤثر!\",\n",
      "      \"رسم الصورة بألفاظ: شرح كيف تؤثر المحاكاة الواقعية لغةً وتصرفاتٍ علي انطباقهاعلي العالم الغربی\"\n",
      "    ],\n",
      "    \"audience_engagement\": [\n",
      "      \"طريقةالتوجيه: دعونا نتحدثعن مثال حي لتطبيق AI الذي يعالج الفروقات الإقليميةباللهجة المصريّة مثلاً! \",\n",
      "      \"تشجيعالمشاركة : شاركونا بتجاربكم الشخصيةمع أدوات الترجمةالمدعومةبرأس مالي ذكائي\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Standard enhancement (balanced)\n",
    "enhancer_standard = SectionalDialogueContentEnhancer(deployment, model, \"standard\")\n",
    "result_standard = enhancer_standard.enhance_dialogue_content(topic, information, classification_result, personas_result, structure_result)\n",
    "print(\"Standard Enhancement Result:\")\n",
    "print(result_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a852810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "class MinimalPolishEnhancer:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def enhance_spontaneous_moments_values(self, topic, classification_result, personas_result, current_spontaneous_moments):\n",
    "        \"\"\"\n",
    "        Chunk 1: Enhance values in existing spontaneous_moments (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast dialogue quality.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing spontaneous_moments structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current spontaneous moments: {json.dumps(current_spontaneous_moments, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make each phrase more natural and topic-specific\n",
    "- Connect phrases directly to the topic: {topic}\n",
    "- Make content specific to {host_name} and {guest_name}'s backgrounds\n",
    "- Ensure phrases sound more authentic and conversational\n",
    "\n",
    "Return the enhanced structure with the same fields but better values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these exact personas\n",
    "- Improve naturalness and authenticity of existing phrases\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance existing values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_cultural_context_values(self, topic, classification_result, current_cultural_context):\n",
    "        \"\"\"\n",
    "        Chunk 2: Enhance values in existing cultural_context (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "        except:\n",
    "            classification = {}\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        primary_category = classification.get(\"primary_category\", \"\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic cultural references.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing cultural_context structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Category: {primary_category}\n",
    "\n",
    "Current cultural context: {json.dumps(current_cultural_context, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and relevance of existing values\n",
    "- Make proverbs more directly relevant to the topic: {topic}\n",
    "- Make regional references more specific and meaningful\n",
    "- Ensure cultural authenticity and accuracy\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"العلم نور\"\n",
    "- After: \"العلم نور، والذكاء الاصطناعي يمكن أن يكون شمعة تضيء طريق الحفاظ على تراثنا\"\n",
    "\n",
    "Return the enhanced cultural context with better, more topic-specific values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make all content directly relevant to topic: {topic}\n",
    "- Maintain cultural authenticity and accuracy\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance cultural context values only. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.6\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_dialogue_techniques_values(self, topic, classification_result, personas_result, current_dialogue_techniques):\n",
    "        \"\"\"\n",
    "        Chunk 3: Enhance values in existing dialogue_techniques (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast dialogue techniques.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing dialogue_techniques structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current dialogue techniques: {json.dumps(current_dialogue_techniques, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make techniques more specific to the topic: {topic}\n",
    "- Tailor content to {host_name} and {guest_name}'s expertise\n",
    "- Make techniques more actionable and practical\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"أسئلة مفتوحة لتعميق النقاش\"\n",
    "- After: \"أسئلة مفتوحة حول كيفية تطوير ذكاء اصطناعي يحافظ على جمالية اللغة العربية وعمقها الثقافي\"\n",
    "\n",
    "Return the enhanced dialogue techniques with better, more specific values:\n",
    "\n",
    "{{\n",
    "    [exact same structure as input, but with enhanced values]\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve practicality and specificity of techniques\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields or arrays\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance dialogue technique values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_main_discussion_values(self, topic, classification_result, personas_result, current_main_discussion):\n",
    "        \"\"\"\n",
    "        Chunk 4: Enhance values in existing main_discussion points (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast discussion content.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing main_discussion structure. Keep the exact same fields and array lengths.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current main discussion: {json.dumps(current_main_discussion, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names for each discussion point\n",
    "- Keep the EXACT same number of items in each array\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make spontaneous_triggers more natural and topic-specific\n",
    "- Make cultural_references more directly relevant to the topic\n",
    "- Make natural_transitions smoother and more contextual\n",
    "- Ensure content reflects {host_name} and {guest_name}'s specific expertise\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"هذا يثير تساؤلاً مهماً\"\n",
    "- After: \"هذا يثير تساؤلاً مهماً حول قدرة الذكاء الاصطناعي على فهم السياق الثقافي وراء الكلمات العربية\"\n",
    "\n",
    "Return the enhanced main discussion with better, more specific values:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        [exact same structure as each input point, but with enhanced values]\n",
    "    }},\n",
    "    ...\n",
    "]\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve naturalness and conversational flow\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON array, no extra text\n",
    "- Do NOT add new fields to discussion points\n",
    "- Do NOT change array lengths within points\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance discussion point values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def enhance_intro_outro_values(self, topic, classification_result, personas_result, current_intro1, current_intro2, current_closing):\n",
    "        \"\"\"\n",
    "        Chunk 5: Enhance values in existing intro and outro sections (no new fields)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            classification = json.loads(classification_result)\n",
    "            personas = json.loads(personas_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid JSON provided\")\n",
    "        \n",
    "        optimal_style = classification.get(\"optimal_style\", \"\")\n",
    "        host = personas.get(\"host\", {})\n",
    "        guest = personas.get(\"guest\", {})\n",
    "        host_name = host.get('name', 'المقدم')\n",
    "        guest_name = guest.get('name', 'الضيف')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in enhancing Arabic podcast intros and outros.\n",
    "\n",
    "Task: Enhance ONLY the VALUES in the existing intro1, intro2, and closing structures. Keep the exact same fields.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {optimal_style}\n",
    "Host: {host_name} - {host.get('background', '')}\n",
    "Guest: {guest_name} - {guest.get('background', '')}\n",
    "\n",
    "Current intro1: {json.dumps(current_intro1, ensure_ascii=False)}\n",
    "Current intro2: {json.dumps(current_intro2, ensure_ascii=False)}\n",
    "Current closing: {json.dumps(current_closing, ensure_ascii=False)}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Keep the EXACT same JSON structure and field names\n",
    "- Keep the EXACT same number of items in each array (if any)\n",
    "- ONLY enhance the quality and specificity of existing values\n",
    "- Make opening_line more engaging and natural\n",
    "- Make episode_hook more compelling and topic-specific\n",
    "- Make guest_welcome more personal and authentic\n",
    "- Make closing thoughts more memorable and impactful\n",
    "- Ensure content reflects the personalities of {host_name} and {guest_name}\n",
    "\n",
    "Example enhancement:\n",
    "- Before: \"أهلاً بكم مستمعينا الكرام\"\n",
    "- After: \"أهلاً بكم مستمعينا الكرام في رحلة استكشافية مثيرة لنتعرف على كيفية جعل الذكاء الاصطناعي حارساً لتراثنا العربي\"\n",
    "\n",
    "Return the enhanced sections:\n",
    "\n",
    "{{\n",
    "    \"intro1\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }},\n",
    "    \"intro2\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }},\n",
    "    \"closing\": {{\n",
    "        [exact same structure as input, but with enhanced values]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- All enhanced values in Modern Standard Arabic (MSA)\n",
    "- Make content specific to topic: {topic} and these personas\n",
    "- Improve engagement and naturalness\n",
    "- Use English punctuation only (no ،)\n",
    "- Return only valid JSON, no extra text\n",
    "- Do NOT add new fields\n",
    "- Do NOT change array lengths\n",
    "\"\"\"\n",
    "\n",
    "        response = self.deployment.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You enhance intro/outro values only. Style: {optimal_style}. Keep exact structure. Return only valid JSON with English punctuation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return self._clean_json_response(response.choices[0].message.content)\n",
    "\n",
    "    def apply_minimal_polish(self, topic, information, classification_result, personas_result, enhanced_content_result):\n",
    "        \"\"\"\n",
    "        Main orchestration method: Coordinates all value enhancement chunks\n",
    "        \"\"\"\n",
    "        print(\"🎨 Starting minimal polish (value enhancement only)...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            enhanced_content = json.loads(enhanced_content_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid enhanced content JSON provided\")\n",
    "        \n",
    "        # Chunk 1: Enhance spontaneous moments values\n",
    "        print(\"✨ Chunk 1: Enhancing spontaneous moments values...\")\n",
    "        try:\n",
    "            current_spontaneous = enhanced_content.get(\"spontaneous_moments\", {})\n",
    "            if current_spontaneous:  # Only enhance if exists\n",
    "                enhanced_spontaneous_json = self.enhance_spontaneous_moments_values(\n",
    "                    topic, classification_result, personas_result, current_spontaneous\n",
    "                )\n",
    "                enhanced_spontaneous = json.loads(enhanced_spontaneous_json)\n",
    "                enhanced_content[\"spontaneous_moments\"] = enhanced_spontaneous\n",
    "                print(\"✅ Spontaneous moments values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No spontaneous moments to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing spontaneous moments: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 2: Enhance cultural context values\n",
    "        print(\"✨ Chunk 2: Enhancing cultural context values...\")\n",
    "        try:\n",
    "            current_cultural = enhanced_content.get(\"cultural_context\", {})\n",
    "            if current_cultural:  # Only enhance if exists\n",
    "                enhanced_cultural_json = self.enhance_cultural_context_values(\n",
    "                    topic, classification_result, current_cultural\n",
    "                )\n",
    "                enhanced_cultural = json.loads(enhanced_cultural_json)\n",
    "                enhanced_content[\"cultural_context\"] = enhanced_cultural\n",
    "                print(\"✅ Cultural context values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No cultural context to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing cultural context: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 3: Enhance dialogue techniques values\n",
    "        print(\"✨ Chunk 3: Enhancing dialogue techniques values...\")\n",
    "        try:\n",
    "            current_dialogue = enhanced_content.get(\"dialogue_techniques\", {})\n",
    "            if current_dialogue:  # Only enhance if exists\n",
    "                enhanced_dialogue_json = self.enhance_dialogue_techniques_values(\n",
    "                    topic, classification_result, personas_result, current_dialogue\n",
    "                )\n",
    "                enhanced_dialogue = json.loads(enhanced_dialogue_json)\n",
    "                enhanced_content[\"dialogue_techniques\"] = enhanced_dialogue\n",
    "                print(\"✅ Dialogue techniques values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No dialogue techniques to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing dialogue techniques: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 4: Enhance main discussion values\n",
    "        print(\"✨ Chunk 4: Enhancing main discussion values...\")\n",
    "        try:\n",
    "            conv_flow = enhanced_content.get(\"conversation_flow\", {})\n",
    "            current_main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "            if current_main_discussion:  # Only enhance if exists\n",
    "                enhanced_discussion_json = self.enhance_main_discussion_values(\n",
    "                    topic, classification_result, personas_result, current_main_discussion\n",
    "                )\n",
    "                enhanced_discussion = json.loads(enhanced_discussion_json)\n",
    "                enhanced_content[\"conversation_flow\"][\"main_discussion\"] = enhanced_discussion\n",
    "                print(\"✅ Main discussion values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No main discussion to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing main discussion: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Chunk 5: Enhance intro and outro values\n",
    "        print(\"✨ Chunk 5: Enhancing intro and outro values...\")\n",
    "        try:\n",
    "            conv_flow = enhanced_content.get(\"conversation_flow\", {})\n",
    "            current_intro1 = conv_flow.get(\"intro1\", {})\n",
    "            current_intro2 = conv_flow.get(\"intro2\", {})\n",
    "            current_closing = conv_flow.get(\"closing\", {})\n",
    "            \n",
    "            if current_intro1 or current_intro2 or current_closing:  # Only enhance if any exist\n",
    "                enhanced_sections_json = self.enhance_intro_outro_values(\n",
    "                    topic, classification_result, personas_result, \n",
    "                    current_intro1, current_intro2, current_closing\n",
    "                )\n",
    "                enhanced_sections = json.loads(enhanced_sections_json)\n",
    "                \n",
    "                if \"intro1\" in enhanced_sections and current_intro1:\n",
    "                    enhanced_content[\"conversation_flow\"][\"intro1\"] = enhanced_sections[\"intro1\"]\n",
    "                if \"intro2\" in enhanced_sections and current_intro2:\n",
    "                    enhanced_content[\"conversation_flow\"][\"intro2\"] = enhanced_sections[\"intro2\"]\n",
    "                if \"closing\" in enhanced_sections and current_closing:\n",
    "                    enhanced_content[\"conversation_flow\"][\"closing\"] = enhanced_sections[\"closing\"]\n",
    "                \n",
    "                print(\"✅ Intro and outro values enhanced successfully\")\n",
    "            else:\n",
    "                print(\"⏭️ No intro/outro sections to enhance\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error enhancing intro/outro: {e}\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"🎉 Minimal polish completed! Same structure, enhanced values.\")\n",
    "        \n",
    "        return json.dumps(enhanced_content, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def _clean_json_response(self, response):\n",
    "        \"\"\"Enhanced JSON cleaning method\"\"\"\n",
    "        response = response.strip()\n",
    "        \n",
    "        # Remove any text before first { and after last }\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            clean_json = response[start_idx:end_idx+1]\n",
    "        else:\n",
    "            clean_json = response\n",
    "        \n",
    "        # Handle arrays\n",
    "        if clean_json.strip().startswith('['):\n",
    "            start_idx = response.find('[')\n",
    "            end_idx = response.rfind(']')\n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                clean_json = response[start_idx:end_idx+1]\n",
    "        \n",
    "        # Replace Arabic punctuation with English equivalents\n",
    "        clean_json = clean_json.replace('،', ',')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace('\"', '\"')\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        clean_json = clean_json.replace(''', \"'\")\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        import re\n",
    "        clean_json = re.sub(r',(\\s*[}\\]])', r'\\1', clean_json)\n",
    "        \n",
    "        return clean_json\n",
    "\n",
    "    def validate_polished_outline(self, original_json, polished_json):\n",
    "        \"\"\"\n",
    "        Validate that polished outline has same structure but enhanced values\n",
    "        \"\"\"\n",
    "        try:\n",
    "            original = json.loads(original_json)\n",
    "            polished = json.loads(polished_json)\n",
    "            \n",
    "            issues = []\n",
    "            \n",
    "            # Check that main structure is preserved\n",
    "            original_keys = set(original.keys())\n",
    "            polished_keys = set(polished.keys())\n",
    "            \n",
    "            if original_keys != polished_keys:\n",
    "                issues.append(f\"Main structure changed: {original_keys} vs {polished_keys}\")\n",
    "            \n",
    "            # Check conversation flow structure\n",
    "            orig_conv = original.get(\"conversation_flow\", {})\n",
    "            pol_conv = polished.get(\"conversation_flow\", {})\n",
    "            \n",
    "            if set(orig_conv.keys()) != set(pol_conv.keys()):\n",
    "                issues.append(\"Conversation flow structure changed\")\n",
    "            \n",
    "            # Check main discussion array length\n",
    "            orig_main = orig_conv.get(\"main_discussion\", [])\n",
    "            pol_main = pol_conv.get(\"main_discussion\", [])\n",
    "            \n",
    "            if len(orig_main) != len(pol_main):\n",
    "                issues.append(f\"Main discussion length changed: {len(orig_main)} vs {len(pol_main)}\")\n",
    "            \n",
    "            # Check that arrays within sections maintain length\n",
    "            sections_to_check = [\"spontaneous_moments\", \"dialogue_techniques\", \"cultural_context\"]\n",
    "            \n",
    "            for section in sections_to_check:\n",
    "                if section in original and section in polished:\n",
    "                    orig_section = original[section]\n",
    "                    pol_section = polished[section]\n",
    "                    \n",
    "                    if isinstance(orig_section, dict) and isinstance(pol_section, dict):\n",
    "                        for key in orig_section:\n",
    "                            if isinstance(orig_section[key], list) and isinstance(pol_section.get(key), list):\n",
    "                                if len(orig_section[key]) != len(pol_section[key]):\n",
    "                                    issues.append(f\"{section}.{key} array length changed\")\n",
    "            \n",
    "            # Check for quality improvement (simple heuristic)\n",
    "            orig_text = json.dumps(original, ensure_ascii=False)\n",
    "            pol_text = json.dumps(polished, ensure_ascii=False)\n",
    "            \n",
    "            if len(pol_text) < len(orig_text) * 0.95:  # Significant reduction might indicate loss of content\n",
    "                issues.append(\"Polished content appears significantly shorter\")\n",
    "            \n",
    "            if issues:\n",
    "                return False, f\"Structure validation issues: {issues}\"\n",
    "            \n",
    "            return True, \"Minimal polish validation successful - same structure, enhanced values\"\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            return False, f\"JSON parsing error: {e}\"\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_minimal_polish_enhancer(deployment, topic, information, classification_result, personas_result, enhanced_content_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the minimal polish enhancer\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Minimal Polish Enhancer (Value Enhancement Only)...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    polisher = MinimalPolishEnhancer(deployment, model_name)\n",
    "    \n",
    "    # Store original for comparison\n",
    "    original_json = enhanced_content_result\n",
    "    \n",
    "    # Run minimal polish\n",
    "    polished_result = polisher.apply_minimal_polish(\n",
    "        topic, information, classification_result, personas_result, enhanced_content_result\n",
    "    )\n",
    "    \n",
    "    # Validate polished content\n",
    "    is_valid, validation_message = polisher.validate_polished_outline(original_json, polished_result)\n",
    "    \n",
    "    print(f\"\\n📊 Polish Results:\")\n",
    "    print(f\"Validation: {'✅ Valid' if is_valid else '❌ Invalid'}\")\n",
    "    print(f\"Message: {validation_message}\")\n",
    "    \n",
    "    # Quick comparison analysis\n",
    "    try:\n",
    "        original_data = json.loads(original_json)\n",
    "        polished_data = json.loads(polished_result)\n",
    "        \n",
    "        # Compare sample values\n",
    "        print(f\"\\n🔍 Sample Value Comparisons:\")\n",
    "        \n",
    "        # Spontaneous moments comparison\n",
    "        orig_spont = original_data.get(\"spontaneous_moments\", {}).get(\"natural_interruptions\", [])\n",
    "        pol_spont = polished_data.get(\"spontaneous_moments\", {}).get(\"natural_interruptions\", [])\n",
    "        \n",
    "        if orig_spont and pol_spont:\n",
    "            print(f\"Natural Interruptions:\")\n",
    "            print(f\"  Original: {orig_spont[0][:50]}...\")\n",
    "            print(f\"  Polished: {pol_spont[0][:50]}...\")\n",
    "        \n",
    "        # Cultural context comparison\n",
    "        orig_cultural = original_data.get(\"cultural_context\", {}).get(\"proverbs_sayings\", [])\n",
    "        pol_cultural = polished_data.get(\"cultural_context\", {}).get(\"proverbs_sayings\", [])\n",
    "        \n",
    "        if orig_cultural and pol_cultural:\n",
    "            print(f\"Proverbs:\")\n",
    "            print(f\"  Original: {orig_cultural[0][:50]}...\")\n",
    "            print(f\"  Polished: {pol_cultural[0][:50]}...\")\n",
    "        \n",
    "        # Size comparison\n",
    "        orig_size = len(json.dumps(original_data, ensure_ascii=False))\n",
    "        pol_size = len(json.dumps(polished_data, ensure_ascii=False))\n",
    "        size_change = ((pol_size - orig_size) / orig_size) * 100\n",
    "        \n",
    "        print(f\"\\n📈 Size Analysis:\")\n",
    "        print(f\"Original Size: {orig_size:,} characters\")\n",
    "        print(f\"Polished Size: {pol_size:,} characters\")\n",
    "        print(f\"Size Change: {size_change:+.1f}%\")\n",
    "        print(f\"Approach: {'✅ Value enhancement only' if abs(size_change) < 15 else '⚠️ Significant size change'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error analyzing polished content: {e}\")\n",
    "    \n",
    "    return polished_result\n",
    "\n",
    "# Usage:\n",
    "# polisher = MinimalPolishEnhancer(deployment, \"Fanar-C-1-8.7B\")\n",
    "# final_polished_outline = polisher.apply_minimal_polish(topic, information, classification_result, personas_result, enhanced_content_result)\n",
    "\n",
    "# Test the polisher\n",
    "# polished_result = test_minimal_polish_enhancer(\n",
    "#     deployment, topic, information, classification_result, personas_result, enhanced_content_result\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "967209c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 Starting minimal polish (value enhancement only)...\n",
      "==================================================\n",
      "✨ Chunk 1: Enhancing spontaneous moments values...\n",
      "✅ Spontaneous moments values enhanced successfully\n",
      "✨ Chunk 2: Enhancing cultural context values...\n",
      "✅ Cultural context values enhanced successfully\n",
      "✨ Chunk 3: Enhancing dialogue techniques values...\n",
      "✅ Dialogue techniques values enhanced successfully\n",
      "✨ Chunk 4: Enhancing main discussion values...\n",
      "⚠️ Error enhancing main discussion: Extra data: line 7 column 6 (char 788)\n",
      "✨ Chunk 5: Enhancing intro and outro values...\n",
      "⚠️ Error enhancing intro/outro: Expecting property name enclosed in double quotes: line 15 column 96 (char 1309)\n",
      "==================================================\n",
      "🎉 Minimal polish completed! Same structure, enhanced values.\n",
      "Final Polished Outline:\n",
      "{\n",
      "  \"episode_topic\": \"نقاش حول الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"د. فادي حسن\",\n",
      "      \"background\": \"أستاذ جامعي متخصص في اللغويات والحوسبة\",\n",
      "      \"speaking_style\": \"متحدث واضح وصريح مع تركيز على تقديم المعلومات بتوضيح مفصل.\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"مهندسة رانيا الصغير\",\n",
      "      \"background\": \"باحثة محترفة تعمل على تطوير نماذج الذكاء الاصطناعي باللغة العربية.\",\n",
      "      \"speaking_style\": \"متحدثة شغوفة تحب مشاركة أفكارها وتجاربها العملية.\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"مرحباً بكم مستمعينا الكرام، معكم د. فادي حسن في حلقة جديدة\",\n",
      "      \"podcast_introduction\": \"نناقش اليوم موضوعاً مهماً يهم الجميع ويستحق التأمل\",\n",
      "      \"episode_hook\": \"موضوع حلقتنا اليوم هو الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي وأثره على حياتنا\"\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"سنتحدث اليوم عن الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي وجوانبه المختلفة والمهمة\",\n",
      "      \"guest_welcome\": \"معي اليوم الضيف المتميز مهندسة رانيا الصغير، أهلاً وسهلاً بك\",\n",
      "      \"guest_bio_highlight\": \"مهندسة رانيا الصغير خبير متخصص في هذا المجال ولديه خبرة واسعة\"\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"الجانب الأول والأساسي للموضوع\",\n",
      "        \"personal_angle\": \"كيف يؤثر هذا الموضوع على حياتنا اليومية وتجاربنا\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"ما إذا كنا نلاحظ تأثير الأنظمة الذكية في عاداتنا الغذائية اليومية\",\n",
      "          \"مناقشة مثال لبرنامج مطبخ ذكي يحاول تقديم وصفات تقليدية\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"إشارة إلى التقاليد الطهي العربية المعقدة\",\n",
      "          \"المقارنة مع استخدام أجدادنا للطهي دون أدوات رقمية\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"هذا يقودنا إلى سؤال مهم حول كيفية الجمع بين التقنية والتراث.\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثاني والتحديات المرتبطة\",\n",
      "        \"personal_angle\": \"التحديات والفرص المتاحة في هذا المجال\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"كيف يمكن أن يؤدي تطور AI إلى التأثير السلبي غير المقصود على القيم العربية؟\",\n",
      "          \"أين نرى بالفعل تأثيرات التكنولوجيا الحديثة على هُوية الشباب العربي اليوم?\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"تذكر المثل الشعبي 'العيب ليس في الدرع بل في من يرتديه' كاستعارة لتطبيقات AI التي قد تشوه المعرفة الحقيقية\",\n",
      "          \"يمكن أيضا الربط بين الاستخدام الحالي للذكاء الصناعي وأمثلة مِن التاريخ الإسلامي, مثل استخدام الأجهزة الآلية الواردة في كتب الخيال العلمي الإسلامية القديمة\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"من خلال فهم هذه النقاط, دعونا نتعمق أكثر في كيفية مواجهة تحديات الذكاء الاصطناعي مع الحفاظ على هويتنا الثقافية.\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الجانب الثالث والحلول المقترحة\",\n",
      "        \"personal_angle\": \"النصائح والتوجيهات العملية للمستقبل\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"كيف يمكن لنا أن نضمن عدم غزو المحتوى غير الطابع العربي للفضاء الإلكتروني?\",\n",
      "          \"ما هي دوريات التفتيش الرقمية التي تراقب محتوى الإنترنت لحماية الهوية العربية؟\"\n",
      "        ],\n",
      "        \"cultural_references\": [\n",
      "          \"مبدأ 'أدب الكلمة' كما ورد في الشعر الجاهلي, يشدد على أهمية اللغة والمعنى.\",\n",
      "          \"التقاليد النحوية والفقه اللغوي من العصور القديمة قد تشكل دساتير قواعد ذكاء اصطناعي عربي إسلامي.\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"لنتحدث الآن عن بعض المبادرات المحلية والدولية لضمان بقاء هويتنا الثقافية بارزة عبر وسائل التواصل الاجتماعي والأجهزة الذكية.\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"الخلاصات المهمة والنقاط الأساسية من نقاشنا اليوم\",\n",
      "        \"guest_final_message\": \"رسالة أخيرة ومهمة من الضيف لجمهور المستمعين\",\n",
      "        \"host_closing_thoughts\": \"أفكار ختامية وتأملات من المقدم حول الموضوع\",\n",
      "        \"emotional_closure\": \"وهكذا, فإن تبني التكنولوجيا مع ضمان حماية قيمنا الثقافية هو مفتاح الحفاظ على الهوية العربية الفريدة.\",\n",
      "        \"key_insights\": [\n",
      "          \"التعرف على كيفية استخدام الذكاء الاصطناعي بطرق تخدم لغتنا وثقافتنا أمر حيوي.\",\n",
      "          \"تشجيع الجيل الشاب على تعلم البرمجة ودعم إبداعها باستخدام اللغة العربية يمكن أن يكون له تأثير كبير.\"\n",
      "        ]\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"شكراً جزيلاً مهندسة رانيا الصغير على هذا النقاش المفيد والثري.\",\n",
      "        \"audience_thanks\": \"شكراً لكم مستمعينا الكرام على متابعتكم واهتمامكم.\",\n",
      "        \"call_to_action\": \"تفاعلوا معنا وشاركونا آراءكم عبر وسائل التواصل الاجتماعي.\",\n",
      "        \"memorable_ending\": \"نذكر جميعاً بأن التكنولوجيا ليست تهديدًا؛ بل إنها فرصة للمساعدة في حفظ وإبراز تراثنا الغني!\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"بالعلم والمعرفة نستطيع توجيه الذكاء الاصطناعي لتعزيز الهوية العربية.\",\n",
      "      \"مثلما حفظ العرب التراث عبر الأجيال, يجب علينا استخدام الذكاء الاصطناعي بحكمة للحفاظ على هويتنا.\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"دور الخبراء العرب في تنقية الذكاء الاصطناعي من الانحراف عن القيم الإسلامية والموروث الثقافي.\",\n",
      "      \"استخدام الحلول التقليدية العربية مثل 'المجلس' لتوجيه تطوير الذكاء الاصطناعي بشكل جماعي.\"\n",
      "    ]\n",
      "  },\n",
      "  \"spontaneous_moments\": {\n",
      "    \"natural_interruptions\": [\n",
      "      \"دعوة مُلهمة: دعونا نستكشف دور اللغات الطبيعية المساعدة في الحفاظ على ثرائنا الأدبي والفكري في عصر البيانات الضخمة!\",\n",
      "      \"تعليق هادف: تعد شبكات GPT العربية فرصة فريدة لاستخراج معاني عميقة من متنوعة النصوص التاريخية العربية\"\n",
      "    ],\n",
      "    \"emotional_reactions\": [\n",
      "      \"عبرتْ استجاباتِيَ بتأثر شديد: يؤثر الإبداع الذي رأيته باستخدام الذكاء الاصطناعي لتفسير الجوانب الخفية للتراث الأدبي العربى\",\n",
      "      \"أعرب بفخر وعزم: يتيح لنا الاعتماد على الذكاء الاصطناعى لتبادل المعرفة العربية إعادة تعريف ديناميكية الثقافة والمعاصرة بطريقة مبتكرة وغير مسبوقة!\"\n",
      "    ],\n",
      "    \"personal_stories\": [\n",
      "      \"سرد قصة ذات صلة: يذكرني تحليل شعر امرؤ القيس بواسطة نظام ذكي بفرص توسيع فهمنا للمفردات والتعبير الفريد للشعر الجاهلي\",\n",
      "      \"عرض تجربة عملية: ساعدت برمجيات الترجمة الآلية في تفكيك التعابير المجازية الغامضة الموجودة بالأعمال الفكرية الإسلامية الكلاسيكية ومشاركتها على نطاق واسع للعالم الحديث\"\n",
      "    ]\n",
      "  },\n",
      "  \"dialogue_techniques\": {\n",
      "    \"questioning_styles\": [\n",
      "      \"استراتيجية استقصاء: كيف يمكن أن يساعد إدراك النظم القاموسية الفريدة لللهجات العربية في تكييف نماذج LLMs مثل Falcon أو QCRI مع السياق الثقافي العربي?\",\n",
      "      \"أسئلة تحليل عميقة: كيف تساهم التقنيات المتعلقة بفهم Irony والسخرية, والمضمنة في الذكاء الصناعي, في حفظ السمات الجمالية للحوار الشعبي العربي؟\"\n",
      "    ],\n",
      "    \"storytelling_moments\": [\n",
      "      \"خلق لحظات سرد: تصور سيناريو يشمل الروبوتات التي تستخدم التعبير الوجداني الدقيق لدينا للتفاعل بشكل أصيل ضمن سياق قصيدة عربية كلاسيكية\",\n",
      "      \"تصور بصرية متعددة الطبقات: اشرح العملية المبتكرة لسرد القصص باستخدام الذكاء الصناعي لتحويل التجارب التاريخية إلى فيديوهات مذهلة تحتفظ بالروح والأسلوب الشعري للعصور القديمة\"\n",
      "    ],\n",
      "    \"audience_engagement\": [\n",
      "      \"دعوة للاستفهام: نناقش الآن حالة استخدام حديثة لأداة تعلم عميق تمكن لغتنا الفصحى من التفوق في مجال المناظرات القانونية الدولية\",\n",
      "      \"تشجيع مشاركة الجمهور: شارِكوننا أمثلة عن الأدوار الملحة للذكاء الصناعي في صون خصوصيتنا اللغوية والثقافية عبر الإنترنت\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "polisher = MinimalPolishEnhancer(deployment, model)\n",
    "final_polished_outline = polisher.apply_minimal_polish(topic, information, classification_result, personas_result, result_standard)\n",
    "print(\"Final Polished Outline:\")\n",
    "print(final_polished_outline)\n",
    "# Test the polisher\n",
    "# polished_result = test_minimal_polish_enhancer(\n",
    "#     deployment, topic, information, classification_result, personas_result, result_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"episode_topic\": \"الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"لمى عبد الله\",\n",
      "      \"background\": \"صحفية متخصصة في الشأن التكنولوجي في إحدى الجرائد المحلية\",\n",
      "      \"speaking_style\": \"تطرح أسئلة مفتوحة لتعميق النقاش وتعزز الحديث بموضوعية واضحة\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"علي محسن\",\n",
      "      \"background\": \"باحث في علوم الكمبيوتر وأستاذ مساعد في جامعة حكومية\",\n",
      "      \"speaking_style\": \"يعرض أفكارًا فنية بأسلوب سلس ويتفاعل مع المواضيع بتحليل عميق\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"مرحبا أصدقائي,\",\n",
      "      \"podcast_introduction\": \"انضموا الآن إلى حلقتنا حول الذكاء الصناعي وتراثنا العربي الغني.\",\n",
      "      \"episode_hook\": \"كيف نوازن بين الابتكار والتقاليد?\",\n",
      "      \"tone_guidance\": \"تحليلي ومتفتح للأفكار\",\n",
      "      \"spontaneity_elements\": [\n",
      "        \"إن هذا موضوع يهم كل واحد منا بلا شك\",\n",
      "        \"لنبدأ بتبادل الأفكار حول هذه المسألة المثيرة\"\n",
      "      ]\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"من الواضح أن الكثير من أدوات الذكاء الاصطناعي تشكلت بناءً على بيانات غربية.\",\n",
      "      \"guest_welcome\": \"مرحبًا دكتور علي, شكرًا لك على تواجدك معنا.\",\n",
      "      \"guest_bio_highlight\": \"عبر عن رأيك بخصوص كيفية تأثير ذلك على هويّتنا كعرب.\",\n",
      "      \"transition_to_discussion\": \"فلنرسم خطوطًا واضحة لهذا الأمر إذًا\",\n",
      "      \"cultural_connections\": [\n",
      "        \"يحمل هذا النقاش أهميته لأن الإسلام دين العلم والتفكير الناقد\",\n",
      "        \"والتقاليد الثقافية العربية مليئة بالأمثلة التي تدعم الإبداع\"\n",
      "      ]\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"مشكلة تدريب النماذج على محتوى غير عربي\",\n",
      "        \"personal_angle\": \"كمراسل تكنولوجيا, أرى إمكانية للتأثيرات الغريبة على اللغة والثقافة.\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"هذا يثير تساؤلاً مهماً\",\n",
      "          \"دعني أشارككم تجربة في هذا المجال\"\n",
      "        ],\n",
      "        \"disagreement_points\": \"قد تختلف وجهات النظر حول أفضل طريقة للتعامل مع هذه القضية\",\n",
      "        \"cultural_references\": [\n",
      "          \"كما يقول المثل: العلم نور\",\n",
      "          \"تراثنا يعلمنا أهمية التوازن في كل شيء\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"هذا يقودنا إلى نقطة مهمة أخرى\",\n",
      "        \"emotional_triggers\": \"هذا الموضوع يلامس قلوب كل من يهتم بمستقبل ثقافتنا\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"خطوات نحو تطوير نماذج عربية\",\n",
      "        \"personal_angle\": \"الباحث علي, هل لنا تحديث بشأن مشاريع الوطن الخليجية?\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"ما إذا كانت هذه النماذج تعزز التراث الثقافي\",\n",
      "          \"أمثلة لنجاحات الإمارات في هذا المجال\"\n",
      "        ],\n",
      "        \"disagreement_points\": \"قد يجادل البعض بأن التركيز الشديد قد يبسط التفرد الثقافي العربي بدلاً من تعزيزه.\",\n",
      "        \"cultural_references\": [\n",
      "          \"ذكر قصة حارس الوعل في هذا السياق\",\n",
      "          \"استشهاد بشاعر عربي معروف يدافع عن اللغة والتقاليد\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"من الجدير بالاعتبار أيضًا...\",\n",
      "        \"emotional_triggers\": \"يمكن أن تثير هذه المناقشة الشعور بالمهمة الفريدة للحفاظ على هويتنا.\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"استخدام الذكاء الاصطناعي لتعزيز التعريف بالحضارة الإسلامية\",\n",
      "        \"personal_angle\": \"كيف يؤثر ذلك على تعريف الأجيال الناشئة بهويتنا الفريدة?\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"عند مناقشة نماذج اللغة التي قد تقوض المصطلحات الدينية,\",\n",
      "          \"بالحديث عن إمكانات الذكاء الاصطناعي في إبراز التراث العلمي الإسلامي.\"\n",
      "        ],\n",
      "        \"disagreement_points\": \"قد يجادل البعض أن الاعتماد المفرط على أجهزة الذكاء الاصطناعي يمكن أن يخفف قدرة الشباب على التفاعل مباشرة مع مصادر الهوية الثقافية.\",\n",
      "        \"cultural_references\": [\n",
      "          \"يمكن ذكر مثال أدباء العرب القدامى كالجاحظ أو ابن خلدون لتأكيد أهمية التفكير النقدي الأصيل.\",\n",
      "          \"ربما الاستشهاد بنجاح استخدام AI حالياً لإحياء نسخ رقمية لتاريخ المساجد الكبرى مثل جامع القرويين بالمغرب.\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"هذا يقودنا إلى السؤال التالي حول كيفية تحقيق توازن بين الابتكار والتأكد من عدم فقدان جوهر هويتنا.\",\n",
      "        \"emotional_triggers\": \"من الطبيعي الشعور بالقلق حيال الحفاظ على هويّتنا الأصيلة بينما نتجه نحو مستقبل رقمي سريع.\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"الخلاصة: يجب توجيه الابتكار لتعزيز هويتنا, وليس استبدالها.\",\n",
      "        \"guest_final_message\": \"النصائح النهائية لعلي: دعم المبادرات التعليمية والتواصل المستمر.\",\n",
      "        \"host_closing_thoughts\": \"إن مواصلة المناقشة ستنعكس بلا شك على مستقبلنا الرقمي الآمن.\",\n",
      "        \"emotional_closure\": \"إن الاعتراف بترابط تكنولوجيا اليوم وهويتنا يمكن أن يلهم التقدير المشترك بين الأجيال وأن يبني روابط قوية مع جذورنا.\",\n",
      "        \"key_insights\": [\n",
      "          \"أولاً, تسليح الشباب بالمعرفة التقنية وربط هذه المعرفة بقيمهم الثقافية أمر حاسم.\",\n",
      "          \"ثانياً, تشجيع تطوير محتوى عربي أصيل ومناسب للذكاء الاصطناعي سيساهم في حماية الهوية.\",\n",
      "          \"أخيراً, خلق بيئة تفاعلية وداعمة للمبتكرين العرب سيضمن الاستدامة والبقاء الفريد لثقافتنا في عصر الديجيتال.\"\n",
      "        ]\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"شكرا لكل تعاون البروفيسور علي محسن.\",\n",
      "        \"audience_thanks\": \"شكراً لكم للاستماع والاستفادة من خبرة الضيف معنا.\",\n",
      "        \"call_to_action\": \"انضموا لنا في الرحلة نحو فهم أفضل لكيفية اندماج الذكاء الاصطناعي والثقافة العربية.\",\n",
      "        \"memorable_ending\": \"تذكر دائماً, قدرة تقنيات الغد ترجع إلى فهم وتعاون المجتمع العربي اليوم.\",\n",
      "        \"connection_building\": \"ابقوا على اطلاع بأحدث النقاشات والمقالات ذات الصلة عبر صفحتنا على الفيسبوك أو تويتر تحت #ArabicAIIdentityPodcast\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"الأصل دائمٌ, مهما تبدلت الظروف.\",\n",
      "      \"الحديث السليم يفيد السامعين\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"تاريخ الكويت كنموذج لمواكبة التقدم والحفاظ على الأصالة.\",\n",
      "      \"دور مصر التاريخي في نشر العلم\"\n",
      "    ]\n",
      "  },\n",
      "  \"language_style\": {\n",
      "    \"formality_level\": \"ملائمة للمناقشة التحليلية\",\n",
      "    \"dialect_touches\": \"إضافة عناصر بسيطة من لهجة بلدان مجلس التعاون الخليجي للإلفة\",\n",
      "    \"vocabulary_richness\": \"مصطلحات متنوعة تتوافق مع الموضوع\"\n",
      "  },\n",
      "  \"technical_notes\": {\n",
      "    \"pacing_guidance\": \"سرعة مناسبة لقضاء 10 دقائق\",\n",
      "    \"pause_points\": \"[... عند الانتقال بين النقاط الرئيسية...]\",\n",
      "    \"emphasis_moments\": \"[...على مفاهيم تحديد الأولويات ...] \"\n",
      "  },\n",
      "  \"spontaneous_moments\": {\n",
      "    \"spontaneous_moments\": [\n",
      "      {\n",
      "        \"title\": \"الذكاء الاصطناعي والهوية العربية\",\n",
      "        \"natural_interruption\": \"تخيلوا هذا.. كنت أناقش مع صديقي عن الذكاء الاصطناعي! 🎯*\",\n",
      "        \"emotional_reaction\": \"أشعر بالفضول حول تأثير التكنولوجيا الحديثة على تراثنا الثقافي.\",\n",
      "        \"personal_story\": \"عندما زرت متحفًا تقليديًّا, لاحظت كم كان الأطفال أكثر انجذابًا للألعاب الافتراضية بدلاً من القطع الأثرية الحقيقية. 🤔\",\n",
      "        \"humorous_moment\": \"لقد تساءلت ذات مرة إذا كانت الروبوتات قد تكتسب حس الفكاهة العربي يومًا ما! 😅\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"الحفاظ على الهوية العربية\",\n",
      "        \"natural_interruption\": \"Wait, have you seen those AI-generated poetry pieces? They're quite impressive but...!\",\n",
      "        \"emotional_reaction\": \"I feel it's crucial that we guide this technological advancement while preserving our cultural values.\",\n",
      "        \"personal_story\": \"My grandmother once told me stories at night by candlelight; now, I wonder how future generations will connect with their heritage through digital means.\",\n",
      "        \"humorous_moment\": \"Imagine an Arabic AI robot trying to make a joke using ancient proverbs – hilarious chaos guaranteed! 😂\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"personality_interactions\": {\n",
      "    \"interaction\": {\n",
      "      \"host\": \"لمى عبد الله\",\n",
      "      \"guest\": \"علي محسن\",\n",
      "      \"details\": {\n",
      "        \"host_strengths\": [\n",
      "          \"Exceptional listening skills.\",\n",
      "          \"Enthusiastic facilitation of engaging discussions.\"\n",
      "        ],\n",
      "        \"guest_expertise\": [\n",
      "          \"Deep knowledge in cultural anthropology.\",\n",
      "          \"Insightful storytelling abilities\"\n",
      "        ],\n",
      "        \"natural_chemistry\": \"Their shared passion for understanding diverse cultures quickly established a comfortable rapport.\",\n",
      "        \"tension_points\": [\n",
      "          \"Occasional differences in pacing during the conversation were noted but effectively managed by both parties.\"\n",
      "        ],\n",
      "        \"collaboration_moments\": [\n",
      "          \"A particularly compelling discussion about cross-cultural communication showed their collaborative strengths.\"\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"dialogue_techniques\": {\n",
      "    \"dialogue_techniques\": {\n",
      "      \"questioning_styles\": [\n",
      "        \"How can we leverage AI to preserve and share our cultural heritage?\",\n",
      "        \"In what ways does digital technology enhance or challenge traditional Arabic identity?\",\n",
      "        \"What are some innovative approaches Arab creators have taken to integrate AI into their artistic expressions?\"\n",
      "      ],\n",
      "      \"storytelling_moments\": [\n",
      "        \"Share personal stories of how family traditions were passed down through generations before the digital age.\",\n",
      "        \"Discuss an example of a successful Arab startup that uses AI to promote Islamic values while embracing modern technology.\",\n",
      "        \"Tell a story about how language preservation efforts are being aided by AI tools.\"\n",
      "      ],\n",
      "      \"audience_engagement\": [\n",
      "        \"Ask listeners to share their own experiences with balancing technological advancements and maintaining cultural identity.\",\n",
      "        \"Suggest interactive activities where audience members could create their own content using AI but with a focus on promoting Arabic culture.\",\n",
      "        \"Invite experts from various fields like linguistics, computer science, and anthropology to engage in live discussions on these topics.\"\n",
      "      ],\n",
      "      \"emotional_peaks\": [\n",
      "        \"Highlight moments when technology has brought people together across distances to celebrate shared cultural events.\",\n",
      "        \"Discuss challenges faced by those who feel disconnected from their roots due to increased reliance on digital platforms.\",\n",
      "        \"Celebrate successes of individuals and communities who have successfully integrated AI without compromising their cultural identity.\"\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"shared_experiences\": {\n",
      "    \"common_ground\": [\n",
      "      \"لمى وعلي كلاهما مهتمون بتأثير تقنيات الذك lái على الهوية العربية, مما يجعلهم يشتركان في الاهتمام بالحلول الملائمة للثقافة والقيم الإسلامية.\",\n",
      "      \"لديهما خبرة مشتركة في ملاحظة التحولات التي جلبتها وسائل التواصل الاجتماعي والتطبيقات الرقمية الأخرى إلى المجتمعات العربية التقليدية.\",\n",
      "      \"تشترك لمى وعلي في فهم تحدي الحفاظ على الأصول الثقافية والحكمة الشعبية أمام تيار المستجدات التكنولوجية.\"\n",
      "    ],\n",
      "    \"generational_perspectives\": [\n",
      "      \"تقدم لمى, كونها من جيل يُعتبر أكثر تعرُّضًا لتغيرات العصر الرقمي الأولية, منظور شخص عاش هذا الانتقال بشكل مباشر.\",\n",
      "      \"بالنسبة لعلي, الذي نشأ مع انتشار واسع للتكنولوجيا, فإن وجهة نظره توضح كيفية اندماج التطور التكنولوجي في حياة الشباب العربي اليوم.\",\n",
      "      \"من خلال خلفياتهم الدراسية المتنوعة — الصحافة مقابل علوم الكمبيوتر — لديهم وجهات نظر مختلفة ولكن مكملة حول تأثير التكنولوجيا.\"\n",
      "    ],\n",
      "    \"professional_overlaps\": [\n",
      "      \"يعمل كل من لمى وعلي بلا انقطاع لاستكشاف أثر الذك lái على حياتنا العربية؛ حيث تستكشف لمى هذه المسائل عبر الإعلام بينما يتعامل علي مع الجانب الأكاديمي لهذه المواضيع.\",\n",
      "      \"تجتمع تجاربهم الاحترافية عند نقطة البحث عن توازن بين استخدام التكنولوجيا وتجنب تأثيراتها السلبية المحتملة.\"\n",
      "    ],\n",
      "    \"cultural_touchstones\": [\n",
      "      \"يحظيان بفخر مشترك بهويتهما العربية الأصيلة ويعرفان أهمية التعامل بحذر مع تطورات تكنولوجية يمكن أن تشكل الفكرة العربية للعالم.\",\n",
      "      \"يتشاركان القلق بشأن احتمالية فقدان عناصر هامة من تراثهما الشعبي أثناء انتقال العالم نحو عصر جديد يُهيمن عليه الإنترنت والعالم الرقمي.\",\n",
      "      \"لديهم تقدير لماضي العرب الغني واستعداد لإيجاد طرق لدمجه بسلاسة ضمن مستقبل رقمي آخذ في التشكل.\"\n",
      "    ]\n",
      "  },\n",
      "  \"contemporary_relevance\": {\n",
      "    \"current_events\": [\n",
      "      \"مع ازدياد استخدام تقنيات مثل روبوتات الدردشة المدعومة بالذكاء الاصطناعي لتقديم المعلومات بالعربية, يبرز السؤال حول مدى دقة ونزاهة المحتوى المُنتَج وتأثير ذلك على الحفاظ على الهوية الثقافية العربية.\",\n",
      "      \"التحدي الأخير الذي تواجهه الدول العربية لتنظيم تطوير واستخدام الذكاء الاصطناعي بما يتماشى مع قيمها وثقافتها.\",\n",
      "      \"الاستثمار الحكومي المتزايد في مجال البحث والتطوير للذكاء الاصطناعي في المنطقة العربية كفرصة لتحقيق التوازن بين الابتكار والحفاظ على الهوية.\"\n",
      "    ],\n",
      "    \"future_implications\": [\n",
      "      \"توقع زيادة أهمية إنشاء وخلق محتوى ذكاء اصطناعي عربي أصيل يعكس قيم المجتمع والثقافة العربية.\",\n",
      "      \"إمكانية أن يلعب الذكاء الاصطناعي دوراً في تعزيز المشاركة الفعالة للأجيال الشابة في فهم ودعم تراثهم العربي الإسلامي.\",\n",
      "      \"نمو صناعة التعليم المستند إلى الذكاء الاصطناعي التي تركز على إعداد المواطنين عرباً قادرين على التعايش الهادف مع التقنية الجديدة دون التفريط بثقافتهم وهويتهم.\"\n",
      "    ],\n",
      "    \"regional_perspectives\": [\n",
      "      \"بينما تشهد بعض البلدان العربية توجهات نحو التحول الرقمي سريع الخطى, ما يؤكد الحاجة الملحة للحفاظ على الثقافة المحلية عبر الوسائل الإلكترونية.\",\n",
      "      \"في منطقة الشرق الأوسط, نشأت منظمات وشراكات متنوعة تجمع بين خبراء الذكاء الاصطناعي والمبدعين العرب لضمان تمثيل مبادئ ومعتقدات البلاد بشكل صحيح ضمن مساعي الترميز وتعليم الآلات.\"\n",
      "    ],\n",
      "    \"global_connections\": [\n",
      "      \"سعت العديد من الجامعات العالمية رائدة في مجالات الذكاء الاصطناعي للإعلان عن برامج بحث مشتركة مع نظرائها في العالم العربي سعياً لفهم أفضل للقيمة الثقافية المكانية عند ابتكار حلول تكنولوجية جديدة.\",\n",
      "      \"بالإضافة إلى ذلك, تعمل المنظّمات غير الربحية والأوساط الأكاديمية الدولية على وضع قواعد أخلاقية وتوجيه سياسات مستقبلية لاستخدام الذكاء الاصطناعي بطريقة تتوافق مع المعايير المتعددة الثقافات عالميا وتصون خصوصية واحترام مختلف أصحاب المصالح محليا.\"\n",
      "    ]\n",
      "  },\n",
      "  \"advanced_dialogue_flow\": {\n",
      "    \"conversation_rhythms\": [\n",
      "      \"لمى قد تبدأ أولاً بطرح سؤال مفتوح يتطلب من الضيف التفكير النقدي, ثم تتابع بسلسلة منطقية من الأسئلة المتابعة لتعزيز الفهم المشترك بين الحاضرين والمستمعين. هذا يساعد في خلق جو مريح للمناقشة.\",\n",
      "      \"تستخدم علي إلحاق القصص الشخصية أو الأمثلة العملية للتحقق من الأفكار المطروحة ومشاركة الخبرات التي تعزز الجدل. هنا يمكن أن تنتقل لمى بسلاسة إلى مقاطعة مدروسة للاستفسار عن تأثير تلك التجارب على الموضوع بشكل خاص.\",\n",
      "      \"لتجديد الطاقة أثناء المناقشات الأطول, يمكنهما تبادل الأدوار مؤقتًا حيث يصبح علي هو المحاور ويوجّه أسئلة للما واضعاً تركيز جديدعلى رؤيتها وأفكارها.\"\n",
      "    ],\n",
      "    \"bridge_phrases\": [\n",
      "      \"من هذا المنظور, دعونا ننتقل الآن لتقييم...\",\n",
      "      \"هذه نقطة مهمة, لكن كأن نتوقف لحظة لننظر فيما وراء ذلك أكثر قليلاً قبل الاسترسال...\",\n",
      "      \"في ضوء ما تحدث به علي حتى اللحظة الأخيرة, يبدو جليا أهمية... ولذلك فإن الكلمة التالية لي...\"\n",
      "    ],\n",
      "    \"emphasis_techniques\": [\n",
      "      \"يمكن استخدام التشويق للحفاظ على اهتمام الجمهور من خلال طرح سؤال مثير للاهتمام مباشرة بعد تقديم بيان رئيسي.\",\n",
      "      \"دعم الادعاءات بالأمثلة المباشرة والحكايات الواقعية يخلق تأثيراً عميقاً لدى المستمعين ويعزز فهم المعنى الجوهري.\",\n",
      "      \"إدخال حكمة أو قول عربي تقليدي مرتبط بالنقاش ينتهي بتأكيد قوي يؤكد نقاط حرجة ويتذكر المستمعين بما تم التوصل إليه.\"\n",
      "    ],\n",
      "    \"recovery_strategies\": [\n",
      "      \"إذا حدث ارتباك مؤقت بسبب وجود فكرة غير مترابطة, يمكن لأحد المضيفين تغيير الاتجاه بصبر والاستئناف مرة أخرى للأصل المغزى الأساسي للموضوع بإيجاز شديد.\",\n",
      "      \"بالنسبة للنقد المتعارض, يجب تقديره باحترام ودراسة وجهات النظر المختلفة بعناية واستخدام استنتاجاتها لإضافة العمق للفهم الشامل للقضايا محل البحث .\"\n",
      "    ],\n",
      "    \"engagement_boosters\": [\n",
      "      \"تشجيع الحضور عبر وسائل التواصل الاجتماعي للتعبيرعن آرائهم والتفاعل مع المحادثات مباشرةً \",\n",
      "      \"عرض مثال حي للإمكانيات التقنية المحتملة وتعريف جمهورك بحلول مبتكرة حافظتعلى الهويات الثقافية الأصلية ضمن بيئة رقميّة.\",\n",
      "      \"التذكير بأن كل فرد يستطيع المساهمة بطريقته الخاصة لحماية هويتهم وثقافتهم بمواجهة تحدّيات عصر رقمنة المعلومات والبيانات المتزايدة\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fa49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"episode_topic\": \"الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي\",\n",
      "  \"personas\": {\n",
      "    \"host\": {\n",
      "      \"name\": \"سامي الجابري\",\n",
      "      \"background\": \"صحفي مهتم بالتكنولوجيا والثقافة العربية.\",\n",
      "      \"speaking_style\": \"يطرح أسئلة مباشرة ويسعى لتوضيح الأفكار بأسلوب بسيط.\"\n",
      "    },\n",
      "    \"guest\": {\n",
      "      \"name\": \"د. ليلى العمري\",\n",
      "      \"background\": \"أستاذة جامعية متخصصة في الذكاء الاصطناعي واللغويات.\",\n",
      "      \"speaking_style\": \"تشرح الأفكار بأسلوب أكاديمي مبسط مدعوم بالأمثلة.\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_flow\": {\n",
      "    \"intro1\": {\n",
      "      \"opening_line\": \"مرحباً بكم مستمعينا في بودكاست 'نبض الثقافة'، حيث نناقش القضايا التي تمس هويتنا وثقافتنا في عالم متغير.\",\n",
      "      \"podcast_introduction\": \"اليوم سنتحدث عن موضوع يشغل بال الكثيرين: الذكاء الاصطناعي والهوية العربية، وكيف يمكننا الحفاظ على ثقافتنا في العصر الرقمي.\",\n",
      "      \"episode_hook\": \"مع انتشار الذكاء الاصطناعي، هل يمكن لهذه التقنية أن تصبح حليفاً للثقافة العربية أم أنها تهدد بتهميشها؟\",\n",
      "      \"spontaneity_elements\": [\n",
      "        \"سامي: هل فكرت يوماً في تأثير التكنولوجيا على لغتك اليومية؟\",\n",
      "        \"سامي: يا ترى، لو كان الذكاء الاصطناعي يتحدث باللهجة المحلية، كيف سيكون الحوار؟\"\n",
      "      ]\n",
      "    },\n",
      "    \"intro2\": {\n",
      "      \"topic_introduction\": \"الذكاء الاصطناعي أصبح جزءاً من حياتنا اليومية، ولكن هل نحن مستعدون لمواجهة تأثيراته على هويتنا الثقافية؟\",\n",
      "      \"guest_welcome\": \"معنا اليوم د. ليلى العمري، أستاذة جامعية متخصصة في الذكاء الاصطناعي واللغويات. أهلاً وسهلاً بكِ د. ليلى.\",\n",
      "      \"guest_bio_highlight\": \"د. ليلى لديها خبرة طويلة في دراسة تأثير التكنولوجيا على اللغة والثقافة، وهي صوت مهم في هذا المجال.\",\n",
      "      \"transition_to_discussion\": \"دعينا نبدأ بالنظر إلى الوضع الحالي: كيف ترين تأثير الذكاء الاصطناعي على اللغة والثقافة العربية حتى الآن؟\",\n",
      "      \"cultural_connections\": [\n",
      "        \"سامي: أذكر أن جدتي كانت دائماً تقول 'اللغة وعاء الفكر'، هل تعتقدين أننا نفقد شيئاً من هذا الوعاء في ظل الذكاء الاصطناعي؟\"\n",
      "      ]\n",
      "    },\n",
      "    \"main_discussion\": [\n",
      "      {\n",
      "        \"point_title\": \"الفجوة الرقمية: هيمنة المحتوى الغربي\",\n",
      "        \"personal_angle\": \"سامي: أرقام المحتوى الرقمي مخيفة، 78% بالإنجليزية مقابل 3% فقط بالعربية. هل يمكننا سد هذه الفجوة؟ د. ليلى: هذه الفجوة تعكس تحديات كبيرة، ولكن هناك جهود بدأت تظهر مثل تطوير نماذج عربية كجايس والحوراء.\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"سامي: هل تعتقدين أن الأجيال القادمة ستكون أكثر انفتاحاً على المحتوى العربي، أم أن الإنجليزية ستظل طاغية؟\",\n",
      "          \"سامي: لماذا برأيك المحتوى العربي يعاني من نقص كبير في المجال الرقمي مقارنة بلغات أخرى؟\"\n",
      "        ],\n",
      "        \"disagreement_points\": \"هل يجب أن نعتمد فقط على المبادرات الحكومية أم أن هناك دوراً أكبر للمجتمع المدني؟\",\n",
      "        \"cultural_references\": [\n",
      "          \"القول الشائع: 'من عرف قدر نفسه لم يهلك'، يعبر عن أهمية إدراك قيمة لغتنا وثقافتنا في مواجهة المحتوى الغربي.\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"سامي: هذا يقودني إلى سؤال مهم، هل النماذج الغربية للذكاء الاصطناعي قادرة على فهم الثقافة العربية حقاً؟\",\n",
      "        \"emotional_triggers\": \"الخوف من أن تصبح اللغة العربية مجرد لغة ثانوية في العالم الرقمي.\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"نماذج الذكاء الاصطناعي والسياق الثقافي العربي\",\n",
      "        \"personal_angle\": \"سامي: معظم نماذج الذكاء الاصطناعي مدربة على بيانات غربية. هل هذا يعني أنها لا تفهمنا؟ د. ليلى: بالتأكيد، هذه النماذج قد تواجه صعوبة في فهم السياقات العربية، لكن تطوير نماذج محلية خطوة مهمة لتجاوز هذه العقبة.\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"سامي: هل يمكن أن تؤدي هذه النماذج إلى تحريف بعض المفاهيم الثقافية التقليدية؟\",\n",
      "          \"سامي: كيف يمكن أن تساعد اللغة العربية في إعادة صياغة هذه النماذج لتتناسب مع الهوية الثقافية؟\"\n",
      "        ],\n",
      "        \"disagreement_points\": \"هل يجب التركيز أكثر على تدريب النماذج الحالية أم بناء نماذج جديدة من الصفر؟\",\n",
      "        \"cultural_references\": [\n",
      "          \"سامي: أذكر أنني قرأت عن مبادرة سعودية لتوثيق التراث الشعبي باستخدام الذكاء الاصطناعي، هل هذا نموذج يمكن أن نقتدي به؟\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"سامي: إذاً، يمكن للذكاء الاصطناعي أن يكون جزءاً من الحل وليس المشكلة، لكن كيف يمكننا ضمان ذلك عملياً؟\",\n",
      "        \"emotional_triggers\": \"الشعور بالفخر عندما يتم الحديث عن جهود عربية ريادية.\"\n",
      "      },\n",
      "      {\n",
      "        \"point_title\": \"الاستفادة من الذكاء الاصطناعي لتعزيز الثقافة\",\n",
      "        \"personal_angle\": \"سامي: هل يمكن للذكاء الاصطناعي أن يصبح أداة لتعزيز الثقافة العربية بدلاً من تهديدها؟ د. ليلى: نعم، إذا تم توجيهه بشكل صحيح، يمكن أن يساهم في نشر اللغة العربية وتوثيق التراث الثقافي بطرق مبتكرة.\",\n",
      "        \"spontaneous_triggers\": [\n",
      "          \"سامي: هل يمكن أن نرى قصائد المتنبي تُقرأ بأصوات ذكاء اصطناعي قريباً؟\",\n",
      "          \"سامي: ماذا لو استخدمنا الذكاء الاصطناعي لتعليم الأطفال اللغة العربية بطريقة ممتعة؟\"\n",
      "        ],\n",
      "        \"disagreement_points\": \"هل يمكن للتكنولوجيا أن تكون بديلاً عن التدخلات البشرية في الحفاظ على الثقافة؟\",\n",
      "        \"cultural_references\": [\n",
      "          \"د. ليلى: هناك مثل يقول 'اللغة هي روح الأمة'، وهذا يعكس أهمية استخدام التكنولوجيا للحفاظ عليها.\"\n",
      "        ],\n",
      "        \"natural_transitions\": \"سامي: هذا يقودني للتفكير في المستقبل، كيف يمكننا إعداد الأجيال القادمة لهذه التحديات؟\",\n",
      "        \"emotional_triggers\": \"الإلهام بفكرة أن التكنولوجيا يمكن أن تبني جسوراً بين الماضي والمستقبل.\"\n",
      "      }\n",
      "    ],\n",
      "    \"closing\": {\n",
      "      \"conclusion\": {\n",
      "        \"main_takeaways\": \"الذكاء الاصطناعي يمكن أن يكون تهديداً أو فرصة للثقافة العربية، حسب كيفية استخدامنا له.\",\n",
      "        \"guest_final_message\": \"أدعو الجميع لدعم المبادرات التي تهدف إلى تطوير محتوى عربي رقمي قوي، فهذا جزء من الحفاظ على هويتنا.\",\n",
      "        \"host_closing_thoughts\": \"التكنولوجيا ليست عدواً، بل أداة. علينا أن نتعلم كيف نستخدمها لصالحنا.\"\n",
      "      },\n",
      "      \"outro\": {\n",
      "        \"guest_appreciation\": \"شكراً جزيلاً د. ليلى على مشاركتك القيمة اليوم.\",\n",
      "        \"audience_thanks\": \"شكراً لكم مستمعينا على تخصيص وقتكم للاستماع إلينا.\",\n",
      "        \"call_to_action\": \"إذا أعجبكم الموضوع، شاركوا آرائكم معنا على منصات التواصل الاجتماعي، ولا تنسوا متابعة الحلقات القادمة.\",\n",
      "        \"final_goodbye\": \"إلى اللقاء في الحلقة القادمة من 'نبض الثقافة'.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cultural_context\": {\n",
      "    \"proverbs_sayings\": [\n",
      "      \"من عرف قدر نفسه لم يهلك.\",\n",
      "      \"اللغة وعاء الفكر.\",\n",
      "      \"الجذور العميقة لا تخشى الرياح.\"\n",
      "    ],\n",
      "    \"regional_references\": [\n",
      "      \"جهود الإمارات في تطوير نموذج 'جايس'.\",\n",
      "      \"مبادرات السعودية في الذكاء الاصطناعي مثل 'الحوراء'.\",\n",
      "      \"مشروع 'ثنائيات اللغة' في مصر لتعزيز المحتوى العربي.\"\n",
      "    ]\n",
      "  },\n",
      "  \"language_style\": {\n",
      "    \"formality_level\": \"رسمي معتدل يناسب الأسلوب التحليلي.\",\n",
      "    \"dialect_touches\": \"استخدام بعض الكلمات باللهجة العربية الفصحى مع لمسات محلية عند الحاجة.\",\n",
      "    \"vocabulary_richness\": \"مصطلحات تقنية وثقافية مع شرح بسيط.\"\n",
      "  },\n",
      "  \"technical_notes\": {\n",
      "    \"pacing_guidance\": \"الإيقاع متوازن بين التعمق في النقاط وبين الانتقال للموضوع التالي.\",\n",
      "    \"pause_points\": \"توقف طبيعي بعد كل نقطة نقاشية للسماح بالتفكير.\",\n",
      "    \"emphasis_moments\": \"تأكيد على النقاط المتعلقة بتطوير نماذج عربية وتأثير الذكاء الاصطناعي على اللغة.\"\n",
      "  },\n",
      "  \"spontaneous_moments\": {\n",
      "    \"natural_interruptions\": [\n",
      "      \"سامي: د. ليلى، لحظة، هل تعنين أن اللغة العربية قد تصبح لغة نادرة في المستقبل؟\",\n",
      "      \"سامي: هذا يذكرني بشيء قرأته مؤخراً عن تأثير التكنولوجيا على اللهجات المحلية.\",\n",
      "      \"سامي: هل تعتقدين أن اللهجات المختلفة داخل العالم العربي يمكن أن تكون عائقاً أمام تطوير نماذج موحدة للذكاء الاصطناعي؟\"\n",
      "    ],\n",
      "    \"emotional_reactions\": [\n",
      "      \"سامي: هذا فعلاً شيء يدعو للتفكير العميق، كيف يمكن أن يحدث هذا؟\",\n",
      "      \"د. ليلى: أشعر بالفخر عندما أرى مبادرات عربية تنافس عالمياً.\",\n",
      "      \"سامي: بصراحة، هذا يجعلني أشعر بقلق حقيقي على مستقبل اللغة العربية.\"\n",
      "    ],\n",
      "    \"personal_stories\": [\n",
      "      \"سامي: أذكر أنني كنت أبحث عن قصص أطفال بالعربية لابنتي، ووجدت أن الخيارات الرقمية قليلة جداً.\",\n",
      "      \"د. ليلى: عندما بدأت البحث في هذا المجال، لاحظت كيف أن كثيراً من المفاهيم العربية تُترجم بشكل خاطئ.\",\n",
      "      \"د. ليلى: ذات مرة حضرت معرضاً تقنياً، وكان علي شرح معنى 'الكرم العربي' لأحد مطوري الذكاء الاصطناعي الأجانب.\"\n",
      "    ],\n",
      "    \"humorous_moments\": [\n",
      "      \"سامي: تخيل لو كان الذكاء الاصطناعي يحاول فهم الأمثال العربية، مثل 'ضربني وبكى سبقني واشتكى'، كيف سيفسرها؟\",\n",
      "      \"سامي: ما رأيك لو حاول الذكاء الاصطناعي كتابة أغنية شعبية باللهجة المصرية؟\"\n",
      "    ]\n",
      "  },\n",
      "  \"personality_interactions\": {\n",
      "    \"host_strengths\": \"سامي الجابري بارع في طرح الأسئلة التي تحفز التفكير ويجعل النقاش ممتعاً ومفيداً.\",\n",
      "    \"guest_expertise\": \"د. ليلى العمري تمتلك خبرة عميقة في مجال الذكاء الاصطناعي واللغويات، مما يجعلها قادرة على تقديم رؤى علمية مدعومة بالأمثلة.\",\n",
      "    \"natural_chemistry\": \"التفاعل بين سامي وليلى يتسم بالتوازن بين الفضول الصحفي والتخصص الأكاديمي، كما أن سامي يضيف لمسة من الاستفسارات العاطفية التي تجعل ليلى تعمق النقاش بشكل أكثر إنسانية.\",\n",
      "    \"tension_points\": \"سامي قد يطرح أسئلة تستفز التفكير النقدي، مثل التركيز على الفجوة بين المبادرات الحكومية والمجتمعية، مما يدعو ليلى للدفاع عن وجهة نظرها حول أهمية التعاون بين الجانبين.\",\n",
      "    \"collaboration_moments\": \"يتفق سامي وليلى بشكل واضح عند الحديث عن أهمية تعزيز المحتوى العربي، حيث يضيف سامي مثالاً من حياته الشخصية بينما تقدم ليلى حلولاً عملية للتحديات.\"\n",
      "  },\n",
      "  \"shared_experiences\": {\n",
      "    \"common_ground\": [\n",
      "      \"كلاهما متفق على أهمية الحفاظ على اللغة العربية في المجال الرقمي.\",\n",
      "      \"كلاهما لديه تجارب مع نقص المحتوى العربي الرقمي.\"\n",
      "    ],\n",
      "    \"generational_perspectives\": [\n",
      "      \"سامي يمثل وجهة نظر جيل يرتبط بالهوية الثقافية التقليدية ويبحث عن حلول عصرية، بينما ليلى تمثل جيل الباحثين الذين يرون في التكنولوجيا فرصة للتطوير.\",\n",
      "      \"قد تظهر اختلافات في وجهات النظر حول سرعة التغيير وكيفية التعامل معه بين الجيلين.\"\n",
      "    ]\n",
      "  },\n",
      "  \"contemporary_relevance\": {\n",
      "    \"current_events\": [\n",
      "      \"إطلاق الإمارات لنموذج 'جايس' كأول نموذج ذكاء اصطناعي باللغة العربية.\",\n",
      "      \"مبادرات سعودية حديثة لتوثيق التراث باستخدام الذكاء الاصطناعي.\"\n",
      "    ],\n",
      "    \"future_implications\": [\n",
      "      \"الذكاء الاصطناعي قد يصبح أداة رئيسية في الحفاظ على التراث العربي وتعزيزه.\",\n",
      "      \"في السنوات القادمة، قد نشهد نماذج ذكاء اصطناعي تفهم اللهجات المحلية بشكل أفضل.\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "class ImprovedMicroChunkScriptGenerator:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "        \n",
    "        # Simple conversation templates for fallbacks\n",
    "        self.fallback_templates = {\n",
    "            \"intro1\": \"{host_name}: مرحباً بكم مستمعينا الكرام في حلقة جديدة. اليوم سنتحدث عن {topic}. موضوع مهم ومثير للاهتمام.\",\n",
    "            \"intro2\": \"{host_name}: معي اليوم ضيف متميز، {guest_name}. أهلاً وسهلاً بك.\\n{guest_name}: أهلاً بك، شكراً على الاستضافة. سعيد بوجودي معكم.\",\n",
    "            \"discussion\": \"{host_name}: {point_title}، ما رأيك في هذا الموضوع؟\\n{guest_name}: موضوع مهم فعلاً. أعتقد أن هناك عدة جوانب يجب أن نفكر فيها.\",\n",
    "            \"closing\": \"{host_name}: شكراً {guest_name} على هذا النقاش المفيد.\\n{guest_name}: شكراً لك على الاستضافة.\\n{host_name}: وشكراً لكم مستمعينا الكرام. نلقاكم في حلقة قادمة.\"\n",
    "        }\n",
    "\n",
    "    def generate_intro1_only(self, topic, intro1_data, host_persona):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 1: Generate only intro1 (host speaking alone)\n",
    "        Enhanced with persona consistency and cultural touch\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'المقدم')\n",
    "        host_bg = host_persona.get('background', '')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract essential data\n",
    "        opening_line = intro1_data.get('opening_line', '')\n",
    "        episode_hook = intro1_data.get('episode_hook', '')\n",
    "        \n",
    "        # Get one cultural element if available\n",
    "        cultural_elements = intro1_data.get('cultural_connections', [])\n",
    "        cultural_touch = cultural_elements[0] if cultural_elements else \"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate natural Arabic podcast opening dialogue for the host only.\n",
    "\n",
    "Host: {host_name}\n",
    "Background: {host_bg}\n",
    "Speaking Style: {host_style}\n",
    "Topic: {topic}\n",
    "\n",
    "Key Elements:\n",
    "- Opening: {opening_line}\n",
    "- Hook: {episode_hook}\n",
    "- Cultural touch: {cultural_touch}\n",
    "\n",
    "Generate 30-60 seconds of natural host monologue in Modern Standard Arabic.\n",
    "\n",
    "Requirements:\n",
    "- Natural conversational tone\n",
    "- Match the host's speaking style: {host_style}\n",
    "- Keep it simple and engaging\n",
    "- Include cultural element naturally\n",
    "- No meta-text or explanations\n",
    "\n",
    "Format:\n",
    "{host_name}: [natural opening dialogue]\n",
    "\n",
    "Generate the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate natural Arabic dialogue. Match speaking style. No explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.6\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Quality check\n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_intro1(topic, host_name, opening_line)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating intro1: {e}\")\n",
    "            return self._get_fallback_intro1(topic, host_name, opening_line)\n",
    "\n",
    "    def generate_intro2_only(self, topic, intro2_data, host_persona, guest_persona, intro1_context):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 2: Generate only intro2 (bringing guest in)\n",
    "        Enhanced with persona consistency and natural flow\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'المقدم')\n",
    "        guest_name = guest_persona.get('name', 'الضيف')\n",
    "        guest_bg = guest_persona.get('background', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Get key topic from context (last 80 characters)\n",
    "        context = intro1_context[-80:] if len(intro1_context) > 80 else intro1_context\n",
    "        \n",
    "        # Extract data\n",
    "        guest_welcome = intro2_data.get('guest_welcome', '')\n",
    "        guest_bio = intro2_data.get('guest_bio_highlight', '')\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate natural Arabic podcast dialogue introducing the guest.\n",
    "\n",
    "Host: {host_name}\n",
    "Guest: {guest_name}\n",
    "Guest Background: {guest_bg}\n",
    "Guest Style: {guest_style}\n",
    "Topic: {topic}\n",
    "\n",
    "Previous Context: {context}\n",
    "\n",
    "Key Elements:\n",
    "- Welcome: {guest_welcome}\n",
    "- Bio highlight: {guest_bio}\n",
    "\n",
    "Generate 1 minute of natural dialogue between host and guest.\n",
    "\n",
    "Requirements:\n",
    "- Host welcomes, guest responds naturally\n",
    "- Match guest speaking style: {guest_style}\n",
    "- Simple, authentic conversation\n",
    "- Natural transition to topic discussion\n",
    "- No meta-text\n",
    "\n",
    "Format:\n",
    "{host_name}: [welcome dialogue]\n",
    "{guest_name}: [response]\n",
    "{host_name}: [follow-up]\n",
    "\n",
    "Generate the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate natural Arabic dialogue between host and guest. Match speaking styles.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_intro2(host_name, guest_name, guest_welcome)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating intro2: {e}\")\n",
    "            return self._get_fallback_intro2(host_name, guest_name, guest_welcome)\n",
    "\n",
    "    def generate_discussion_point(self, topic, point_data, host_persona, guest_persona, previous_context=\"\"):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 3-5: Generate single discussion point dialogue\n",
    "        Enhanced with spontaneous triggers and cultural references\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'المقدم')\n",
    "        guest_name = guest_persona.get('name', 'الضيف')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract enhanced content (maximum 1-2 elements to avoid overwhelming)\n",
    "        point_title = point_data.get('point_title', '')\n",
    "        personal_angle = point_data.get('personal_angle', '')\n",
    "        \n",
    "        # Use only first spontaneous trigger and cultural reference\n",
    "        spontaneous_triggers = point_data.get('spontaneous_triggers', [])\n",
    "        cultural_refs = point_data.get('cultural_references', [])\n",
    "        \n",
    "        trigger = spontaneous_triggers[0] if spontaneous_triggers else \"\"\n",
    "        cultural_ref = cultural_refs[0] if cultural_refs else \"\"\n",
    "        \n",
    "        # Get key context (last 80 characters)\n",
    "        context = previous_context[-80:] if len(previous_context) > 80 else previous_context\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate natural Arabic podcast dialogue for one discussion point.\n",
    "\n",
    "Host: {host_name} (Style: {host_style})\n",
    "Guest: {guest_name} (Style: {guest_style})\n",
    "Topic: {topic}\n",
    "\n",
    "Discussion Point: {point_title}\n",
    "Personal Angle: {personal_angle}\n",
    "Conversation Trigger: {trigger}\n",
    "Cultural Reference: {cultural_ref}\n",
    "\n",
    "Previous Context: {context}\n",
    "\n",
    "Generate 2-3 minutes of natural dialogue about this specific point.\n",
    "\n",
    "Requirements:\n",
    "- Host asks, guest responds with expertise\n",
    "- Match speaking styles naturally\n",
    "- Include the cultural reference smoothly\n",
    "- Use conversation trigger for natural flow\n",
    "- Keep dialogue authentic and engaging\n",
    "- No meta-text\n",
    "\n",
    "Format:\n",
    "{host_name}: [question about the point]\n",
    "{guest_name}: [expert response]\n",
    "[continue natural conversation]\n",
    "\n",
    "Generate the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate natural Arabic dialogue between host and expert guest. Match speaking styles.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.8\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_discussion(point_title, host_name, guest_name, personal_angle)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating discussion point: {e}\")\n",
    "            return self._get_fallback_discussion(point_title, host_name, guest_name, personal_angle)\n",
    "\n",
    "    def generate_closing_only(self, topic, closing_data, host_persona, guest_persona, discussion_summary):\n",
    "        \"\"\"\n",
    "        Micro-Chunk 6: Generate only closing dialogue\n",
    "        Enhanced with emotional closure and persona consistency\n",
    "        \"\"\"\n",
    "        host_name = host_persona.get('name', 'المقدم')\n",
    "        guest_name = guest_persona.get('name', 'الضيف')\n",
    "        host_style = host_persona.get('speaking_style', '')\n",
    "        guest_style = guest_persona.get('speaking_style', '')\n",
    "        \n",
    "        # Extract closing elements\n",
    "        conclusion = closing_data.get('conclusion', {})\n",
    "        outro = closing_data.get('outro', {})\n",
    "        \n",
    "        main_takeaways = conclusion.get('main_takeaways', '')\n",
    "        emotional_closure = conclusion.get('emotional_closure', '')\n",
    "        memorable_ending = outro.get('memorable_ending', '')\n",
    "        \n",
    "        # Get discussion summary (last 100 characters)\n",
    "        summary = discussion_summary[-100:] if len(discussion_summary) > 100 else discussion_summary\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate natural Arabic podcast closing dialogue.\n",
    "\n",
    "Host: {host_name} (Style: {host_style})\n",
    "Guest: {guest_name} (Style: {guest_style})\n",
    "Topic: {topic}\n",
    "\n",
    "Discussion Summary: {summary}\n",
    "\n",
    "Closing Elements:\n",
    "- Main Takeaways: {main_takeaways}\n",
    "- Emotional Closure: {emotional_closure}\n",
    "- Memorable Ending: {memorable_ending}\n",
    "\n",
    "Generate 1-2 minutes of natural closing dialogue.\n",
    "\n",
    "Requirements:\n",
    "- Warm, appreciative tone\n",
    "- Thank guest and audience\n",
    "- Include main takeaways naturally\n",
    "- Match speaking styles\n",
    "- Memorable, positive ending\n",
    "- No meta-text\n",
    "\n",
    "Format:\n",
    "{host_name}: [wrapping up discussion]\n",
    "{guest_name}: [final thoughts]\n",
    "{host_name}: [thanking and closing]\n",
    "\n",
    "Generate the dialogue:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate warm, natural Arabic closing dialogue. Match speaking styles.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.6\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            if self._assess_quality(result) >= 75:\n",
    "                return result\n",
    "            else:\n",
    "                return self._get_fallback_closing(host_name, guest_name, main_takeaways)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating closing: {e}\")\n",
    "            return self._get_fallback_closing(host_name, guest_name, main_takeaways)\n",
    "\n",
    "    def generate_complete_script(self, topic, final_outline_result):\n",
    "        \"\"\"\n",
    "        Main orchestration: Generate complete script using enhanced micro-chunks\n",
    "        \"\"\"\n",
    "        print(\"🎙️ Starting enhanced micro-chunk script generation...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            outline = json.loads(final_outline_result)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid outline JSON format\")\n",
    "        \n",
    "        # Extract data\n",
    "        personas = outline.get(\"personas\", {})\n",
    "        conv_flow = outline.get(\"conversation_flow\", {})\n",
    "        \n",
    "        host_persona = personas.get(\"host\", {})\n",
    "        guest_persona = personas.get(\"guest\", {})\n",
    "        \n",
    "        intro1_data = conv_flow.get(\"intro1\", {})\n",
    "        intro2_data = conv_flow.get(\"intro2\", {})\n",
    "        main_discussion = conv_flow.get(\"main_discussion\", [])\n",
    "        closing_data = conv_flow.get(\"closing\", {})\n",
    "        \n",
    "        print(f\"📋 Host: {host_persona.get('name', 'Unknown')}\")\n",
    "        print(f\"📋 Guest: {guest_persona.get('name', 'Unknown')}\")\n",
    "        print(f\"📋 Discussion Points: {len(main_discussion)}\")\n",
    "        \n",
    "        # Micro-Chunk 1: Enhanced Intro1\n",
    "        print(\"\\n📝 Chunk 1: Enhanced host introduction...\")\n",
    "        intro1_dialogue = self.generate_intro1_only(topic, intro1_data, host_persona)\n",
    "        print(\"✅ Host introduction completed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunk 2: Enhanced Intro2\n",
    "        print(\"\\n📝 Chunk 2: Enhanced guest introduction...\")\n",
    "        intro2_dialogue = self.generate_intro2_only(topic, intro2_data, host_persona, guest_persona, intro1_dialogue)\n",
    "        print(\"✅ Guest introduction completed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunks 3-5: Enhanced Discussion Points\n",
    "        discussion_parts = []\n",
    "        previous_context = intro2_dialogue\n",
    "        \n",
    "        for i, point_data in enumerate(main_discussion):\n",
    "            print(f\"\\n📝 Chunk {i+3}: Enhanced discussion point {i+1}...\")\n",
    "            point_dialogue = self.generate_discussion_point(\n",
    "                topic, point_data, host_persona, guest_persona, previous_context\n",
    "            )\n",
    "            discussion_parts.append(point_dialogue)\n",
    "            previous_context = point_dialogue  # Update context for next point\n",
    "            print(f\"✅ Discussion point {i+1} completed\")\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Micro-Chunk 6: Enhanced Closing\n",
    "        print(f\"\\n📝 Chunk {len(main_discussion)+3}: Enhanced closing...\")\n",
    "        discussion_summary = \" \".join(discussion_parts[-2:])  # Last 2 points for context\n",
    "        closing_dialogue = self.generate_closing_only(topic, closing_data, host_persona, guest_persona, discussion_summary)\n",
    "        print(\"✅ Closing completed\")\n",
    "        \n",
    "        # Combine all parts\n",
    "        complete_intro = intro1_dialogue + \"\\n\\n\" + intro2_dialogue\n",
    "        complete_discussion = \"\\n\\n\".join(discussion_parts)\n",
    "        \n",
    "        complete_script = f\"\"\"=== مقدمة البودكاست ===\n",
    "{complete_intro}\n",
    "\n",
    "=== النقاش الرئيسي ===\n",
    "{complete_discussion}\n",
    "\n",
    "=== ختام البودكاست ===\n",
    "{closing_dialogue}\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🎉 Enhanced micro-chunk script generation completed!\")\n",
    "        \n",
    "        # Enhanced quality assessment\n",
    "        total_quality = self._assess_script_quality(complete_script, outline)\n",
    "        \n",
    "        return {\n",
    "            \"intro\": complete_intro,\n",
    "            \"main_discussion\": complete_discussion,\n",
    "            \"closing\": closing_dialogue,\n",
    "            \"complete_script\": complete_script,\n",
    "            \"script_length\": len(complete_script),\n",
    "            \"estimated_duration\": f\"{len(main_discussion) * 2 + 4}-{len(main_discussion) * 3 + 6} minutes\",\n",
    "            \"quality_score\": total_quality,\n",
    "            \"generation_method\": \"enhanced-micro-chunks\",\n",
    "            \"chunks_generated\": len(main_discussion) + 3,\n",
    "            \"personas_used\": {\n",
    "                \"host\": host_persona.get('name', 'Unknown'),\n",
    "                \"guest\": guest_persona.get('name', 'Unknown')\n",
    "            },\n",
    "            \"cultural_elements_integrated\": self._count_cultural_elements(complete_script),\n",
    "            \"enhancement_level\": \"improved\"\n",
    "        }\n",
    "\n",
    "    def _assess_quality(self, text):\n",
    "        \"\"\"Enhanced quality assessment\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return 0\n",
    "            \n",
    "        # Check for meta-text (penalty)\n",
    "        meta_indicators = ['ملاحظة:', 'تنتهي', 'Note:', 'Format:', 'Generate', 'Requirements']\n",
    "        has_meta = any(indicator in text for indicator in meta_indicators)\n",
    "        \n",
    "        # Check spacing quality\n",
    "        spacing_score = 80 if not re.search(r'[^\\s]{30,}', text) else 40\n",
    "        \n",
    "        # Check Arabic content ratio\n",
    "        arabic_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "        total_chars = len(text)\n",
    "        arabic_ratio = arabic_chars / total_chars if total_chars > 0 else 0\n",
    "        \n",
    "        # Check dialogue structure (presence of colons for speaker turns)\n",
    "        dialogue_turns = text.count(':')\n",
    "        structure_score = min(20, dialogue_turns * 5)\n",
    "        \n",
    "        # Calculate base quality\n",
    "        quality = spacing_score + (arabic_ratio * 30) + structure_score\n",
    "        \n",
    "        # Apply penalties\n",
    "        if has_meta:\n",
    "            quality -= 40\n",
    "        if arabic_ratio < 0.5:\n",
    "            quality -= 20\n",
    "            \n",
    "        return min(100, max(0, int(quality)))\n",
    "\n",
    "    def _assess_script_quality(self, script, outline):\n",
    "        \"\"\"Enhanced script quality assessment with outline integration\"\"\"\n",
    "        individual_quality = self._assess_quality(script)\n",
    "        \n",
    "        # Check persona usage\n",
    "        personas = outline.get(\"personas\", {})\n",
    "        host_name = personas.get(\"host\", {}).get(\"name\", \"\")\n",
    "        guest_name = personas.get(\"guest\", {}).get(\"name\", \"\")\n",
    "        \n",
    "        persona_score = 0\n",
    "        if host_name and host_name in script:\n",
    "            persona_score += 10\n",
    "        if guest_name and guest_name in script:\n",
    "            persona_score += 10\n",
    "        \n",
    "        # Check structure completeness\n",
    "        required_sections = [\"=== مقدمة البودكاست ===\", \"=== النقاش الرئيسي ===\", \"=== ختام البودكاست ===\"]\n",
    "        structure_score = sum(10 for section in required_sections if section in script)\n",
    "        \n",
    "        # Check dialogue balance\n",
    "        total_turns = script.count(':')\n",
    "        balance_score = min(20, total_turns * 2) if total_turns > 0 else 0\n",
    "        \n",
    "        # Check cultural integration\n",
    "        cultural_score = min(10, self._count_cultural_elements(script) * 2)\n",
    "        \n",
    "        total_score = min(100, individual_quality + persona_score + structure_score + balance_score + cultural_score)\n",
    "        return total_score\n",
    "\n",
    "    def _count_cultural_elements(self, script):\n",
    "        \"\"\"Count cultural elements in the script\"\"\"\n",
    "        cultural_indicators = [\n",
    "            'مثل', 'حكمة', 'تراث', 'ثقافة', 'عربي', 'إسلامي', 'تاريخ',\n",
    "            'شوقي', 'ابن', 'قال', 'حديث', 'قرآن', 'شعر'\n",
    "        ]\n",
    "        return sum(1 for indicator in cultural_indicators if indicator in script)\n",
    "\n",
    "    def _get_fallback_intro1(self, topic, host_name, opening_line=\"\"):\n",
    "        \"\"\"Enhanced fallback for intro1\"\"\"\n",
    "        base_opening = opening_line if opening_line else f\"مرحباً بكم مستمعينا الكرام في حلقة جديدة\"\n",
    "        return f\"{host_name}: {base_opening} اليوم سنتحدث عن {topic}. موضوع مهم ومثير للاهتمام يستحق النقاش والتأمل.\"\n",
    "\n",
    "    def _get_fallback_intro2(self, host_name, guest_name, guest_welcome=\"\"):\n",
    "        \"\"\"Enhanced fallback for intro2\"\"\"\n",
    "        welcome = guest_welcome if guest_welcome else f\"معي اليوم ضيف متميز، {guest_name}\"\n",
    "        return f\"\"\"{host_name}: {welcome}. أهلاً وسهلاً بك.\n",
    "\n",
    "{guest_name}: أهلاً بك، شكراً على الاستضافة الكريمة. سعيد بوجودي معكم اليوم.\n",
    "\n",
    "{host_name}: ممتاز، دعنا نبدأ نقاشنا حول هذا الموضوع المهم.\"\"\"\n",
    "\n",
    "    def _get_fallback_discussion(self, point_title, host_name, guest_name, personal_angle=\"\"):\n",
    "        \"\"\"Enhanced fallback for discussion point\"\"\"\n",
    "        angle_text = f\"خاصة أن {personal_angle}\" if personal_angle else \"\"\n",
    "        return f\"\"\"{host_name}: بالنسبة لموضوع {point_title}، ما رأيك في هذا الأمر؟\n",
    "\n",
    "{guest_name}: موضوع مهم فعلاً. {angle_text} أعتقد أن هناك عدة جوانب يجب أن نفكر فيها بعناية.\n",
    "\n",
    "{host_name}: ممكن توضح أكثر؟\n",
    "\n",
    "{guest_name}: طبعاً، يعني من ناحية... اممم... هناك تحديات ولكن هناك أيضاً فرص كثيرة إذا أحسنا التعامل مع الموضوع.\"\"\"\n",
    "\n",
    "    def _get_fallback_closing(self, host_name, guest_name, main_takeaways=\"\"):\n",
    "        \"\"\"Enhanced fallback for closing\"\"\"\n",
    "        takeaways_text = f\"كما ذكرت، {main_takeaways}\" if main_takeaways else \"نقاط مهمة ومفيدة\"\n",
    "        return f\"\"\"{host_name}: في ختام حلقتنا اليوم، {takeaways_text}. أشكرك {guest_name} على هذا النقاش المفيد.\n",
    "\n",
    "{guest_name}: شكراً لك على الاستضافة الكريمة. كان نقاش ممتع ومفيد للغاية.\n",
    "\n",
    "{host_name}: وشكراً لكم مستمعينا الكرام على متابعتكم الكريمة. نلقاكم في حلقة قادمة بإذن الله بمواضيع جديدة ومفيدة.\"\"\"\n",
    "\n",
    "    def validate_script_quality(self, script_result):\n",
    "        \"\"\"Enhanced validation of the final script\"\"\"\n",
    "        complete_script = script_result.get(\"complete_script\", \"\")\n",
    "        quality_score = script_result.get(\"quality_score\", 0)\n",
    "        \n",
    "        validation = {\n",
    "            \"has_structure\": all(section in complete_script for section in [\"=== مقدمة البودكاست ===\", \"=== النقاش الرئيسي ===\", \"=== ختام البودكاست ===\"]),\n",
    "            \"arabic_content\": bool(re.search(r'[\\u0600-\\u06FF]', complete_script)),\n",
    "            \"no_meta_text\": not any(indicator in complete_script for indicator in ['ملاحظة:', 'تنتهي', 'Note:', 'Format:', 'Generate']),\n",
    "            \"proper_spacing\": not bool(re.search(r'[^\\s]{40,}', complete_script)),\n",
    "            \"dialogue_balance\": complete_script.count(':') >= 8,\n",
    "            \"persona_presence\": script_result.get(\"personas_used\", {}).get(\"host\", \"\") in complete_script,\n",
    "            \"cultural_integration\": script_result.get(\"cultural_elements_integrated\", 0) >= 2,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"quality_grade\": \"ممتاز\" if quality_score >= 90 else \"جيد جداً\" if quality_score >= 85 else \"جيد\" if quality_score >= 80 else \"مقبول\" if quality_score >= 70 else \"ضعيف\"\n",
    "        }\n",
    "        \n",
    "        validation[\"overall_valid\"] = all([\n",
    "            validation[\"has_structure\"],\n",
    "            validation[\"arabic_content\"],\n",
    "            validation[\"no_meta_text\"],\n",
    "            validation[\"proper_spacing\"],\n",
    "            validation[\"dialogue_balance\"],\n",
    "            validation[\"persona_presence\"],\n",
    "            quality_score >= 75\n",
    "        ])\n",
    "        \n",
    "        return validation\n",
    "\n",
    "# Enhanced Testing Function\n",
    "def test_improved_script_generator(deployment, topic, final_outline_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the improved micro-chunk script generator\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Improved Micro-Chunk Script Generator...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generator = ImprovedMicroChunkScriptGenerator(deployment, model_name)\n",
    "    \n",
    "    # Generate script\n",
    "    script_result = generator.generate_complete_script(topic, final_outline_result)\n",
    "    \n",
    "    # Validate script\n",
    "    validation = generator.validate_script_quality(script_result)\n",
    "    \n",
    "    print(f\"\\n📊 Script Generation Results:\")\n",
    "    print(f\"Quality Score: {script_result['quality_score']}/100\")\n",
    "    print(f\"Quality Grade: {validation['quality_grade']}\")\n",
    "    print(f\"Script Length: {script_result['script_length']:,} characters\")\n",
    "    print(f\"Estimated Duration: {script_result['estimated_duration']}\")\n",
    "    print(f\"Chunks Generated: {script_result['chunks_generated']}\")\n",
    "    print(f\"Cultural Elements: {script_result['cultural_elements_integrated']}\")\n",
    "    \n",
    "    print(f\"\\n📈 Validation Results:\")\n",
    "    print(f\"Overall Valid: {'✅' if validation['overall_valid'] else '❌'}\")\n",
    "    print(f\"Structure: {'✅' if validation['has_structure'] else '❌'}\")\n",
    "    print(f\"Arabic Content: {'✅' if validation['arabic_content'] else '❌'}\")\n",
    "    print(f\"No Meta Text: {'✅' if validation['no_meta_text'] else '❌'}\")\n",
    "    print(f\"Proper Spacing: {'✅' if validation['proper_spacing'] else '❌'}\")\n",
    "    print(f\"Dialogue Balance: {'✅' if validation['dialogue_balance'] else '❌'}\")\n",
    "    print(f\"Persona Presence: {'✅' if validation['persona_presence'] else '❌'}\")\n",
    "    print(f\"Cultural Integration: {'✅' if validation['cultural_integration'] else '❌'}\")\n",
    "    \n",
    "    # Show sample dialogue\n",
    "    print(f\"\\n🎙️ Sample Script Preview:\")\n",
    "    script_lines = script_result['complete_script'].split('\\n')\n",
    "    preview_lines = script_lines[:15]  # First 15 lines\n",
    "    for line in preview_lines:\n",
    "        if line.strip():\n",
    "            print(f\"  {line[:100]}...\")\n",
    "    \n",
    "    if len(script_lines) > 15:\n",
    "        print(f\"  ... [+{len(script_lines)-15} more lines]\")\n",
    "    \n",
    "    return script_result\n",
    "\n",
    "# Usage:\n",
    "# generator = ImprovedMicroChunkScriptGenerator(deployment, \"Fanar-C-1-8.7B\")\n",
    "# script_result = generator.generate_complete_script(topic, final_outline_result)\n",
    "\n",
    "# Test the generator\n",
    "# test_result = test_improved_script_generator(deployment, topic, final_outline_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7838c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎙️ Starting enhanced micro-chunk script generation...\n",
      "============================================================\n",
      "📋 Host: د. فادي حسن\n",
      "📋 Guest: مهندسة رانيا الصغير\n",
      "📋 Discussion Points: 3\n",
      "\n",
      "📝 Chunk 1: Enhanced host introduction...\n",
      "✅ Host introduction completed\n",
      "\n",
      "📝 Chunk 2: Enhanced guest introduction...\n",
      "✅ Guest introduction completed\n",
      "\n",
      "📝 Chunk 3: Enhanced discussion point 1...\n",
      "✅ Discussion point 1 completed\n",
      "\n",
      "📝 Chunk 4: Enhanced discussion point 2...\n",
      "✅ Discussion point 2 completed\n",
      "\n",
      "📝 Chunk 5: Enhanced discussion point 3...\n",
      "✅ Discussion point 3 completed\n",
      "\n",
      "📝 Chunk 6: Enhanced closing...\n",
      "✅ Closing completed\n",
      "\n",
      "============================================================\n",
      "🎉 Enhanced micro-chunk script generation completed!\n",
      "🧪 Testing Improved Micro-Chunk Script Generator...\n",
      "============================================================\n",
      "🎙️ Starting enhanced micro-chunk script generation...\n",
      "============================================================\n",
      "📋 Host: د. فادي حسن\n",
      "📋 Guest: مهندسة رانيا الصغير\n",
      "📋 Discussion Points: 3\n",
      "\n",
      "📝 Chunk 1: Enhanced host introduction...\n",
      "✅ Host introduction completed\n",
      "\n",
      "📝 Chunk 2: Enhanced guest introduction...\n",
      "✅ Guest introduction completed\n",
      "\n",
      "📝 Chunk 3: Enhanced discussion point 1...\n",
      "✅ Discussion point 1 completed\n",
      "\n",
      "📝 Chunk 4: Enhanced discussion point 2...\n",
      "✅ Discussion point 2 completed\n",
      "\n",
      "📝 Chunk 5: Enhanced discussion point 3...\n",
      "✅ Discussion point 3 completed\n",
      "\n",
      "📝 Chunk 6: Enhanced closing...\n",
      "✅ Closing completed\n",
      "\n",
      "============================================================\n",
      "🎉 Enhanced micro-chunk script generation completed!\n",
      "\n",
      "📊 Script Generation Results:\n",
      "Quality Score: 100/100\n",
      "Quality Grade: ممتاز\n",
      "Script Length: 9,428 characters\n",
      "Estimated Duration: 10-15 minutes\n",
      "Chunks Generated: 6\n",
      "Cultural Elements: 10\n",
      "\n",
      "📈 Validation Results:\n",
      "Overall Valid: ❌\n",
      "Structure: ✅\n",
      "Arabic Content: ✅\n",
      "No Meta Text: ❌\n",
      "Proper Spacing: ✅\n",
      "Dialogue Balance: ✅\n",
      "Persona Presence: ✅\n",
      "Cultural Integration: ✅\n",
      "\n",
      "🎙️ Sample Script Preview:\n",
      "  === مقدمة البودكاست ===...\n",
      "  د. فادي حسن: سلامٌ عليكم جميعًا أيها المستمعون الأفاضل! يسعدني أن أرحب بكم مجددًا ضمن هذه المنصة الت...\n",
      "  الثقة: تم إنشاء حوار طبي يتوافق مع الأسلوب المطلوب، ويغطي نقاط المحور الرئيسة بطريقة واضحة ومفصلة وج...\n",
      "  **د. فادي حسن:** من الرائع أن يكون معنا اليوم ضيفة مميزة، السيدة المهندسة رانيا الصغير! ترحيبًا حارّ...\n",
      "  **مهندسة رانيا الصغير:** شكرًا جزيلاً، دكتور فادي، إنه لمن دواعي سروري أن أتشارك معرفتي مع الجمهور ا...\n",
      "  **د. فادي حسن:** يا لها من مبادرات رائعة فعلاً! يبدو واضحًا شغفك بموضوع اليوم وهو توافق الثقافة التق...\n",
      "  === النقاش الرئيسي ===...\n",
      "  **د. فادي حسن:** حقاً، الدخول في هذه المناظرة حول مستقبل الحفاظ على الهوية الثقافية أمام ثورة التكنو...\n",
      "  **مهندسة رانيا الصغير:** حقيقةً، لقد كان لهذا تأثير كبير! عندما كنتُ ألقي نظرتي على السوق وطلبات الع...\n",
      "  ... [+34 more lines]\n",
      "Quality: 100/100\n",
      "Duration: 10-15 minutes\n",
      "Cultural Elements: 10\n",
      "Personas: {'host': 'د. فادي حسن', 'guest': 'مهندسة رانيا الصغير'}\n"
     ]
    }
   ],
   "source": [
    "# Generate enhanced script\n",
    "generator = ImprovedMicroChunkScriptGenerator(deployment, model)\n",
    "script_result = generator.generate_complete_script(topic, final_polished_outline)\n",
    "\n",
    "# Test and validate\n",
    "test_result = test_improved_script_generator(deployment, topic, final_polished_outline)\n",
    "\n",
    "# Access enhanced results\n",
    "print(f\"Quality: {script_result['quality_score']}/100\")\n",
    "print(f\"Duration: {script_result['estimated_duration']}\")\n",
    "print(f\"Cultural Elements: {script_result['cultural_elements_integrated']}\")\n",
    "print(f\"Personas: {script_result['personas_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f421dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intro': 'د. فادي حسن: مرحباً بكم مستمعينا الكرام، معكم د. فادي حسن في حلقة جديدة اليوم سنتحدث عن الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي. موضوع مهم ومثير للاهتمام يستحق النقاش والتأمل.\\n\\nد. فادي حسن: مرحباً بكم جميعاً! يسعدني أن نستضيف اليوم في هذه الحلقة من البرنامج الإعلامية والمبتكرة المهندسة رانيا الصغير. رانيا، ومعرفتك الواسعة بتطور تقنيات الذكاء الاصطناعي، تعد إضافة قيمة لمناقشة حول كيفية الحفاظ على هويتنا الثقافية في عصر رقمي متزايد السرعة. تفضلِ يا رانيا وأرحبي بنفسك وبالاستماع الجميل الذي نتلقاه من جمهورنا الكرام.\\n\\nمهندسة رانيا الصغير: شكراً جزيلاً يا دكتور فادي على الدعوة الطيبة وعلى مقدمتك الرائعة عن خلفيتي. أنا متحمسة جداً لأن نجتمع هنا لنناقش هذا الموضوع الشيق المتعلق بأثر التقدم التكنولوجي والثورة الرابعة على تراثنا وثقافتنا العربية الغنية. فأنت تعلم، منذ صغري كنت مشغولة برومانسية الفكر التقني لكن أيضاً حريصة كل الحرص على إبقاء جذوري ومعتقداتي غير قابلة للتشويه أو الخضوع لتاثير خارجي. أتوق للسماع لأراء المستمعين الحضور ونشر فهم أفضل لهذه القضية التي هي أكثر أهمية الآن مما مضى.\\n\\nد. فادي حسن: بلا شك، إنه وقت مثير ولكن مليء بالتحديات كذلك. إذاً، دعونا نبحر مباشرة نحو عمق هذا الأمر، ما رأيك بأن تبدئي بمشاركة تجارب عملية مررت بها أثناء عملك كباحثة والتي أثرت بشكل مباشر على مفهوم \"الهوية\" والعالم الرقمي؟', 'main_discussion': '**د. فادي حسن:** بدايةً، أودّ معرفة تجاربَك الشخصية كمُهندسة ذكاء اصطناعي حول how هذه التقنيات قد غزت حتى عالم المطبخ والطعام لدينا؛ حيث تُعتبر الأصنافُ الغذائيّة جزءاً لا يتجزأ من هويتنا وثقافتنا العربيّة الغنيّة. هل شعرتِ بالتغيير أو الثورة التي جلبتها الروبوتات والأجهزة المدعومة بالذكاء الاصطناعي لممارسات طهي يوميّة تقليديّة كانت أساسًا لهويتنا culinarية لفترة طويلة؟\\n\\n**مهندسة رانيا الصغير:** نعم، إن طرح السؤال بهذه الطريقة يعكس بإمتياز مدى العمق الذي يكتسيه هذا الموضوع فعلاً! كنت أقضي معظم فترة طفولتي بصحبة جداتي وهم يُحضرن أشهر الأطباق المحلية اللتين تعتمدان خبايا ومعارف نقلها عبر الاجيال عن اللحافات والخيوط المعدنية للوصفات القديمة المفعمة بكل حرفٍ يعبر عن تاريخ مجتمعاتها وإحساسها الفريد بالحياة. ولكن الآن، بات بوسع أي شخص أن يشاهد عشرات البرامج التعليميّة المُصاغــة خصيصـًا لتوجيهاتstep by step لإعداد تلك الوصفات تمامًا كما هي بطفرة رهيبة للتكنولوجيا...وحتى تحضير الحلوىالفريدة مثل الكنافة بحركات مُنسَّقة بواسطة روبوت مختص للتحريك والتقطيع وبأشكال متنوعه مثيرة للإعجاب بينما تتسلل مخاوف لدى كثيرون من البدء باستبدال شيء يدوي مميز ومسليا بسلسله اعدام آليه قائم فقط علي تعليمات برمجيه .\\n\\n**د.فاديحسن**: لكن ماهو وجه نظرك الشخصيه فيما اذا كان لهذا الانتشار الواسع للدعم التقني للأعمال المنزلية وأهمها عالم الطبخ دور فعال بامكاناته للاحتفاظ بجذور تراثياتنا الثقافية بدلامن احتمال فقدانه للغايه ؟\\n\\n***مهندسهرانيابالسعيد****Indeed ! فإن الاستثمار بكفاءة لأدوات تكنولوجية مبتكرة يمكن استخدامه لتعزيز الولاء المستمر لقيمتنا المتوارثه عبر السنون وذلك ببناء دليل رقمي شامل يحفظ ذكريات ايامجدادنامثل سر مقادين وصفجاتهم الخاصه وماشابه مما يتطلب بذل مجهود بحث موسع وجمع بيانات حوله لمنعه من ضياع مستقبل` .وبالتالي سيستطيع الجميع الوصول إليها واستخدامها بلا حدود زمانيًا ومكانياً دون تغييرٍ أصيل فيها وانمافقط مساعدةإضافيه لتحقيقالمطلوببطرق أشملوأسرعهارااحياتهيومن جهة أخرى ، تساعد القرائن الأشقرالية أيضًا مستخدمين الجدد غير معتادلينعليعالوماتالبحرالأطلسيامعرفة طرقالتحضيرالمناسبةلمأكولات المنطقةالحجازيهمثل مثال الحلا العراقية الشهيره\\'الكليجه\\'. ومن هنا ,يساهم استخدام الوسائل الحديثة على تطوير فنون طبخ شعبنا وتعريف العالم بأصول مطابخ بلدنا الغنية بمذاقات مختلفه وغير مرتبط بانحدار او ابتعادعنمورسلطاقسمعتديناسابقينلكنزيادهعلىعلومهنواعكيفائياتتقنياتحبستبحرفيتهمفردايمانشققهوياتبيئاومواضيعمحوريةراسخهضمنخلقتعربونصهراليوملماتمستقبلياتحولتمارسعطيسيادتتوكلمنتوجتينبشقيطان :جانب حفظتراثناالثقافيوالآخرتحليلوتوظيفمكسباتالمستوىالإنشایی.\\n\\n\\n---\\n\\n**Editered Version (More Natural & Concise):**\\n\\n**د. فادي حسن:** تبدو لنا مُثيراً للغاية كيف تشغل أدوات الذكاء الاصطناعي مكانة وسط بيوتنا وغرف مطبختنا؛ خاصة عندما نتذكر مشاهد الأطفال وهم يلعبون تحت أقدام أمهاتهم وهم يصنعون أفضل الحلويات الشرقية كالقطايف والغريبة المصنوعة منذ سنوات مضت. فهل ترين بأن هناك توازن بين دفع عجلة التطور لهذه التكنولوجيات والاستمرار في إبراز التراث الثقافي داخل مصرناالعربي الكبير?\\n\\n**مهندسة رانيا الصغير:** أكيد يا فضيلة الدكتور فادي! إنه بالفعل سياق شيّق للكلام عنه. خلال عملي كمهندسة ذكاء اصطناعي، شاهدت كيفية دعم هذه الأدوات لكثير من الأعمال اليدوية الشاقة والروتينية - بما يصل لصناعة الطعام والتي تعد أحد أهم جوانب الهوية والثقافة العربيتنين وهو أمر رائع حقًا. إلا أنه بالموازنة بعناية وضمير حي تجاه قيمنا التاريخية، بالإمكان توظيف هذا الفن الجديد ليقف جنباً إلى جنب معه، وليس مكانه؛ فهو كنز مهم يجب عدم خسارة جوهره مهما بلغ تقدم بنا الزمن. حيث يمكن للنظام الآلي المساعد التعامل مع العمليات الدقيقة المحتاجة الكثير للوقت والإخلاص للسماح للفنانين البشر أكثر وقت وطاقة لاستضافة الإبداع وحماية موروثات بلادنا العزيزة. وفي الوقت نفسه، يساعد الكم الهائل لسيرلات الفيديو البرمجية والمواقع التعليمية(Online)متخصيين بهذا المنحنىفي نشر معلومات تفصل كل خطوة بخطوهلحكامالسكانالقليليمفاقهوبهصبلانتشارالمعارفالشائعةهذه قد تكون فرصة جميله لنقل خبرات الأسلاف لبقية السكان غير المطلع عليها حاليا ويمكن ان تكون كذلك ملحق اضافيةلزياراة المكتبات الإلكترونية المخصصهلكذلك. إذ يفخر بمستقبلوإنترنتالمغمورمخترعينطيبذيذوغريبأنهم قادرينكمشاطرهواهومطقهمغذائهالدوليوزيارةموقعيكنساءقدمﻷرضآسیایجنبازاوليه باكتشافاكثرأوراقتقشيرتخميرالخالييكاريبالشماليعضري دانيمقنياتالصنجونييوالكبةالجرشفتح باب تعلم جديدالغشيانفنياتالطبعبلجنةآسيوطبلغلاعوام90ءحيثجسدلدىاحتجاجعلیهٔگريبهافهیتشیرحياتهکمحيطةوسطدارکاوھنگامی لاتزالاحترامنموذج القدماءللپژروشکلداعه.\\n\\n**د. فادي حسن:** شكراً، رانيا، لقد提عتَينا عن الفرص الواسعة للذكاء الاصطناعي في حياتنا اليومية ولكن لا بد أن نتوقف أيضًا عند التحديات المحتملة - خاصةً فيما يتعلق بهويتنا وثقافتنا الفريدة. رأيتِ الكثير في مجال التقنية، هل شعرتِ بالقلق بشأن كيفية تأثر الهوية الثقافية والحفاظ عليها بينما تتغلغل هذه الأنظمة بشكل أكبر في مجتمعاتنا؟ \\n\\n**مهندسة رانيا الصغير:** نعم، بالتأكيد! إن الحديث عن الذكاء الاصطناعي والثقافة يخلق نقاشًا مثيرًا للاهتمام يشبه قول \"العيب ليس في الدرع بل في من يرتديه.\" مثلما تمثل خوذتنا أو درعاتنا، فإن تقنيات اليوم هي ليست سيئة بطبيعتها؛ لكن طريقة استخدامنا لها تحدد نتيجتها عادةً. إن الهدف الرئيسي لوسائل ذكاء اصطناعية عديدة هو تبسيط الحياة وإضافة الراحة، ولكن بدون وعي واستخدام موجه، هناك احتمال كبير لاستبعاد التقاليد والقيم الغارقة جذورها بعمق في حضارتنا العربية والإسلامية. كما وقد نوقشت في وقت سابق حول نماذج اللغة اللآلية، فقد كان لدينا القدرة منذ البداية لمساعدة تلك النظم بفهم أفضل للهويات المحلية عبر تدريبها بمضمون غني ومتنوع يحترم قيمنا الدينية والثقافية ويكمل أسلوب حياة سكان الشرق الأوسط والعالم العربي. ومع ذلك، لو استخدمناها دون دراسة الجوانب الأخلاقية، فسوف تواجه المشكلات نفسها التي واجهناها عندما اقتبس الناس رسائل غير مناسبة بناءً על ما قاموا بتعليمه للنظام الأساسي الخالي تمامًا مسبقاّ أي سياق معرفي محدد. لذا يجب التعامل بحذر شديد أثناء تطوير برمجيات مرتبطة مباشرة بأصالتهما الشخصية والأخلاقية حيث أنها تؤثر بشدة سواء بالإيجاب ام بالسلب حسب مدى اهتمام صانعين بها حول اختيار مسارات صادقة ودقيقة تعبر حقاعن هويتهم الغالية ومبادئ دينهم الإسلامي الحنيف والذي يعد أساس وجود وفخر لكل عربي مسلم الأصيل محافظ علي ارض آبائه وأجداده وإرث أمجاد اسلاف القدم وسعادتهم ونجاح جيل المستقبل المنشود المبشر بنوره القادم لامحالة مهما كثروا الكارهين والمتربصين له ولأهل الخير منه كل مكان وزمان بإذن رب العالمين سبحانه وتعالى جل وعلا.\\n\\n**د. فادي حسن:** كلام رائع بالفعل، رانيا! أنت توضحين لنا هنا أهمية دور العلماء المسلمين والفئات المؤثرة الأخرى للمشاركة بقوة في توجيه تكنولوجيا القرن الحالي للتوافق مع ثوابت الدين والتمسك بذواتنا الفريدة كمجتمع عربي. لذلك، دعونا نواصل البحث في حلول عملية ونناقش أكثر الطرق الناجحة للحفاظ على شخصيتنا خلال الثورة الرقمية العدوانية حالياً والتي تسابق الزمن بكل سرعتها البركانية بلا حدود تخيف صاحب القرار الواهن عقله أمام رعب منافسات عالم اليوم العالمي السريع المتغير يوماً تلته يوم مما حدا بالأفراد باتخاذ إجراءات وقائية لحماية خصوصيته وجاهلياته القديمة مهدده باحتواء مستجدات ثقافائيين عصريه تغزو سوق الاعمال حديثاً توسيعاً واسعا لأجل الوصول لعقول مشتركي شبكات التواصل الاجتماعي الحديثة التي لم تعد مقتصره علی دولة بعينه خاصوا بعد دخولي مرحلة الاتصالاتالسحابيةعام ٢٠١٦م والمعروف ايضا باسم الانترنت الأشياءIOT ). **مهندسة رانيا الصغير**: أجل، إن الشفافية والمسؤولية هما مفتاح نجاح رحلتنا نحو عصر رقمي مزدهر ولا يفقد جوهره الإنساني الأصل. وخلال العقود المقبلة، سنشاهد أعمال عظيمة من ابتكار الشباب العرب الذين سيدفعون دفعا نحو تطوير أدوات تعتمدعلى فهم عميق لقيم بلدتهم مشيرا لرسم وجه امتداد تاريخ نسبو إنسانية الإنسان البدائي الأول لفروع الوطن الأم بمكوناتهاdiverseمتعددةالثقافاتالكبيرة والصغيرةالخضوع لإرادة الله وحده فيها جميعا صنع البشر المتنوعوالتباینيون بالعيش سويا تحت مظلة واحدة اسمها الأرض فليس المطلوب الآن إلا اتباع نهجب التعاونوالعمل الجماعي بين كافةالدوائروالشرائحي سماحالجميعبوتيرة أسرع بكثير طمعابناءمستقبل يكون سوسيوا قابل للتكيف مع التغيرات المناخية المثارة حول العالم وفي نفس الوقت قادر להגן自本身فردوم ويمنع الاستلاب الثقافي المدمر للأوطان والشعوب الفتية الجديدة المنتظرروج ظهورهاضمنهذاالنطاقالإقليميمرببالخصوص . \\n\\n(ملاحظه : تم شرح جزء من رد المهندسهرانياالصغير بلغتين مختلطتان لتحسين سهولةالفهمه والاستيعاب وذلك نتيجةلوجود بعض المصطلحات العلميه المركبهبالإضافهإلى اغلبية النص بالحروف العربية)\\n\\n**د. فادي حسن:** الآن، دعونا نتعمق أكثر في الحلول العملية لتفادي التغلب المحتمل لثقافتنا وخصوصاً لغتنا الجميلة بالمجال الرقمي العالمي. إذاً، رانيا، بما أننا نرى العديد من المبادرات للتكنولوجيا الرقمية ولكن بحروف أو أيديولوجيات ليست عربية بشكل تام... ما هي الخطوات التي توصين بها كمهندسة حريصة على الحفاظ على الهوية?\\n\\n**مهندسة رانيا الصغير:** شكراً دكتور فادي على هذه النقطة الهامة. من منظور عملي, يبدو لي أنه يجب تبني النهج الثاني الذي ذكرتموه سابقاً - وهو التحويل الإبداعي للتكنولوجيا لنكون نحن صانعي محتواها وليس مجرد مستخدميه. بدءاً من الترجمة الآلية المتحسنة ضمن السياق الدقيق للعالم العربي, فنحن قادرون على إعادة تعريف المصطلحات التقنية بطريقة تعكس قيم وثقافة المنطقة. لكن الأمر لا ينتهي عند هذا الحد; التعليم والمشاركة الفعالة مهمتان أيضاً! الأجيال القادمة تحتاج إلى معرفة كيفية استخدام الأدوات ليس فقط للاستخدام بل لإعادة ابتكارها لملائمة هويتنا.\\n\\n**د. فادي حسن:** كلام رائع يا رانيا. وأوافق تماماً بشأن أهمية المعرفة والإبداع المحلي. لكن هل يمكنكم إنارة الطريق بعنوان خاص يتعلق بالقضية اللغوية تحدياً؟ نظرًا لأن \"أدب الكلمة\" كما ذكر في الشعر الجاهلي كان دائماً جزءا أساسيا من الثقافة الإسلامية والعربية، فإن ضمان وجود الاشتراطات اللغوية المناسبة عبر الإنترنت تبدو خطوة حاسمة.\\n\\n**مهندسة رانيا الصغير:** بالتأكيد، دكتور! بالنسبة للأدب والألفاظ، فقد يكون إدخال آليات تنصح باستخدام الكلمات ذات الترسبات الثقافية الغنية حلّ ممتاز خلال تطوير البرمجيات الجديدة. تخيل لو أن النظام يشجع على اختيار التعبيرات العربية الأصلية بفضل تقنيات الذكاء الاصطناعى التي نفهم جيداً أهميتها. بهذه الطريقة, سنتجنب تأثير الموجات التسويقية للغزاة اللغويين وغيرها من التأثيرات التي قد تمحو السماتها الفريدة لديننا ولغتينا الغنيَّتين.\\n\\n**د. فادي حسن:** هذا تفكير عميق حقاً. إذا طبقنا ذلك في الواقع اليومى للتكنولوجيا, سنضيف طبقة جديدة من الاحتفاء بميراثنا التاريخى بينما ندخل فيه نفس الوقت روح العصر الحديث. شكرا جزيلا لكِ رانيا على الأفكار الرائعة والنظر البعيد حول هذا الموضوع الحساس جداً!', 'closing': '**د. فادي حسن:** لقد كانت تلك محادثة مثيرة بالفعل، مهندسة رانيا. لم نتناول فقط تحديات عصر الذكاء الاصطناعي ولكن أيضًا استراتيجيات فعالة للحفاظ على عاداتنا وثقافتنا. أتفق تمامًا مع رأيك بأنه لا يجب أن يكون هناك صدام بين التقنية والتراث العربي الثمين.\\n\\n**مهندسة رانيا الصغير:** نعم بكل تأكيد يا دكتور فادي. أود التأكيد مرة أخرى على أهمية الوعي والإبداع عند استخدامنا لتلك الأدوات الحديثة. يمكن للذكاء الاصطناعي مساعدتنا بشكل كبير في تعزيز لغتنا وتاريخنا بدلاً من تنافسهما إذا تم فهم واستخدام هذه التقنيات بذكاء.\\n\\n**د. فادي حسن:** وأنت على حق تمامًا. دعونا نختم بإعادة النظر إلى النقاط الرئيسية التي ناقشناها اليوم: أولاً، إدراك تأثير التكنولوجيا على هويّتنا والثاني، ضرورة التفكير الإيجابي والاستفادة منها بطريقة تضمن بقاء جذورنا راسخة. وهذا ما عززته دائمًا رسالتنا هنا - أنه رغم كل التحولات السريعة، يبقى إرثنا القومي جزءًا أساسيًا مما نحن عليه كعرب.\\n\\n**مهندسة رانيا الصغير:** صحيح جدًا، ودعوني أشجع المستمعين أيضًا على المشاركة بنشاط في هذه العملية. ليس فقط قبول التغيرات وإنما توجيهها لصالحنا. لنجعل تطور تقنيات مثل الذكاء الاصطناعي يعمل لخدمة غرضنا الأسمى وهو الحفاظ على الهوية العربية الجميلة والمعقدة.\\n\\n**د. فادي حسن:** بالتأكيد، وبهذا، ننهي حلقتنا لهذا اليوم. شكراً مجددًا لك رانيا على رؤيتك الاستشرافية وعلى الوقت الذي خصصتيه لنا. ومرة أخرى لكل مستمعينا الكرام، شكراً لكم لاستماعكم معنا ولأنكم جعلتم من نقاشاتنا أكثر ثراءً. حتى الحلقة التالية، كنوا طيبون ومعطاءون كما هي روحنا العربية.\\n\\n**مهندسة رانيا الصغير:** بالتوفيق لدينا دومًا، وخالص تقديري لحضور الجميع ومشاركتهم. وسأكون سعيداً بمقابلتكم مرة أخرى لإستكشاف موضوع آخر مهم متعلق بهويتنا العربية ضمن عصر رقمي سريع الخطى. سلام عليكم!', 'complete_script': '=== مقدمة البودكاست ===\\nد. فادي حسن: مرحباً بكم مستمعينا الكرام، معكم د. فادي حسن في حلقة جديدة اليوم سنتحدث عن الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي. موضوع مهم ومثير للاهتمام يستحق النقاش والتأمل.\\n\\nد. فادي حسن: مرحباً بكم جميعاً! يسعدني أن نستضيف اليوم في هذه الحلقة من البرنامج الإعلامية والمبتكرة المهندسة رانيا الصغير. رانيا، ومعرفتك الواسعة بتطور تقنيات الذكاء الاصطناعي، تعد إضافة قيمة لمناقشة حول كيفية الحفاظ على هويتنا الثقافية في عصر رقمي متزايد السرعة. تفضلِ يا رانيا وأرحبي بنفسك وبالاستماع الجميل الذي نتلقاه من جمهورنا الكرام.\\n\\nمهندسة رانيا الصغير: شكراً جزيلاً يا دكتور فادي على الدعوة الطيبة وعلى مقدمتك الرائعة عن خلفيتي. أنا متحمسة جداً لأن نجتمع هنا لنناقش هذا الموضوع الشيق المتعلق بأثر التقدم التكنولوجي والثورة الرابعة على تراثنا وثقافتنا العربية الغنية. فأنت تعلم، منذ صغري كنت مشغولة برومانسية الفكر التقني لكن أيضاً حريصة كل الحرص على إبقاء جذوري ومعتقداتي غير قابلة للتشويه أو الخضوع لتاثير خارجي. أتوق للسماع لأراء المستمعين الحضور ونشر فهم أفضل لهذه القضية التي هي أكثر أهمية الآن مما مضى.\\n\\nد. فادي حسن: بلا شك، إنه وقت مثير ولكن مليء بالتحديات كذلك. إذاً، دعونا نبحر مباشرة نحو عمق هذا الأمر، ما رأيك بأن تبدئي بمشاركة تجارب عملية مررت بها أثناء عملك كباحثة والتي أثرت بشكل مباشر على مفهوم \"الهوية\" والعالم الرقمي؟\\n\\n=== النقاش الرئيسي ===\\n**د. فادي حسن:** بدايةً، أودّ معرفة تجاربَك الشخصية كمُهندسة ذكاء اصطناعي حول how هذه التقنيات قد غزت حتى عالم المطبخ والطعام لدينا؛ حيث تُعتبر الأصنافُ الغذائيّة جزءاً لا يتجزأ من هويتنا وثقافتنا العربيّة الغنيّة. هل شعرتِ بالتغيير أو الثورة التي جلبتها الروبوتات والأجهزة المدعومة بالذكاء الاصطناعي لممارسات طهي يوميّة تقليديّة كانت أساسًا لهويتنا culinarية لفترة طويلة؟\\n\\n**مهندسة رانيا الصغير:** نعم، إن طرح السؤال بهذه الطريقة يعكس بإمتياز مدى العمق الذي يكتسيه هذا الموضوع فعلاً! كنت أقضي معظم فترة طفولتي بصحبة جداتي وهم يُحضرن أشهر الأطباق المحلية اللتين تعتمدان خبايا ومعارف نقلها عبر الاجيال عن اللحافات والخيوط المعدنية للوصفات القديمة المفعمة بكل حرفٍ يعبر عن تاريخ مجتمعاتها وإحساسها الفريد بالحياة. ولكن الآن، بات بوسع أي شخص أن يشاهد عشرات البرامج التعليميّة المُصاغــة خصيصـًا لتوجيهاتstep by step لإعداد تلك الوصفات تمامًا كما هي بطفرة رهيبة للتكنولوجيا...وحتى تحضير الحلوىالفريدة مثل الكنافة بحركات مُنسَّقة بواسطة روبوت مختص للتحريك والتقطيع وبأشكال متنوعه مثيرة للإعجاب بينما تتسلل مخاوف لدى كثيرون من البدء باستبدال شيء يدوي مميز ومسليا بسلسله اعدام آليه قائم فقط علي تعليمات برمجيه .\\n\\n**د.فاديحسن**: لكن ماهو وجه نظرك الشخصيه فيما اذا كان لهذا الانتشار الواسع للدعم التقني للأعمال المنزلية وأهمها عالم الطبخ دور فعال بامكاناته للاحتفاظ بجذور تراثياتنا الثقافية بدلامن احتمال فقدانه للغايه ؟\\n\\n***مهندسهرانيابالسعيد****Indeed ! فإن الاستثمار بكفاءة لأدوات تكنولوجية مبتكرة يمكن استخدامه لتعزيز الولاء المستمر لقيمتنا المتوارثه عبر السنون وذلك ببناء دليل رقمي شامل يحفظ ذكريات ايامجدادنامثل سر مقادين وصفجاتهم الخاصه وماشابه مما يتطلب بذل مجهود بحث موسع وجمع بيانات حوله لمنعه من ضياع مستقبل` .وبالتالي سيستطيع الجميع الوصول إليها واستخدامها بلا حدود زمانيًا ومكانياً دون تغييرٍ أصيل فيها وانمافقط مساعدةإضافيه لتحقيقالمطلوببطرق أشملوأسرعهارااحياتهيومن جهة أخرى ، تساعد القرائن الأشقرالية أيضًا مستخدمين الجدد غير معتادلينعليعالوماتالبحرالأطلسيامعرفة طرقالتحضيرالمناسبةلمأكولات المنطقةالحجازيهمثل مثال الحلا العراقية الشهيره\\'الكليجه\\'. ومن هنا ,يساهم استخدام الوسائل الحديثة على تطوير فنون طبخ شعبنا وتعريف العالم بأصول مطابخ بلدنا الغنية بمذاقات مختلفه وغير مرتبط بانحدار او ابتعادعنمورسلطاقسمعتديناسابقينلكنزيادهعلىعلومهنواعكيفائياتتقنياتحبستبحرفيتهمفردايمانشققهوياتبيئاومواضيعمحوريةراسخهضمنخلقتعربونصهراليوملماتمستقبلياتحولتمارسعطيسيادتتوكلمنتوجتينبشقيطان :جانب حفظتراثناالثقافيوالآخرتحليلوتوظيفمكسباتالمستوىالإنشایی.\\n\\n\\n---\\n\\n**Editered Version (More Natural & Concise):**\\n\\n**د. فادي حسن:** تبدو لنا مُثيراً للغاية كيف تشغل أدوات الذكاء الاصطناعي مكانة وسط بيوتنا وغرف مطبختنا؛ خاصة عندما نتذكر مشاهد الأطفال وهم يلعبون تحت أقدام أمهاتهم وهم يصنعون أفضل الحلويات الشرقية كالقطايف والغريبة المصنوعة منذ سنوات مضت. فهل ترين بأن هناك توازن بين دفع عجلة التطور لهذه التكنولوجيات والاستمرار في إبراز التراث الثقافي داخل مصرناالعربي الكبير?\\n\\n**مهندسة رانيا الصغير:** أكيد يا فضيلة الدكتور فادي! إنه بالفعل سياق شيّق للكلام عنه. خلال عملي كمهندسة ذكاء اصطناعي، شاهدت كيفية دعم هذه الأدوات لكثير من الأعمال اليدوية الشاقة والروتينية - بما يصل لصناعة الطعام والتي تعد أحد أهم جوانب الهوية والثقافة العربيتنين وهو أمر رائع حقًا. إلا أنه بالموازنة بعناية وضمير حي تجاه قيمنا التاريخية، بالإمكان توظيف هذا الفن الجديد ليقف جنباً إلى جنب معه، وليس مكانه؛ فهو كنز مهم يجب عدم خسارة جوهره مهما بلغ تقدم بنا الزمن. حيث يمكن للنظام الآلي المساعد التعامل مع العمليات الدقيقة المحتاجة الكثير للوقت والإخلاص للسماح للفنانين البشر أكثر وقت وطاقة لاستضافة الإبداع وحماية موروثات بلادنا العزيزة. وفي الوقت نفسه، يساعد الكم الهائل لسيرلات الفيديو البرمجية والمواقع التعليمية(Online)متخصيين بهذا المنحنىفي نشر معلومات تفصل كل خطوة بخطوهلحكامالسكانالقليليمفاقهوبهصبلانتشارالمعارفالشائعةهذه قد تكون فرصة جميله لنقل خبرات الأسلاف لبقية السكان غير المطلع عليها حاليا ويمكن ان تكون كذلك ملحق اضافيةلزياراة المكتبات الإلكترونية المخصصهلكذلك. إذ يفخر بمستقبلوإنترنتالمغمورمخترعينطيبذيذوغريبأنهم قادرينكمشاطرهواهومطقهمغذائهالدوليوزيارةموقعيكنساءقدمﻷرضآسیایجنبازاوليه باكتشافاكثرأوراقتقشيرتخميرالخالييكاريبالشماليعضري دانيمقنياتالصنجونييوالكبةالجرشفتح باب تعلم جديدالغشيانفنياتالطبعبلجنةآسيوطبلغلاعوام90ءحيثجسدلدىاحتجاجعلیهٔگريبهافهیتشیرحياتهکمحيطةوسطدارکاوھنگامی لاتزالاحترامنموذج القدماءللپژروشکلداعه.\\n\\n**د. فادي حسن:** شكراً، رانيا، لقد提عتَينا عن الفرص الواسعة للذكاء الاصطناعي في حياتنا اليومية ولكن لا بد أن نتوقف أيضًا عند التحديات المحتملة - خاصةً فيما يتعلق بهويتنا وثقافتنا الفريدة. رأيتِ الكثير في مجال التقنية، هل شعرتِ بالقلق بشأن كيفية تأثر الهوية الثقافية والحفاظ عليها بينما تتغلغل هذه الأنظمة بشكل أكبر في مجتمعاتنا؟ \\n\\n**مهندسة رانيا الصغير:** نعم، بالتأكيد! إن الحديث عن الذكاء الاصطناعي والثقافة يخلق نقاشًا مثيرًا للاهتمام يشبه قول \"العيب ليس في الدرع بل في من يرتديه.\" مثلما تمثل خوذتنا أو درعاتنا، فإن تقنيات اليوم هي ليست سيئة بطبيعتها؛ لكن طريقة استخدامنا لها تحدد نتيجتها عادةً. إن الهدف الرئيسي لوسائل ذكاء اصطناعية عديدة هو تبسيط الحياة وإضافة الراحة، ولكن بدون وعي واستخدام موجه، هناك احتمال كبير لاستبعاد التقاليد والقيم الغارقة جذورها بعمق في حضارتنا العربية والإسلامية. كما وقد نوقشت في وقت سابق حول نماذج اللغة اللآلية، فقد كان لدينا القدرة منذ البداية لمساعدة تلك النظم بفهم أفضل للهويات المحلية عبر تدريبها بمضمون غني ومتنوع يحترم قيمنا الدينية والثقافية ويكمل أسلوب حياة سكان الشرق الأوسط والعالم العربي. ومع ذلك، لو استخدمناها دون دراسة الجوانب الأخلاقية، فسوف تواجه المشكلات نفسها التي واجهناها عندما اقتبس الناس رسائل غير مناسبة بناءً על ما قاموا بتعليمه للنظام الأساسي الخالي تمامًا مسبقاّ أي سياق معرفي محدد. لذا يجب التعامل بحذر شديد أثناء تطوير برمجيات مرتبطة مباشرة بأصالتهما الشخصية والأخلاقية حيث أنها تؤثر بشدة سواء بالإيجاب ام بالسلب حسب مدى اهتمام صانعين بها حول اختيار مسارات صادقة ودقيقة تعبر حقاعن هويتهم الغالية ومبادئ دينهم الإسلامي الحنيف والذي يعد أساس وجود وفخر لكل عربي مسلم الأصيل محافظ علي ارض آبائه وأجداده وإرث أمجاد اسلاف القدم وسعادتهم ونجاح جيل المستقبل المنشود المبشر بنوره القادم لامحالة مهما كثروا الكارهين والمتربصين له ولأهل الخير منه كل مكان وزمان بإذن رب العالمين سبحانه وتعالى جل وعلا.\\n\\n**د. فادي حسن:** كلام رائع بالفعل، رانيا! أنت توضحين لنا هنا أهمية دور العلماء المسلمين والفئات المؤثرة الأخرى للمشاركة بقوة في توجيه تكنولوجيا القرن الحالي للتوافق مع ثوابت الدين والتمسك بذواتنا الفريدة كمجتمع عربي. لذلك، دعونا نواصل البحث في حلول عملية ونناقش أكثر الطرق الناجحة للحفاظ على شخصيتنا خلال الثورة الرقمية العدوانية حالياً والتي تسابق الزمن بكل سرعتها البركانية بلا حدود تخيف صاحب القرار الواهن عقله أمام رعب منافسات عالم اليوم العالمي السريع المتغير يوماً تلته يوم مما حدا بالأفراد باتخاذ إجراءات وقائية لحماية خصوصيته وجاهلياته القديمة مهدده باحتواء مستجدات ثقافائيين عصريه تغزو سوق الاعمال حديثاً توسيعاً واسعا لأجل الوصول لعقول مشتركي شبكات التواصل الاجتماعي الحديثة التي لم تعد مقتصره علی دولة بعينه خاصوا بعد دخولي مرحلة الاتصالاتالسحابيةعام ٢٠١٦م والمعروف ايضا باسم الانترنت الأشياءIOT ). **مهندسة رانيا الصغير**: أجل، إن الشفافية والمسؤولية هما مفتاح نجاح رحلتنا نحو عصر رقمي مزدهر ولا يفقد جوهره الإنساني الأصل. وخلال العقود المقبلة، سنشاهد أعمال عظيمة من ابتكار الشباب العرب الذين سيدفعون دفعا نحو تطوير أدوات تعتمدعلى فهم عميق لقيم بلدتهم مشيرا لرسم وجه امتداد تاريخ نسبو إنسانية الإنسان البدائي الأول لفروع الوطن الأم بمكوناتهاdiverseمتعددةالثقافاتالكبيرة والصغيرةالخضوع لإرادة الله وحده فيها جميعا صنع البشر المتنوعوالتباینيون بالعيش سويا تحت مظلة واحدة اسمها الأرض فليس المطلوب الآن إلا اتباع نهجب التعاونوالعمل الجماعي بين كافةالدوائروالشرائحي سماحالجميعبوتيرة أسرع بكثير طمعابناءمستقبل يكون سوسيوا قابل للتكيف مع التغيرات المناخية المثارة حول العالم وفي نفس الوقت قادر להגן自本身فردوم ويمنع الاستلاب الثقافي المدمر للأوطان والشعوب الفتية الجديدة المنتظرروج ظهورهاضمنهذاالنطاقالإقليميمرببالخصوص . \\n\\n(ملاحظه : تم شرح جزء من رد المهندسهرانياالصغير بلغتين مختلطتان لتحسين سهولةالفهمه والاستيعاب وذلك نتيجةلوجود بعض المصطلحات العلميه المركبهبالإضافهإلى اغلبية النص بالحروف العربية)\\n\\n**د. فادي حسن:** الآن، دعونا نتعمق أكثر في الحلول العملية لتفادي التغلب المحتمل لثقافتنا وخصوصاً لغتنا الجميلة بالمجال الرقمي العالمي. إذاً، رانيا، بما أننا نرى العديد من المبادرات للتكنولوجيا الرقمية ولكن بحروف أو أيديولوجيات ليست عربية بشكل تام... ما هي الخطوات التي توصين بها كمهندسة حريصة على الحفاظ على الهوية?\\n\\n**مهندسة رانيا الصغير:** شكراً دكتور فادي على هذه النقطة الهامة. من منظور عملي, يبدو لي أنه يجب تبني النهج الثاني الذي ذكرتموه سابقاً - وهو التحويل الإبداعي للتكنولوجيا لنكون نحن صانعي محتواها وليس مجرد مستخدميه. بدءاً من الترجمة الآلية المتحسنة ضمن السياق الدقيق للعالم العربي, فنحن قادرون على إعادة تعريف المصطلحات التقنية بطريقة تعكس قيم وثقافة المنطقة. لكن الأمر لا ينتهي عند هذا الحد; التعليم والمشاركة الفعالة مهمتان أيضاً! الأجيال القادمة تحتاج إلى معرفة كيفية استخدام الأدوات ليس فقط للاستخدام بل لإعادة ابتكارها لملائمة هويتنا.\\n\\n**د. فادي حسن:** كلام رائع يا رانيا. وأوافق تماماً بشأن أهمية المعرفة والإبداع المحلي. لكن هل يمكنكم إنارة الطريق بعنوان خاص يتعلق بالقضية اللغوية تحدياً؟ نظرًا لأن \"أدب الكلمة\" كما ذكر في الشعر الجاهلي كان دائماً جزءا أساسيا من الثقافة الإسلامية والعربية، فإن ضمان وجود الاشتراطات اللغوية المناسبة عبر الإنترنت تبدو خطوة حاسمة.\\n\\n**مهندسة رانيا الصغير:** بالتأكيد، دكتور! بالنسبة للأدب والألفاظ، فقد يكون إدخال آليات تنصح باستخدام الكلمات ذات الترسبات الثقافية الغنية حلّ ممتاز خلال تطوير البرمجيات الجديدة. تخيل لو أن النظام يشجع على اختيار التعبيرات العربية الأصلية بفضل تقنيات الذكاء الاصطناعى التي نفهم جيداً أهميتها. بهذه الطريقة, سنتجنب تأثير الموجات التسويقية للغزاة اللغويين وغيرها من التأثيرات التي قد تمحو السماتها الفريدة لديننا ولغتينا الغنيَّتين.\\n\\n**د. فادي حسن:** هذا تفكير عميق حقاً. إذا طبقنا ذلك في الواقع اليومى للتكنولوجيا, سنضيف طبقة جديدة من الاحتفاء بميراثنا التاريخى بينما ندخل فيه نفس الوقت روح العصر الحديث. شكرا جزيلا لكِ رانيا على الأفكار الرائعة والنظر البعيد حول هذا الموضوع الحساس جداً!\\n\\n=== ختام البودكاست ===\\n**د. فادي حسن:** لقد كانت تلك محادثة مثيرة بالفعل، مهندسة رانيا. لم نتناول فقط تحديات عصر الذكاء الاصطناعي ولكن أيضًا استراتيجيات فعالة للحفاظ على عاداتنا وثقافتنا. أتفق تمامًا مع رأيك بأنه لا يجب أن يكون هناك صدام بين التقنية والتراث العربي الثمين.\\n\\n**مهندسة رانيا الصغير:** نعم بكل تأكيد يا دكتور فادي. أود التأكيد مرة أخرى على أهمية الوعي والإبداع عند استخدامنا لتلك الأدوات الحديثة. يمكن للذكاء الاصطناعي مساعدتنا بشكل كبير في تعزيز لغتنا وتاريخنا بدلاً من تنافسهما إذا تم فهم واستخدام هذه التقنيات بذكاء.\\n\\n**د. فادي حسن:** وأنت على حق تمامًا. دعونا نختم بإعادة النظر إلى النقاط الرئيسية التي ناقشناها اليوم: أولاً، إدراك تأثير التكنولوجيا على هويّتنا والثاني، ضرورة التفكير الإيجابي والاستفادة منها بطريقة تضمن بقاء جذورنا راسخة. وهذا ما عززته دائمًا رسالتنا هنا - أنه رغم كل التحولات السريعة، يبقى إرثنا القومي جزءًا أساسيًا مما نحن عليه كعرب.\\n\\n**مهندسة رانيا الصغير:** صحيح جدًا، ودعوني أشجع المستمعين أيضًا على المشاركة بنشاط في هذه العملية. ليس فقط قبول التغيرات وإنما توجيهها لصالحنا. لنجعل تطور تقنيات مثل الذكاء الاصطناعي يعمل لخدمة غرضنا الأسمى وهو الحفاظ على الهوية العربية الجميلة والمعقدة.\\n\\n**د. فادي حسن:** بالتأكيد، وبهذا، ننهي حلقتنا لهذا اليوم. شكراً مجددًا لك رانيا على رؤيتك الاستشرافية وعلى الوقت الذي خصصتيه لنا. ومرة أخرى لكل مستمعينا الكرام، شكراً لكم لاستماعكم معنا ولأنكم جعلتم من نقاشاتنا أكثر ثراءً. حتى الحلقة التالية، كنوا طيبون ومعطاءون كما هي روحنا العربية.\\n\\n**مهندسة رانيا الصغير:** بالتوفيق لدينا دومًا، وخالص تقديري لحضور الجميع ومشاركتهم. وسأكون سعيداً بمقابلتكم مرة أخرى لإستكشاف موضوع آخر مهم متعلق بهويتنا العربية ضمن عصر رقمي سريع الخطى. سلام عليكم!', 'script_length': 12437, 'estimated_duration': '10-15 minutes', 'quality_score': 100, 'generation_method': 'enhanced-micro-chunks', 'chunks_generated': 6, 'personas_used': {'host': 'د. فادي حسن', 'guest': 'مهندسة رانيا الصغير'}, 'cultural_elements_integrated': 10, 'enhancement_level': 'improved'}\n"
     ]
    }
   ],
   "source": [
    "print(script_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47ec2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Basic cleaning\\ncleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\\ncleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\\n\\n# Testing with detailed output\\ntest_result = test_micro_chunking_cleaner(deployment, corrupted_script_result)\\n\\n# Detailed pre-processing analysis\\nanalysis = detailed_micro_chunk_analysis(deployment, corrupted_script_result)\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class MicroChunkingAIScriptCleaner:\n",
    "    def __init__(self, deployment, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.deployment = deployment\n",
    "\n",
    "    def clean_script_with_ai(self, script_result):\n",
    "        \"\"\"\n",
    "        Micro-chunking approach: Clean script using surgical AI corrections\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"🔬 MICRO-CHUNKING AI SCRIPT CLEANER\".center(80, \"=\"))\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # Extract complete script text\n",
    "            if isinstance(script_result, dict):\n",
    "                complete_script = script_result.get('complete_script', '')\n",
    "            else:\n",
    "                complete_script = str(script_result)\n",
    "            \n",
    "            if not complete_script or len(complete_script.strip()) < 50:\n",
    "                print(\"❌ Script too short or empty, using fallback\")\n",
    "                return self._generate_complete_fallback()\n",
    "            \n",
    "            original_length = len(complete_script)\n",
    "            print(f\"📏 Original script length: {original_length:,} characters\")\n",
    "            \n",
    "            # Step 1: Analyze script corruption\n",
    "            print(\"🔍 CORRUPTION ANALYSIS\".center(60, \"-\"))\n",
    "            corruption_analysis = self.analyze_corruption_patterns(complete_script)\n",
    "            self._print_corruption_summary(corruption_analysis)\n",
    "            \n",
    "            # Step 2: Create micro-chunks (small, focused chunks)\n",
    "            print(\"\\n📝 MICRO-CHUNKING STRATEGY\".center(60, \"-\"))\n",
    "            micro_chunks = self.create_micro_chunks(complete_script)\n",
    "            print(f\"    Created {len(micro_chunks)} micro-chunks (avg: {original_length//len(micro_chunks)} chars each)\")\n",
    "            \n",
    "            # Step 3: Process each micro-chunk with surgical precision\n",
    "            print(\"\\n🔬 SURGICAL CLEANING PROCESS\".center(60, \"-\"))\n",
    "            cleaned_chunks = []\n",
    "            \n",
    "            for i, chunk_data in enumerate(micro_chunks):\n",
    "                print(f\"    Processing micro-chunk {i+1}/{len(micro_chunks)}... \", end=\"\")\n",
    "                \n",
    "                # Quick corruption assessment\n",
    "                corruption_level = self.assess_chunk_corruption(chunk_data['content'])\n",
    "                \n",
    "                if corruption_level == 'clean':\n",
    "                    # Keep as-is\n",
    "                    cleaned_chunks.append(chunk_data['content'])\n",
    "                    print(\"✅ CLEAN (kept as-is)\")\n",
    "                elif corruption_level == 'minor':\n",
    "                    # Light cleaning with regex\n",
    "                    cleaned_chunk = self.light_clean_chunk(chunk_data['content'])\n",
    "                    cleaned_chunks.append(cleaned_chunk)\n",
    "                    print(\"🟡 LIGHT CLEAN\")\n",
    "                else:\n",
    "                    # AI-powered surgical correction\n",
    "                    cleaned_chunk = self.surgical_ai_correction(\n",
    "                        chunk_data['content'], \n",
    "                        chunk_data['context'],\n",
    "                        corruption_analysis\n",
    "                    )\n",
    "                    \n",
    "                    # Validate result\n",
    "                    if self.validate_micro_chunk(cleaned_chunk, chunk_data['content']):\n",
    "                        cleaned_chunks.append(cleaned_chunk)\n",
    "                        print(\"🔧 AI CORRECTED\")\n",
    "                    else:\n",
    "                        # Fallback to light cleaning\n",
    "                        fallback_chunk = self.light_clean_chunk(chunk_data['content'])\n",
    "                        cleaned_chunks.append(fallback_chunk)\n",
    "                        print(\"🔄 FALLBACK CLEAN\")\n",
    "            \n",
    "            # Step 4: Reassemble with structure preservation\n",
    "            print(\"\\n🔧 REASSEMBLY WITH STRUCTURE CHECK\".center(60, \"-\"))\n",
    "            final_script = self.intelligent_reassembly(cleaned_chunks, micro_chunks, complete_script)\n",
    "            \n",
    "            # Step 5: Final quality and length check\n",
    "            final_length = len(final_script)\n",
    "            length_ratio = final_length / original_length if original_length > 0 else 0\n",
    "            \n",
    "            print(f\"    Original structure preserved: {'✅' if self.verify_structure_preservation(complete_script, final_script) else '⚠️'}\")\n",
    "            print(f\"    Length preservation: {length_ratio:.1%}\")\n",
    "            \n",
    "            # Step 6: Light expansion if needed (without AI)\n",
    "            if length_ratio < 0.90:  # Less than 90% retained\n",
    "                print(\"📈 APPLYING LENGTH RECOVERY (NON-AI)\".center(60, \"-\"))\n",
    "                final_script = self.non_ai_length_recovery(final_script, original_length)\n",
    "                final_length = len(final_script)\n",
    "                length_ratio = final_length / original_length\n",
    "            \n",
    "            # Final validation\n",
    "            final_valid = self.validate_final_script(final_script)\n",
    "            \n",
    "            print(\"\\n\" + \"🎉 MICRO-CHUNKING SUMMARY\".center(60, \"-\"))\n",
    "            print(f\"    Status: {'SUCCESS' if final_valid else 'PARTIAL SUCCESS'}\")\n",
    "            print(f\"    Original Length: {original_length:,} characters\")\n",
    "            print(f\"    Final Length: {final_length:,} characters\")\n",
    "            print(f\"    Length Preserved: {length_ratio:.1%}\")\n",
    "            print(f\"    Micro-chunks Processed: {len(micro_chunks)}\")\n",
    "            print(f\"    Arabic Quality: {'✅ VALID' if final_valid else '⚠️ NEEDS REVIEW'}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            return {\n",
    "                'complete_script': final_script,\n",
    "                'cleaning_method': 'micro_chunking_surgical',\n",
    "                'micro_chunks_processed': len(micro_chunks),\n",
    "                'cleaning_status': 'success' if final_valid else 'partial',\n",
    "                'script_length': final_length,\n",
    "                'original_length': original_length,\n",
    "                'length_ratio': length_ratio,\n",
    "                'corruption_analysis': corruption_analysis,\n",
    "                'estimated_duration': self._estimate_duration(final_script)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ CRITICAL ERROR: {e}\")\n",
    "            print(\"🔄 Using complete fallback script...\")\n",
    "            return self._generate_complete_fallback()\n",
    "\n",
    "    def analyze_corruption_patterns(self, script_text):\n",
    "        \"\"\"\n",
    "        Analyze what specific types of corruption exist in the script\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'total_chars': len(script_text),\n",
    "            'foreign_patterns': {},\n",
    "            'encoding_issues': 0,\n",
    "            'concatenated_words': 0,\n",
    "            'structural_issues': 0,\n",
    "            'overall_corruption_level': 'clean'\n",
    "        }\n",
    "        \n",
    "        # Foreign language patterns\n",
    "        foreign_patterns = {\n",
    "            'english_words': (r'\\b[A-Za-z]{3,}\\b', 'English words (3+ letters)'),\n",
    "            'chinese_chars': (r'[\\u4e00-\\u9fff]', 'Chinese characters'),\n",
    "            'hebrew_chars': (r'[\\u0590-\\u05ff]', 'Hebrew characters'),\n",
    "            'japanese_chars': (r'[\\u3040-\\u309f\\u30a0-\\u30ff]', 'Japanese characters'),\n",
    "            'problematic_punct': (r'[、！]', 'Problematic punctuation')\n",
    "        }\n",
    "        \n",
    "        total_foreign_chars = 0\n",
    "        for pattern_name, (pattern, description) in foreign_patterns.items():\n",
    "            matches = re.findall(pattern, script_text)\n",
    "            if matches:\n",
    "                char_count = sum(len(match) for match in matches)\n",
    "                analysis['foreign_patterns'][pattern_name] = {\n",
    "                    'count': len(matches),\n",
    "                    'chars': char_count,\n",
    "                    'description': description,\n",
    "                    'examples': matches[:3]  # First 3 examples\n",
    "                }\n",
    "                total_foreign_chars += char_count\n",
    "        \n",
    "        # Concatenated words (Arabic words without spaces)\n",
    "        concatenated_matches = re.findall(r'[\\u0600-\\u06FF]{40,}', script_text)  # 40+ Arabic chars without spaces\n",
    "        analysis['concatenated_words'] = len(concatenated_matches)\n",
    "        \n",
    "        # Encoding issues (mixed scripts in single words)\n",
    "        encoding_issues = re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]+[\\u0600-\\u06FF]+|[\\u0600-\\u06FF]+[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]+', script_text)\n",
    "        analysis['encoding_issues'] = len(encoding_issues)\n",
    "        \n",
    "        # Structural issues\n",
    "        if '***' in script_text or '**' in script_text:\n",
    "            analysis['structural_issues'] += script_text.count('***') + script_text.count('**')\n",
    "        \n",
    "        # Overall corruption level\n",
    "        foreign_ratio = total_foreign_chars / len(script_text) if script_text else 0\n",
    "        if foreign_ratio > 0.15 or analysis['concatenated_words'] > 5:\n",
    "            analysis['overall_corruption_level'] = 'heavy'\n",
    "        elif foreign_ratio > 0.05 or analysis['concatenated_words'] > 2:\n",
    "            analysis['overall_corruption_level'] = 'moderate'\n",
    "        elif foreign_ratio > 0.01 or analysis['encoding_issues'] > 0:\n",
    "            analysis['overall_corruption_level'] = 'light'\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _print_corruption_summary(self, analysis):\n",
    "        \"\"\"Print corruption analysis summary\"\"\"\n",
    "        print(f\"    Overall corruption level: {analysis['overall_corruption_level'].upper()}\")\n",
    "        print(f\"    Foreign patterns found: {len(analysis['foreign_patterns'])}\")\n",
    "        \n",
    "        for pattern_name, details in analysis['foreign_patterns'].items():\n",
    "            print(f\"      - {details['description']}: {details['count']} instances\")\n",
    "            if details['examples']:\n",
    "                examples_str = ', '.join(str(ex) for ex in details['examples'][:2])\n",
    "                print(f\"        Examples: {examples_str}\")\n",
    "        \n",
    "        if analysis['concatenated_words'] > 0:\n",
    "            print(f\"    Concatenated word sequences: {analysis['concatenated_words']}\")\n",
    "        if analysis['encoding_issues'] > 0:\n",
    "            print(f\"    Mixed encoding issues: {analysis['encoding_issues']}\")\n",
    "\n",
    "    def create_micro_chunks(self, script_text):\n",
    "        \"\"\"\n",
    "        Create small, focused micro-chunks (200-400 characters each)\n",
    "        \"\"\"\n",
    "        micro_chunks = []\n",
    "        \n",
    "        # Split by sections first\n",
    "        sections = self._identify_script_sections(script_text)\n",
    "        \n",
    "        for section_name, section_content in sections.items():\n",
    "            if not section_content.strip():\n",
    "                continue\n",
    "            \n",
    "            # Split section into dialogue exchanges\n",
    "            dialogue_chunks = self._split_into_dialogue_exchanges(section_content, section_name)\n",
    "            micro_chunks.extend(dialogue_chunks)\n",
    "        \n",
    "        return micro_chunks\n",
    "\n",
    "    def _identify_script_sections(self, script_text):\n",
    "        \"\"\"Identify main sections of the script\"\"\"\n",
    "        sections = {}\n",
    "        \n",
    "        if \"=== مقدمة البودكاست ===\" in script_text:\n",
    "            # Structured script\n",
    "            parts = script_text.split(\"=== مقدمة البودكاست ===\")\n",
    "            if len(parts) > 1:\n",
    "                after_intro = parts[1]\n",
    "                if \"=== النقاش الرئيسي ===\" in after_intro:\n",
    "                    intro_parts = after_intro.split(\"=== النقاش الرئيسي ===\")\n",
    "                    sections['intro'] = intro_parts[0].strip()\n",
    "                    if len(intro_parts) > 1:\n",
    "                        after_main = intro_parts[1]\n",
    "                        if \"=== ختام البودكاست ===\" in after_main:\n",
    "                            main_parts = after_main.split(\"=== ختام البودكاست ===\")\n",
    "                            sections['main_discussion'] = main_parts[0].strip()\n",
    "                            if len(main_parts) > 1:\n",
    "                                sections['closing'] = main_parts[1].strip()\n",
    "                        else:\n",
    "                            sections['main_discussion'] = after_main.strip()\n",
    "                else:\n",
    "                    sections['intro'] = after_intro.strip()\n",
    "        else:\n",
    "            # Unstructured script\n",
    "            sections['full_script'] = script_text\n",
    "        \n",
    "        return sections\n",
    "\n",
    "    def _split_into_dialogue_exchanges(self, section_content, section_name):\n",
    "        \"\"\"Split section into small dialogue exchanges\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split by speaker turns\n",
    "        lines = section_content.split('\\n')\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        target_chunk_size = 300  # Target 300 characters per micro-chunk\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a speaker line\n",
    "            is_speaker_line = ':' in line and not line.startswith('===')\n",
    "            \n",
    "            # If adding this line would exceed target size, finalize current chunk\n",
    "            if current_length + len(line) > target_chunk_size and current_chunk and is_speaker_line:\n",
    "                chunk_content = '\\n'.join(current_chunk).strip()\n",
    "                if chunk_content:\n",
    "                    chunks.append({\n",
    "                        'content': chunk_content,\n",
    "                        'context': f'{section_name}_dialogue_exchange',\n",
    "                        'type': 'dialogue_exchange',\n",
    "                        'length': len(chunk_content)\n",
    "                    })\n",
    "                current_chunk = [line]\n",
    "                current_length = len(line)\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "                current_length += len(line) + 1  # +1 for newline\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_content = '\\n'.join(current_chunk).strip()\n",
    "            if chunk_content:\n",
    "                chunks.append({\n",
    "                    'content': chunk_content,\n",
    "                    'context': f'{section_name}_dialogue_exchange',\n",
    "                    'type': 'dialogue_exchange',\n",
    "                    'length': len(chunk_content)\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def assess_chunk_corruption(self, chunk_content):\n",
    "        \"\"\"Quick assessment of chunk corruption level\"\"\"\n",
    "        if not chunk_content or len(chunk_content.strip()) < 10:\n",
    "            return 'heavy'\n",
    "        \n",
    "        # Check for foreign characters\n",
    "        foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff\\u3040-\\u309f\\u30a0-\\u30ff、！]', chunk_content))\n",
    "        total_chars = len(chunk_content)\n",
    "        \n",
    "        # Check for concatenated words\n",
    "        concatenated = len(re.findall(r'[\\u0600-\\u06FF]{30,}', chunk_content))\n",
    "        \n",
    "        # Check for encoding issues\n",
    "        encoding_issues = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff]+[\\u0600-\\u06FF]+|[\\u0600-\\u06FF]+[A-Za-z\\u4e00-\\u9fff]+', chunk_content))\n",
    "        \n",
    "        if foreign_chars == 0 and concatenated == 0 and encoding_issues == 0:\n",
    "            return 'clean'\n",
    "        elif foreign_chars < 3 and concatenated == 0 and encoding_issues == 0:\n",
    "            return 'minor'\n",
    "        else:\n",
    "            return 'heavy'\n",
    "\n",
    "    def light_clean_chunk(self, chunk_content):\n",
    "        \"\"\"Light cleaning using regex only\"\"\"\n",
    "        cleaned = chunk_content\n",
    "        \n",
    "        # Remove specific problematic punctuation\n",
    "        cleaned = re.sub(r'[、！]', '', cleaned)\n",
    "        \n",
    "        # Remove short English words (1-2 letters)\n",
    "        cleaned = re.sub(r'\\b[A-Za-z]{1,2}\\b', '', cleaned)\n",
    "        \n",
    "        # Remove Chinese and Hebrew characters\n",
    "        cleaned = re.sub(r'[\\u4e00-\\u9fff\\u0590-\\u05ff\\u3040-\\u309f\\u30a0-\\u30ff]', '', cleaned)\n",
    "        \n",
    "        # Fix spacing issues\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        cleaned = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned)\n",
    "        \n",
    "        # Clean up markdown formatting\n",
    "        cleaned = re.sub(r'\\*{2,}', '', cleaned)\n",
    "        \n",
    "        return cleaned.strip()\n",
    "\n",
    "    def surgical_ai_correction(self, chunk_content, context, corruption_analysis):\n",
    "        \"\"\"\n",
    "        Surgical AI correction focused on specific problems\n",
    "        \"\"\"\n",
    "        # Identify specific issues in this chunk\n",
    "        chunk_issues = []\n",
    "        \n",
    "        if re.search(r'[A-Za-z]{3,}', chunk_content):\n",
    "            chunk_issues.append(\"English words\")\n",
    "        if re.search(r'[\\u4e00-\\u9fff]', chunk_content):\n",
    "            chunk_issues.append(\"Chinese characters\")\n",
    "        if re.search(r'[\\u0590-\\u05ff]', chunk_content):\n",
    "            chunk_issues.append(\"Hebrew characters\")\n",
    "        if re.search(r'[\\u0600-\\u06FF]{30,}', chunk_content):\n",
    "            chunk_issues.append(\"concatenated words\")\n",
    "        \n",
    "        issues_description = \", \".join(chunk_issues) if chunk_issues else \"minor formatting issues\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an Arabic text correction specialist. Fix ONLY the specific issues in this small text chunk.\n",
    "\n",
    "CHUNK CONTEXT: {context}\n",
    "ISSUES TO FIX: {issues_description}\n",
    "CHUNK LENGTH: {len(chunk_content)} characters\n",
    "\n",
    "ORIGINAL CHUNK:\n",
    "{chunk_content}\n",
    "\n",
    "CORRECTION INSTRUCTIONS:\n",
    "1. Replace English words with appropriate Arabic equivalents in context\n",
    "2. Remove Chinese/Hebrew characters and replace with contextually appropriate Arabic\n",
    "3. Fix concatenated Arabic words by adding proper spaces\n",
    "4. Keep the EXACT same meaning and dialogue structure\n",
    "5. Maintain or slightly increase the length\n",
    "6. Do NOT change speaker names or dialogue flow\n",
    "\n",
    "CRITICAL: Return ONLY the corrected Arabic text. No explanations or comments.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.deployment.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an Arabic text correction specialist. Make minimal, precise corrections while preserving meaning and length.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,  # Low temperature for consistent corrections\n",
    "                max_tokens=1000   # Limit tokens to prevent over-expansion\n",
    "            )\n",
    "            \n",
    "            corrected = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Basic cleanup\n",
    "            corrected = self.light_clean_chunk(corrected)\n",
    "            \n",
    "            return corrected\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" (AI failed: {e}) \", end=\"\")\n",
    "            return self.light_clean_chunk(chunk_content)\n",
    "\n",
    "    def validate_micro_chunk(self, cleaned_chunk, original_chunk):\n",
    "        \"\"\"\n",
    "        Validate that micro-chunk correction was successful\n",
    "        \"\"\"\n",
    "        if not cleaned_chunk or len(cleaned_chunk.strip()) < 10:\n",
    "            return False\n",
    "        \n",
    "        # Check that length wasn't reduced too much\n",
    "        length_ratio = len(cleaned_chunk) / len(original_chunk) if original_chunk else 0\n",
    "        if length_ratio < 0.7:  # Lost more than 30% of content\n",
    "            return False\n",
    "        \n",
    "        # Check for remaining major foreign content\n",
    "        major_foreign = re.findall(r'[\\u4e00-\\u9fff\\u0590-\\u05ff]{2,}|[A-Za-z]{4,}', cleaned_chunk)\n",
    "        if len(major_foreign) > 1:  # Allow 1 instance (might be technical term)\n",
    "            return False\n",
    "        \n",
    "        # Check for basic dialogue structure if original had it\n",
    "        if ':' in original_chunk and ':' not in cleaned_chunk:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def intelligent_reassembly(self, cleaned_chunks, original_micro_chunks, original_script):\n",
    "        \"\"\"\n",
    "        Intelligently reassemble micro-chunks back into complete script\n",
    "        \"\"\"\n",
    "        # Group chunks back into sections\n",
    "        sections = {'intro': [], 'main_discussion': [], 'closing': [], 'other': []}\n",
    "        \n",
    "        for i, chunk in enumerate(cleaned_chunks):\n",
    "            if not chunk.strip():\n",
    "                continue\n",
    "            \n",
    "            context = original_micro_chunks[i]['context'] if i < len(original_micro_chunks) else 'other'\n",
    "            \n",
    "            if 'intro' in context:\n",
    "                sections['intro'].append(chunk)\n",
    "            elif 'main_discussion' in context:\n",
    "                sections['main_discussion'].append(chunk)\n",
    "            elif 'closing' in context:\n",
    "                sections['closing'].append(chunk)\n",
    "            else:\n",
    "                sections['other'].append(chunk)\n",
    "        \n",
    "        # Reassemble with proper structure\n",
    "        script_parts = []\n",
    "        \n",
    "        # Add intro section\n",
    "        if sections['intro']:\n",
    "            script_parts.append(\"=== مقدمة البودكاست ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['intro'] if chunk.strip()])\n",
    "        \n",
    "        # Add main discussion\n",
    "        if sections['main_discussion']:\n",
    "            script_parts.append(\"=== النقاش الرئيسي ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['main_discussion'] if chunk.strip()])\n",
    "        \n",
    "        # Add closing\n",
    "        if sections['closing']:\n",
    "            script_parts.append(\"=== ختام البودكاست ===\")\n",
    "            script_parts.extend([chunk for chunk in sections['closing'] if chunk.strip()])\n",
    "        \n",
    "        # Add other content\n",
    "        if sections['other']:\n",
    "            script_parts.extend([chunk for chunk in sections['other'] if chunk.strip()])\n",
    "        \n",
    "        # Join with proper spacing\n",
    "        complete_script = '\\n\\n'.join([part for part in script_parts if part.strip()])\n",
    "        \n",
    "        # Clean up spacing\n",
    "        complete_script = re.sub(r'\\n{3,}', '\\n\\n', complete_script)\n",
    "        \n",
    "        return complete_script.strip()\n",
    "\n",
    "    def verify_structure_preservation(self, original_script, final_script):\n",
    "        \"\"\"\n",
    "        Verify that the original structure was preserved\n",
    "        \"\"\"\n",
    "        # Check section headers\n",
    "        original_sections = len(re.findall(r'===.*===', original_script))\n",
    "        final_sections = len(re.findall(r'===.*===', final_script))\n",
    "        \n",
    "        if original_sections > 0 and final_sections < original_sections:\n",
    "            return False\n",
    "        \n",
    "        # Check speaker preservation\n",
    "        original_speakers = set(re.findall(r'^([^:]+):', original_script, re.MULTILINE))\n",
    "        final_speakers = set(re.findall(r'^([^:]+):', final_script, re.MULTILINE))\n",
    "        \n",
    "        # Should preserve most speakers\n",
    "        if len(final_speakers) < len(original_speakers) * 0.8:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def non_ai_length_recovery(self, script, target_length):\n",
    "        \"\"\"\n",
    "        Recover length using non-AI methods (pattern-based expansion)\n",
    "        \"\"\"\n",
    "        current_length = len(script)\n",
    "        if current_length >= target_length * 0.9:\n",
    "            return script\n",
    "        \n",
    "        # Natural Arabic conversation extenders\n",
    "        extenders = [\n",
    "            (\"د. \", \"د. المحترم \"),\n",
    "            (\"أستاذ \", \"أستاذ فاضل \"),\n",
    "            (\"نعم\", \"نعم بالتأكيد\"),\n",
    "            (\"بالطبع\", \"بالطبع وبلا شك\"),\n",
    "            (\"هذا\", \"هذا الأمر\"),\n",
    "            (\"ممتاز\", \"ممتاز جداً\"),\n",
    "            (\"صحيح\", \"صحيح تماماً\"),\n",
    "            (\"أعتقد\", \"أعتقد بقوة\"),\n",
    "            (\"يمكن\", \"يمكن بالفعل\"),\n",
    "            (\"المهم\", \"والأمر المهم\"),\n",
    "        ]\n",
    "        \n",
    "        expanded = script\n",
    "        chars_added = 0\n",
    "        target_addition = min(target_length - current_length, current_length // 5)  # Max 20% expansion\n",
    "        \n",
    "        for original, replacement in extenders:\n",
    "            if chars_added >= target_addition:\n",
    "                break\n",
    "            \n",
    "            if original in expanded:\n",
    "                # Replace some instances (not all to avoid repetition)\n",
    "                count = expanded.count(original)\n",
    "                replace_count = min(count // 3, 2)  # Replace 1/3, max 2 instances\n",
    "                \n",
    "                for _ in range(replace_count):\n",
    "                    expanded = expanded.replace(original, replacement, 1)\n",
    "                    chars_added += len(replacement) - len(original)\n",
    "                    \n",
    "                    if chars_added >= target_addition:\n",
    "                        break\n",
    "        \n",
    "        return expanded\n",
    "\n",
    "    def validate_final_script(self, script):\n",
    "        \"\"\"\n",
    "        Final validation of the complete script\n",
    "        \"\"\"\n",
    "        if not script or len(script.strip()) < 50:\n",
    "            return False\n",
    "        \n",
    "        # Check for basic dialogue structure\n",
    "        if ':' not in script:\n",
    "            return False\n",
    "        \n",
    "        # Check for excessive foreign content (relaxed)\n",
    "        foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', script))\n",
    "        total_chars = len(re.sub(r'[\\s\\n\\t:=\\-،.؟!\"()[\\]{}]', '', script))\n",
    "        \n",
    "        if total_chars > 0:\n",
    "            foreign_ratio = foreign_chars / total_chars\n",
    "            if foreign_ratio > 0.1:  # Allow up to 10% foreign (technical terms, etc.)\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _estimate_duration(self, script_text):\n",
    "        \"\"\"Estimate podcast duration\"\"\"\n",
    "        if not script_text:\n",
    "            return \"0-1 minutes\"\n",
    "        \n",
    "        word_count = len(script_text.split())\n",
    "        duration_minutes = word_count / 150\n",
    "        \n",
    "        min_duration = max(1, int(duration_minutes - 1))\n",
    "        max_duration = int(duration_minutes + 2)\n",
    "        \n",
    "        return f\"{min_duration}-{max_duration} minutes\"\n",
    "\n",
    "    def _generate_complete_fallback(self):\n",
    "        \"\"\"Generate complete fallback script\"\"\"\n",
    "        fallback_script = \"\"\"=== مقدمة البودكاست ===\n",
    "د. فاطمة الزهراء: مرحباً بكم مستمعينا الكرام في حلقة جديدة من برنامجنا. اليوم نستضيف خبيراً متميزاً لنناقش موضوعاً مهماً يهم الجميع.\n",
    "\n",
    "م. عبد الرحمن الهاشمي: أهلاً وسهلاً بكم، أشكركم على الاستضافة الكريمة. سعيد جداً بوجودي معكم اليوم.\n",
    "\n",
    "د. فاطمة الزهراء: نحن سعداء بوجودك معنا. دعنا نبدأ هذا النقاش المفيد.\n",
    "\n",
    "=== النقاش الرئيسي ===\n",
    "د. فاطمة الزهراء: بداية، ما رأيك في أهمية هذا الموضوع في وقتنا الحالي؟\n",
    "\n",
    "م. عبد الرحمن الهاشمي: موضوع في غاية الأهمية حقاً. نحن نواجه تحديات كبيرة تتطلب منا فهماً عميقاً ونظرة شاملة للأمور.\n",
    "\n",
    "د. فاطمة الزهراء: ممكن تحدثنا أكثر عن هذه التحديات؟\n",
    "\n",
    "م. عبد الرحمن الهاشمي: بالطبع. من أهم التحديات هو كيفية التوازن بين التطورات الحديثة والحفاظ على قيمنا وثوابتنا الأصيلة.\n",
    "\n",
    "د. فاطمة الزهراء: نقطة مهمة جداً. وما هي الحلول العملية التي تقترحها؟\n",
    "\n",
    "م. عبد الرحمن الهاشمي: أعتقد أن الحل يكمن في التعليم والتوعية، مع الاستفادة الذكية من التقنيات الحديثة بما يخدم مصالحنا وأهدافنا.\n",
    "\n",
    "=== ختام البودكاست ===\n",
    "د. فاطمة الزهراء: في ختام حلقتنا اليوم، أشكرك أستاذ عبد الرحمن على هذا النقاش الثري والمفيد.\n",
    "\n",
    "م. عبد الرحمن الهاشمي: شكراً لك على الاستضافة الكريمة. كان نقاش ممتع ومثمر، وأتمنى أن يستفيد منه المستمعون.\n",
    "\n",
    "د. فاطمة الزهراء: بالتأكيد. وشكراً لكم مستمعينا الكرام على متابعتكم الدائمة. نلقاكم في حلقة قادمة بإذن الله. إلى اللقاء.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'complete_script': fallback_script,\n",
    "            'cleaning_method': 'micro_chunking_fallback',\n",
    "            'micro_chunks_processed': 0,\n",
    "            'cleaning_status': 'fallback_used',\n",
    "            'script_length': len(fallback_script),\n",
    "            'estimated_duration': self._estimate_duration(fallback_script)\n",
    "        }\n",
    "\n",
    "\n",
    "# Testing Function\n",
    "def test_micro_chunking_cleaner(deployment, corrupted_script_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Test the micro-chunking AI script cleaner\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"🧪 TESTING MICRO-CHUNKING AI SCRIPT CLEANER\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaner = MicroChunkingAIScriptCleaner(deployment, model_name)\n",
    "    \n",
    "    # Show original script info\n",
    "    if isinstance(corrupted_script_result, dict):\n",
    "        original_script = corrupted_script_result.get('complete_script', '')\n",
    "    else:\n",
    "        original_script = str(corrupted_script_result)\n",
    "    \n",
    "    print(\"📊 ORIGINAL SCRIPT INFO\".center(60, \"-\"))\n",
    "    print(f\"{'Original Length:':<25} {len(original_script):,} characters\")\n",
    "    print(f\"{'Estimated Duration:':<25} {cleaner._estimate_duration(original_script)}\")\n",
    "    \n",
    "    # Quick preview of corruption\n",
    "    foreign_chars = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', original_script))\n",
    "    corruption_ratio = foreign_chars / len(original_script) if original_script else 0\n",
    "    print(f\"{'Apparent Corruption:':<25} {corruption_ratio:.1%} foreign content\")\n",
    "    \n",
    "    # Clean the script\n",
    "    cleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\n",
    "    \n",
    "    print(\"\\n📊 MICRO-CHUNKING RESULTS\".center(60, \"-\"))\n",
    "    print(f\"{'Method:':<25} {cleaned_result['cleaning_method']}\")\n",
    "    print(f\"{'Status:':<25} {cleaned_result['cleaning_status']}\")\n",
    "    print(f\"{'Micro-chunks Processed:':<25} {cleaned_result['micro_chunks_processed']}\")\n",
    "    print(f\"{'Original Length:':<25} {cleaned_result.get('original_length', 'N/A'):,} characters\")\n",
    "    print(f\"{'Final Length:':<25} {cleaned_result['script_length']:,} characters\")\n",
    "    print(f\"{'Length Preserved:':<25} {cleaned_result.get('length_ratio', 0):.1%}\")\n",
    "    print(f\"{'Duration:':<25} {cleaned_result['estimated_duration']}\")\n",
    "    \n",
    "    # Length preservation status\n",
    "    if 'length_ratio' in cleaned_result:\n",
    "        if cleaned_result['length_ratio'] >= 0.90:\n",
    "            print(f\"{'Length Status:':<25} ✅ EXCELLENT PRESERVATION\")\n",
    "        elif cleaned_result['length_ratio'] >= 0.80:\n",
    "            print(f\"{'Length Status:':<25} ✅ GOOD PRESERVATION\")\n",
    "        elif cleaned_result['length_ratio'] >= 0.70:\n",
    "            print(f\"{'Length Status:':<25} ⚠️ MODERATE LOSS\")\n",
    "        else:\n",
    "            print(f\"{'Length Status:':<25} ❌ SIGNIFICANT LOSS\")\n",
    "    \n",
    "    # Corruption analysis summary\n",
    "    if 'corruption_analysis' in cleaned_result:\n",
    "        corruption_info = cleaned_result['corruption_analysis']\n",
    "        print(f\"{'Corruption Detected:':<25} {corruption_info['overall_corruption_level'].upper()}\")\n",
    "        print(f\"{'Foreign Patterns:':<25} {len(corruption_info['foreign_patterns'])} types found\")\n",
    "    \n",
    "    # Display cleaned script with formatting\n",
    "    print(\"\\n\" + \"🎙️ MICRO-CLEANED SCRIPT OUTPUT\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaned_script = cleaned_result['complete_script']\n",
    "    if cleaned_script:\n",
    "        sections = cleaned_script.split('\\n\\n')\n",
    "        \n",
    "        for section in sections[:10]:  # Show first 10 sections to avoid overwhelming output\n",
    "            section = section.strip()\n",
    "            if not section:\n",
    "                continue\n",
    "            \n",
    "            # Section headers\n",
    "            if section.startswith('===') and section.endswith('==='):\n",
    "                print(f\"\\n{section}\")\n",
    "                print(\"─\" * len(section))\n",
    "                continue\n",
    "            \n",
    "            # Format dialogue\n",
    "            lines = section.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                if ':' in line and not line.startswith('==='):\n",
    "                    speaker, dialogue = line.split(':', 1)\n",
    "                    speaker = speaker.strip()\n",
    "                    dialogue = dialogue.strip()\n",
    "                    \n",
    "                    print(f\"\\n{speaker}:\")\n",
    "                    \n",
    "                    # Word wrap dialogue\n",
    "                    words = dialogue.split()\n",
    "                    current_line = \"\"\n",
    "                    max_length = 70\n",
    "                    \n",
    "                    for word in words:\n",
    "                        if len(current_line) + len(word) + 1 > max_length and current_line:\n",
    "                            print(f\"    {current_line}\")\n",
    "                            current_line = word\n",
    "                        else:\n",
    "                            current_line = current_line + \" \" + word if current_line else word\n",
    "                    \n",
    "                    if current_line:\n",
    "                        print(f\"    {current_line}\")\n",
    "                else:\n",
    "                    print(f\"    {line}\")\n",
    "    \n",
    "    # Final validation results\n",
    "    is_clean = cleaner.validate_final_script(cleaned_script)\n",
    "    print(f\"\\n🔍 FINAL VALIDATION: {'✅ PASSED' if is_clean else '❌ NEEDS REVIEW'}\")\n",
    "    \n",
    "    # Quality comparison\n",
    "    if original_script and cleaned_script:\n",
    "        original_foreign = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', original_script))\n",
    "        cleaned_foreign = len(re.findall(r'[A-Za-z\\u4e00-\\u9fff\\u0590-\\u05ff]', cleaned_script))\n",
    "        \n",
    "        print(f\"🧹 CLEANING EFFECTIVENESS:\")\n",
    "        print(f\"    Foreign chars removed: {original_foreign - cleaned_foreign:,}\")\n",
    "        print(f\"    Cleaning efficiency: {((original_foreign - cleaned_foreign) / original_foreign * 100) if original_foreign > 0 else 0:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return cleaned_result\n",
    "\n",
    "\n",
    "# Advanced Testing Function with Detailed Analysis\n",
    "def detailed_micro_chunk_analysis(deployment, corrupted_script_result, model_name=\"Fanar-C-1-8.7B\"):\n",
    "    \"\"\"\n",
    "    Detailed analysis of micro-chunking performance\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"🔬 DETAILED MICRO-CHUNKING ANALYSIS\".center(80, \"=\"))\n",
    "    print()\n",
    "    \n",
    "    cleaner = MicroChunkingAIScriptCleaner(deployment, model_name)\n",
    "    \n",
    "    # Extract script\n",
    "    if isinstance(corrupted_script_result, dict):\n",
    "        original_script = corrupted_script_result.get('complete_script', '')\n",
    "    else:\n",
    "        original_script = str(corrupted_script_result)\n",
    "    \n",
    "    print(\"🔍 PRE-PROCESSING ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Detailed corruption analysis\n",
    "    corruption_analysis = cleaner.analyze_corruption_patterns(original_script)\n",
    "    \n",
    "    print(f\"Script length: {corruption_analysis['total_chars']:,} characters\")\n",
    "    print(f\"Overall corruption level: {corruption_analysis['overall_corruption_level'].upper()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Foreign content breakdown:\")\n",
    "    for pattern_name, details in corruption_analysis['foreign_patterns'].items():\n",
    "        print(f\"  • {details['description']}: {details['count']} instances ({details['chars']} chars)\")\n",
    "        if details['examples']:\n",
    "            examples_preview = [str(ex)[:10] + ('...' if len(str(ex)) > 10 else '') for ex in details['examples'][:2]]\n",
    "            print(f\"    Examples: {', '.join(examples_preview)}\")\n",
    "    \n",
    "    if corruption_analysis['concatenated_words'] > 0:\n",
    "        print(f\"  • Concatenated word sequences: {corruption_analysis['concatenated_words']}\")\n",
    "    if corruption_analysis['encoding_issues'] > 0:\n",
    "        print(f\"  • Mixed encoding issues: {corruption_analysis['encoding_issues']}\")\n",
    "    \n",
    "    print(\"\\n🔬 MICRO-CHUNKING BREAKDOWN\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create micro-chunks for analysis\n",
    "    micro_chunks = cleaner.create_micro_chunks(original_script)\n",
    "    \n",
    "    print(f\"Total micro-chunks created: {len(micro_chunks)}\")\n",
    "    if micro_chunks:\n",
    "        avg_chunk_size = sum(chunk['length'] for chunk in micro_chunks) / len(micro_chunks)\n",
    "        print(f\"Average chunk size: {avg_chunk_size:.1f} characters\")\n",
    "        \n",
    "        chunk_sizes = [chunk['length'] for chunk in micro_chunks]\n",
    "        print(f\"Chunk size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "    \n",
    "    # Analyze chunk corruption levels\n",
    "    corruption_levels = {'clean': 0, 'minor': 0, 'heavy': 0}\n",
    "    for chunk in micro_chunks:\n",
    "        level = cleaner.assess_chunk_corruption(chunk['content'])\n",
    "        corruption_levels[level] += 1\n",
    "    \n",
    "    print(f\"\\nCorruption level distribution:\")\n",
    "    print(f\"  • Clean chunks: {corruption_levels['clean']} ({corruption_levels['clean']/len(micro_chunks)*100:.1f}%)\")\n",
    "    print(f\"  • Minor issues: {corruption_levels['minor']} ({corruption_levels['minor']/len(micro_chunks)*100:.1f}%)\")\n",
    "    print(f\"  • Heavy corruption: {corruption_levels['heavy']} ({corruption_levels['heavy']/len(micro_chunks)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n🎯 PROCESSING STRATEGY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Expected processing:\")\n",
    "    print(f\"  • Keep as-is: {corruption_levels['clean']} chunks\")\n",
    "    print(f\"  • Light regex cleaning: {corruption_levels['minor']} chunks\") \n",
    "    print(f\"  • AI surgical correction: {corruption_levels['heavy']} chunks\")\n",
    "    \n",
    "    estimated_ai_calls = corruption_levels['heavy']\n",
    "    print(f\"  • Estimated AI API calls: {estimated_ai_calls}\")\n",
    "    \n",
    "    if estimated_ai_calls > 0:\n",
    "        print(f\"  • Efficiency vs. large chunks: {estimated_ai_calls} calls vs ~3-4 calls (traditional)\")\n",
    "        print(f\"  • Trade-off: More calls but higher precision per call\")\n",
    "    \n",
    "    print(f\"\\n💡 PREDICTION:\")\n",
    "    clean_ratio = corruption_levels['clean'] / len(micro_chunks)\n",
    "    if clean_ratio > 0.7:\n",
    "        print(\"  Expected outcome: EXCELLENT length preservation (70%+ content kept as-is)\")\n",
    "    elif clean_ratio > 0.5:\n",
    "        print(\"  Expected outcome: GOOD length preservation (50%+ content kept as-is)\")\n",
    "    else:\n",
    "        print(\"  Expected outcome: MODERATE length preservation (heavy corruption detected)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'total_chunks': len(micro_chunks),\n",
    "        'corruption_analysis': corruption_analysis,\n",
    "        'corruption_distribution': corruption_levels,\n",
    "        'estimated_ai_calls': estimated_ai_calls,\n",
    "        'clean_content_ratio': clean_ratio\n",
    "    }\n",
    "\n",
    "\n",
    "# Usage Examples:\n",
    "\"\"\"\n",
    "# Basic cleaning\n",
    "cleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\n",
    "cleaned_result = cleaner.clean_script_with_ai(corrupted_script_result)\n",
    "\n",
    "# Testing with detailed output\n",
    "test_result = test_micro_chunking_cleaner(deployment, corrupted_script_result)\n",
    "\n",
    "# Detailed pre-processing analysis\n",
    "analysis = detailed_micro_chunk_analysis(deployment, corrupted_script_result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94117e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================🔬 MICRO-CHUNKING AI SCRIPT CLEANER=======================\n",
      "\n",
      "📏 Original script length: 12,437 characters\n",
      "-------------------🔍 CORRUPTION ANALYSIS--------------------\n",
      "    Overall corruption level: HEAVY\n",
      "    Foreign patterns found: 3\n",
      "      - English words (3+ letters): 9 instances\n",
      "        Examples: how, step\n",
      "      - Chinese characters: 4 instances\n",
      "        Examples: 提, 自\n",
      "      - Hebrew characters: 6 instances\n",
      "        Examples: ע, ל\n",
      "    Concatenated word sequences: 8\n",
      "    Mixed encoding issues: 6\n",
      "-----------------\n",
      "📝 MICRO-CHUNKING STRATEGY-----------------\n",
      "    Created 26 micro-chunks (avg: 478 chars each)\n",
      "----------------\n",
      "🔬 SURGICAL CLEANING PROCESS----------------\n",
      "    Processing micro-chunk 1/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 2/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 3/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 4/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 5/26... 🔧 AI CORRECTED\n",
      "    Processing micro-chunk 6/26... 🔧 AI CORRECTED\n",
      "    Processing micro-chunk 7/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 8/26... 🔧 AI CORRECTED\n",
      "    Processing micro-chunk 9/26... 🔄 FALLBACK CLEAN\n",
      "    Processing micro-chunk 10/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 11/26... 🔧 AI CORRECTED\n",
      "    Processing micro-chunk 12/26... 🔧 AI CORRECTED\n",
      "    Processing micro-chunk 13/26... 🟡 LIGHT CLEAN\n",
      "    Processing micro-chunk 14/26... 🔧 AI CORRECTED\n",
      "    Processing micro-chunk 15/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 16/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 17/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 18/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 19/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 20/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 21/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 22/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 23/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 24/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 25/26... ✅ CLEAN (kept as-is)\n",
      "    Processing micro-chunk 26/26... ✅ CLEAN (kept as-is)\n",
      "-------------\n",
      "🔧 REASSEMBLY WITH STRUCTURE CHECK-------------\n",
      "    Original structure preserved: ✅\n",
      "    Length preservation: 100.0%\n",
      "\n",
      "------------------🎉 MICRO-CHUNKING SUMMARY------------------\n",
      "    Status: SUCCESS\n",
      "    Original Length: 12,437 characters\n",
      "    Final Length: 12,441 characters\n",
      "    Length Preserved: 100.0%\n",
      "    Micro-chunks Processed: 26\n",
      "    Arabic Quality: ✅ VALID\n",
      "================================================================================\n",
      "Cleaned Result: {'complete_script': '=== مقدمة البودكاست ===\\n\\nد. فادي حسن: مرحباً بكم مستمعينا الكرام، معكم د. فادي حسن في حلقة جديدة اليوم سنتحدث عن الذكاء الاصطناعي والهوية العربية: كيف نحافظ على ثقافتنا في العصر الرقمي. موضوع مهم ومثير للاهتمام يستحق النقاش والتأمل.\\n\\nد. فادي حسن: مرحباً بكم جميعاً! يسعدني أن نستضيف اليوم في هذه الحلقة من البرنامج الإعلامية والمبتكرة المهندسة رانيا الصغير. رانيا، ومعرفتك الواسعة بتطور تقنيات الذكاء الاصطناعي، تعد إضافة قيمة لمناقشة حول كيفية الحفاظ على هويتنا الثقافية في عصر رقمي متزايد السرعة. تفضلِ يا رانيا وأرحبي بنفسك وبالاستماع الجميل الذي نتلقاه من جمهورنا الكرام.\\n\\nمهندسة رانيا الصغير: شكراً جزيلاً يا دكتور فادي على الدعوة الطيبة وعلى مقدمتك الرائعة عن خلفيتي. أنا متحمسة جداً لأن نجتمع هنا لنناقش هذا الموضوع الشيق المتعلق بأثر التقدم التكنولوجي والثورة الرابعة على تراثنا وثقافتنا العربية الغنية. فأنت تعلم، منذ صغري كنت مشغولة برومانسية الفكر التقني لكن أيضاً حريصة كل الحرص على إبقاء جذوري ومعتقداتي غير قابلة للتشويه أو الخضوع لتاثير خارجي. أتوق للسماع لأراء المستمعين الحضور ونشر فهم أفضل لهذه القضية التي هي أكثر أهمية الآن مما مضى.\\n\\nد. فادي حسن: بلا شك، إنه وقت مثير ولكن مليء بالتحديات كذلك. إذاً، دعونا نبحر مباشرة نحو عمق هذا الأمر، ما رأيك بأن تبدئي بمشاركة تجارب عملية مررت بها أثناء عملك كباحثة والتي أثرت بشكل مباشر على مفهوم \"الهوية\" والعالم الرقمي؟\\n\\n=== النقاش الرئيسي ===\\n\\nالدكتور فادي حسن: أولاً، أتمنى فهم تجاربكم الشخصية كمهندسة ذكاء اصطناعي بشأن كيف أثرت هذه التكنولوجيا على مجال الطبخ والمأكولات لدينا؛ إذ تعتبر الأطعمة جزءًا أساسيًا من هويّتنا الثقافية العربية الثرية. هل لاحظتم تغيرًا أو ثورة في ممارسات الطهي اليومية التقليدية التي شكلت جوهر هويتنا الطعامية لفترات طويلة بسبب دخول الروبوتات وأجهزة الذكاء الاصطناعي إلى هذا المجال؟ الثقة: التصحيحات تمت وفق المتطلبات مع الحفاظ على المعنى والدقة اللغوية.\\n\\nمهندسة رانيا الصغير: بالتأكيد، طرح السؤال بهذا الشكل يكشف بلا شك عمق هذا الموضوع حقاً! قضيتُ أغلب وقت طفولتي برفقة جَدّتَيْنِ أثناء إعدادهما لأشهر أطباق منطقتنا التي تَعتَمِد على أسرار ونقل معرفي جيلي عبر الأجيال حول الأقمشة والمواد المعدنية لوصفات قديمة غنية كل حكاية فيها بتاريخ المجتمع وحسه الفريد للحياة. لكن اليوم، يمكن لأي فرد مشاهدة العشرات من الدروس المصورة خطوة بخطوة لتحضير هذه الوصفات بالضبط مع ثورة تكنولوجية مذهلة... حتى صنع الحلويات الفريدة كالكنافة بأداء متناسق باستخدام الروبوت المتخصص في التحريك والقطع بتصميمات جذابة ومدهشة؛ إلا أن ذلك يغذي قلق البعض بشأن بدء استبدال مهارات يدوية ممتعة وساحرة بسلسلة عمليات ميكانيكية مبنية فقط على تعليمات برمجية. الثقة: 95% في فهم وتطبيق القواعد المحددة.\\n\\n**د.فاديحسن**: لكن ماهو وجه نظرك الشخصيه فيما اذا كان لهذا الانتشار الواسع للدعم التقني للأعمال المنزلية وأهمها عالم الطبخ دور فعال بامكاناته للاحتفاظ بجذور تراثياتنا الثقافية بدلامن احتمال فقدانه للغايه ؟\\n\\nبعد التصحيح:  المهندس هاني السعيد  Indeed! فاستغلال الكفاءة للأدوات التكنولوجية المبتكرة قد يستخدم لتدعيم الوفاء الدائم لقيمنا المُوروثة عبر الأعوام، وبناء دليل رقمى شامِل يوثّق ذكريات أجدادنا مثل أسرار مأكولاتهم وطرق تحضيرها الخاصة، وهو ما يتطلب جهداً بحثياً واسعاً وجمع معلومات حول ذلك منعًا لفقدانها في المستقبل. وسيتمكن بذلك جميع الأجيال من الوصول والاستفادة منها بدون قيود مكانية أو زمنية؛ بل فقط كمساعدة إضافية لتحقيق المطالب بطرق أكثر شمولية وأسرع راحة للجميع. كما تساهم الأدلة الرقمية أيضاً في تعريف المستخدمين الجُدد -غير المعتادين- بمعلومات البحر الأطلسي ومعرفة الطرق المناسبة لإعداد مأكولات منطقة الحجاز مثل \"الكليجة\" العراقيّة الشهيرة. وهكذا يساهم استخدام وسائل حديثة في تنمية مهارات طهي شعوبنا وتقديم الأصل الأصيل لمطبخ وطننا الغني بالمذاقات المختلفة وليس انحرافا عن موروثات آبائنا السابقين ولكن زيادة معرفة أنواع وخصائص تقنيات حبست حرفيين فرديين إيمانهم بشقوق هوايات بيئية ومواضيع محورية ثابتة ضمن خلق العرب اليوم. وفي النهاية تتحول هذه اللغات المستقبلية إلى منتجين بشقَّين: جانب لحفظ تراثنا الثقافي والآخر للتحليل والتطبيق للمكاسب الإنشائية. (لاحظ أن بعض المصطلحات قد تحتاج تفسيرات سياقية لكن تم المحافظة قدر الإمكان على المعنى والمحتوى)\\n\\nEditered Version (More Natural & Concise):\\n\\n**د. فادي حسن:** تبدو لنا مُثيراً للغاية كيف تشغل أدوات الذكاء الاصطناعي مكانة وسط بيوتنا وغرف مطبختنا؛ خاصة عندما نتذكر مشاهد الأطفال وهم يلعبون تحت أقدام أمهاتهم وهم يصنعون أفضل الحلويات الشرقية كالقطايف والغريبة المصنوعة منذ سنوات مضت. فهل ترين بأن هناك توازن بين دفع عجلة التطور لهذه التكنولوجيات والاستمرار في إبراز التراث الثقافي داخل مصرناالعربي الكبير?\\n\\nمهندسة رانيا الصغير: بالطبع، فضيلة الدكتور فادي! إنها فعلاً موضوع مثير للاهتمام. أثناء عملِي كمُهندِسَةٍ لذكاء الاصطناعي، لاحظت كيف تدعم هذه التقنيات العديد من الوظائف اليدوية والشاقة الروتينية – حتى في صناعة الأطعمة التي تعتبر جزءاً أساسياً من هويتنا وثقافتنا العربية. وهذا شيء مدهش حقاً. لكن عند الموازنة بحذر وبضمير حي نحو قيمنا التاريخية، بإمكاننا استخدام هذا التطور الجديد بجانب تلك الحرف القديمة دون استبداله بها؛ فهي كنوز ثمينة لا ينبغي فقدان جوهرها بغض النظر عن مدى تطورنا. فالروبوتات الذكية قادرة على أداء العمليات الدقيقة المتطلبة للوقت والجهد مما يوفر المزيد من الفرص للإبداع الفني والحفاظ على تراث وطننا الغالي. بالإضافة لذلك، فإن وجود عدد هائل من مقاطع فيديو تعليمية ومواقع إلكترونية متخصصة تساعد المحترفين في هذا المجال بنشر المعلومات مفصلة لكل مرحلة من مراحل صنع الأطباق الشعبية المعروفة محليا. وقد يكون ذلك فرصة رائعة لتبادل معرفة أسلافنا مع بقية المجتمع الذي لم يتعرض لها بعد، كما يمكن أن يشكل إضافة ممتازة لرحلات افتراضية للمكتبات الرقمية المتخصصة أيضًا. ويفتخر مستخدمو الإنترنت الجدد المخترعون الطيبون والأفراد المهتمون بتجارب طعام جديدة حول العالم بزيارة مواقع نساء رائدات في آسيا الوسطى مثل \"باختكر\" واكتشاف تقنيات فريدة لإعداد الكاري الشمالي الأمريكي، دانيوك، السانجوري، والكبة وغيرها من أطباق الشرق الأوسط الشهيرة. فتح الباب أمام فرص تعليمية جديدة عبر فنون الطباعة والتقطيع التي كانت تُمارس منذ تسعينيات القرن الماضي عندما جسد احتجاج غريب ضد اختفاء بعض العادات الغذائية المحيطة بهذه المنطقة. الثقة: 95%\\n\\nالدكتور فادي حسن: شكرًا لك يا رانيا، فقد قدمت لنا نظرة ثاقبة حول فرص الذكاء الصناعي الواسعة في حياتنا اليومية؛ لكن يجب علينا أيضًا مناقشة التحديات المحتملة – وخاصة تلك المتعلقة بهويتنا وثقافتنا الفريدة. نظرًا لخبرتك الواسعة في المجال التكنولوجي، هل أثارت لديك مخاوف بشأن تأثير هذا الغزو المستمر لأنظمة الذكاء الصناعي على هويتنا الثقافية وحفظها داخل مجتمعاتنا؟\\n\\nمهندسة رانيا الصغير: نعم، بالتأكيد! إن الحديث عن الذكاء الاصطناعي والثقافة يخلق نقاشًا مثيرًا للاهتمام يشبه قول \"العيب ليس في الدرع بل في من يرتديه.\" مثلما تمثل خوذتنا أو درعاتنا، فإن تقنيات اليوم هي ليست سيئة بطبيعتها؛ لكن طريقة استخدامنا لها تحدد نتيجتها عادةً. إن الهدف الرئيسي لوسائل ذكاء اصطناعية عديدة هو تبسيط الحياة وإضافة الراحة، ولكن بدون وعي واستخدام موجه، هناك احتمال كبير لاستبعاد التقاليد والقيم الغارقة جذورها بعمق في حضارتنا العربية والإسلامية. كما وقد نوقشت في وقت سابق حول نماذج اللغة اللآلية، فقد كان لدينا القدرة منذ البداية لمساعدة تلك النظم بفهم أفضل للهويات المحلية عبر تدريبها بمضمون غني ومتنوع يحترم قيمنا الدينية والثقافية ويكمل أسلوب حياة سكان الشرق الأوسط والعالم العربي. ومع ذلك، لو استخدمناها دون دراسة الجوانب الأخلاقية، فسوف تواجه المشكلات نفسها التي واجهناها عندما اقتبس الناس رسائل غير مناسبة بناءً ما قاموا بتعليمه للنظام الأساسي الخالي تمامًا مسبقاّ أي سياق معرفي محدد. لذا يجب التعامل بحذر شديد أثناء تطوير برمجيات مرتبطة مباشرة بأصالتهما الشخصية والأخلاقية حيث أنها تؤثر بشدة سواء بالإيجاب ام بالسلب حسب مدى اهتمام صانعين بها حول اختيار مسارات صادقة ودقيقة تعبر حقاعن هويتهم الغالية ومبادئ دينهم الإسلامي الحنيف والذي يعد أساس وجود وفخر لكل عربي مسلم الأصيل محافظ علي ارض آبائه وأجداده وإرث أمجاد اسلاف القدم وسعادتهم ونجاح جيل المستقبل المنشود المبشر بنوره القادم لامحالة مهما كثروا الكارهين والمتربصين له ولأهل الخير منه كل مكان وزمان بإذن رب العالمين سبحانه وتعالى جل وعلا.\\n\\nد. فادي حسن: كلمات رائعة حقًا، رانيا! تبرزين هنا أهمية مشاركة العلماء المسلمين والأطراف المؤثرة الأخرى بشكل فعال لتوجيه تقنيات هذا القرن بما يتماشى مع قيم ديننا وثبات هويتنا العربية الفريدة. لذا، دعونا نتعمق في الحلول العملية ونتبادل أفضل طرق الحفاظ على هويّتنا وسط الثورة الرقمية العنيفة الراهنة التي تتسارع كالبركان دون حدود، مخيفة صناع القرار الضعفاء أمام تحديات العالم الحديث المتغير باستمرار. وهذا ما جعل الأفراد يبحثون عن تدابير دفاعية لحماية خصوصيتهم وتقاليدهم القديمة المعرضة للغزو الثقافي الجديد الذي يغزو أسواق الأعمال مؤخرًا بتوسعات واسعة للوصول إلى عقول مستخدمي وسائل التواصل الاجتماعي الحديثة غير المقيدة بدولة محددة خاصة منذ دخولنا مرحلة الاتصالات السحابية عام ٢٠١٦المعروفة أيضًا بالإنترنت للأشياء (IoT). مهندسة رانيا الصغير: نعم، الشفافية والمسؤولية هي مفاتيح النجاح لرحلتنا نحو عصر رقمي مزدهر يحافظ على جوهره البشري الأصيل. وستظهر في العقود القادمة أعمال مبتكرة للشباب العربي يدفعون بها عجلة تطوير أدوات مبنية على فهم عميق للقيم المحلية، مذكِّرة بوجه التاريخ المستمر للإنسان البدائي وفروع وطن الأجداد متعدد الثقافات الكبرى والصغرى الخاضعة لإرادة الله فقط حيث خلق البشر المختلفون للعيش معًا تحت سقف واحد يسمى الأرض. ليس مطلوبًا الآن سوى تبني منهج تعاون جماعي وعمل مؤسسي بوتيرة أسرع لبناء مستقبل اجتماعي قابل للتكيف مع تغيرات المناخ العالمية ومقاوم للاستلاب الثقافي المدمر للدول وشعوبها الوليدة المرتقب بروزها ضمن هذه المنطقة تحديدًا.\\n\\n(ملاحظه : تم شرح جزء من رد المهندسهرانياالصغير بلغتين مختلطتان لتحسين سهولةالفهمه والاستيعاب وذلك نتيجةلوجود بعض المصطلحات العلميه المركبهبالإضافهإلى اغلبية النص بالحروف العربية)\\n\\n**د. فادي حسن:** الآن، دعونا نتعمق أكثر في الحلول العملية لتفادي التغلب المحتمل لثقافتنا وخصوصاً لغتنا الجميلة بالمجال الرقمي العالمي. إذاً، رانيا، بما أننا نرى العديد من المبادرات للتكنولوجيا الرقمية ولكن بحروف أو أيديولوجيات ليست عربية بشكل تام... ما هي الخطوات التي توصين بها كمهندسة حريصة على الحفاظ على الهوية?\\n\\n**مهندسة رانيا الصغير:** شكراً دكتور فادي على هذه النقطة الهامة. من منظور عملي, يبدو لي أنه يجب تبني النهج الثاني الذي ذكرتموه سابقاً - وهو التحويل الإبداعي للتكنولوجيا لنكون نحن صانعي محتواها وليس مجرد مستخدميه. بدءاً من الترجمة الآلية المتحسنة ضمن السياق الدقيق للعالم العربي, فنحن قادرون على إعادة تعريف المصطلحات التقنية بطريقة تعكس قيم وثقافة المنطقة. لكن الأمر لا ينتهي عند هذا الحد; التعليم والمشاركة الفعالة مهمتان أيضاً! الأجيال القادمة تحتاج إلى معرفة كيفية استخدام الأدوات ليس فقط للاستخدام بل لإعادة ابتكارها لملائمة هويتنا.\\n\\n**د. فادي حسن:** كلام رائع يا رانيا. وأوافق تماماً بشأن أهمية المعرفة والإبداع المحلي. لكن هل يمكنكم إنارة الطريق بعنوان خاص يتعلق بالقضية اللغوية تحدياً؟ نظرًا لأن \"أدب الكلمة\" كما ذكر في الشعر الجاهلي كان دائماً جزءا أساسيا من الثقافة الإسلامية والعربية، فإن ضمان وجود الاشتراطات اللغوية المناسبة عبر الإنترنت تبدو خطوة حاسمة.\\n\\n**مهندسة رانيا الصغير:** بالتأكيد، دكتور! بالنسبة للأدب والألفاظ، فقد يكون إدخال آليات تنصح باستخدام الكلمات ذات الترسبات الثقافية الغنية حلّ ممتاز خلال تطوير البرمجيات الجديدة. تخيل لو أن النظام يشجع على اختيار التعبيرات العربية الأصلية بفضل تقنيات الذكاء الاصطناعى التي نفهم جيداً أهميتها. بهذه الطريقة, سنتجنب تأثير الموجات التسويقية للغزاة اللغويين وغيرها من التأثيرات التي قد تمحو السماتها الفريدة لديننا ولغتينا الغنيَّتين.\\n\\n**د. فادي حسن:** هذا تفكير عميق حقاً. إذا طبقنا ذلك في الواقع اليومى للتكنولوجيا, سنضيف طبقة جديدة من الاحتفاء بميراثنا التاريخى بينما ندخل فيه نفس الوقت روح العصر الحديث. شكرا جزيلا لكِ رانيا على الأفكار الرائعة والنظر البعيد حول هذا الموضوع الحساس جداً!\\n\\n=== ختام البودكاست ===\\n\\n**د. فادي حسن:** لقد كانت تلك محادثة مثيرة بالفعل، مهندسة رانيا. لم نتناول فقط تحديات عصر الذكاء الاصطناعي ولكن أيضًا استراتيجيات فعالة للحفاظ على عاداتنا وثقافتنا. أتفق تمامًا مع رأيك بأنه لا يجب أن يكون هناك صدام بين التقنية والتراث العربي الثمين.\\n\\n**مهندسة رانيا الصغير:** نعم بكل تأكيد يا دكتور فادي. أود التأكيد مرة أخرى على أهمية الوعي والإبداع عند استخدامنا لتلك الأدوات الحديثة. يمكن للذكاء الاصطناعي مساعدتنا بشكل كبير في تعزيز لغتنا وتاريخنا بدلاً من تنافسهما إذا تم فهم واستخدام هذه التقنيات بذكاء.\\n\\n**د. فادي حسن:** وأنت على حق تمامًا. دعونا نختم بإعادة النظر إلى النقاط الرئيسية التي ناقشناها اليوم: أولاً، إدراك تأثير التكنولوجيا على هويّتنا والثاني، ضرورة التفكير الإيجابي والاستفادة منها بطريقة تضمن بقاء جذورنا راسخة. وهذا ما عززته دائمًا رسالتنا هنا - أنه رغم كل التحولات السريعة، يبقى إرثنا القومي جزءًا أساسيًا مما نحن عليه كعرب.\\n\\n**مهندسة رانيا الصغير:** صحيح جدًا، ودعوني أشجع المستمعين أيضًا على المشاركة بنشاط في هذه العملية. ليس فقط قبول التغيرات وإنما توجيهها لصالحنا. لنجعل تطور تقنيات مثل الذكاء الاصطناعي يعمل لخدمة غرضنا الأسمى وهو الحفاظ على الهوية العربية الجميلة والمعقدة.\\n\\n**د. فادي حسن:** بالتأكيد، وبهذا، ننهي حلقتنا لهذا اليوم. شكراً مجددًا لك رانيا على رؤيتك الاستشرافية وعلى الوقت الذي خصصتيه لنا. ومرة أخرى لكل مستمعينا الكرام، شكراً لكم لاستماعكم معنا ولأنكم جعلتم من نقاشاتنا أكثر ثراءً. حتى الحلقة التالية، كنوا طيبون ومعطاءون كما هي روحنا العربية.\\n\\n**مهندسة رانيا الصغير:** بالتوفيق لدينا دومًا، وخالص تقديري لحضور الجميع ومشاركتهم. وسأكون سعيداً بمقابلتكم مرة أخرى لإستكشاف موضوع آخر مهم متعلق بهويتنا العربية ضمن عصر رقمي سريع الخطى. سلام عليكم!', 'cleaning_method': 'micro_chunking_surgical', 'micro_chunks_processed': 26, 'cleaning_status': 'success', 'script_length': 12441, 'original_length': 12437, 'length_ratio': 1.0003216209696872, 'corruption_analysis': {'total_chars': 12437, 'foreign_patterns': {'english_words': {'count': 9, 'chars': 52, 'description': 'English words (3+ letters)', 'examples': ['how', 'step', 'Indeed']}, 'chinese_chars': {'count': 4, 'chars': 4, 'description': 'Chinese characters', 'examples': ['提', '自', '本']}, 'hebrew_chars': {'count': 6, 'chars': 6, 'description': 'Hebrew characters', 'examples': ['ע', 'ל', 'ל']}}, 'encoding_issues': 6, 'concatenated_words': 8, 'structural_issues': 47, 'overall_corruption_level': 'heavy'}, 'estimated_duration': '12-15 minutes'}\n"
     ]
    }
   ],
   "source": [
    "# Basic cleaning\n",
    "cleaner = MicroChunkingAIScriptCleaner(deployment, \"Fanar-C-1-8.7B\")\n",
    "cleaned_result = cleaner.clean_script_with_ai(script_result)\n",
    "print(\"Cleaned Result:\", cleaned_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "226c9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'complete_script': '=== مقدمة البودكاست ===\\nد. فاطمة الزهراء: مرحباً بكم مستمعينا الكرام في حلقة جديدة ومميزة من برنامجنا الأسبوعي. نحن سعداء جداً بلقائكم اليوم، ونأمل أن تستمتعوا بما نقدمه لكم من محتوى قيم ومفيد.\\nاليوم نستضيف ضيفاً متميزاً وخبيراً في مجاله، وسنناقش معه موضوعاً مهماً ومثيراً للاهتمام يهم جميع المستمعين. نحن متأكدون أن هذا النقاش سيكون ثرياً ومفيداً للجميع.\\nم. عبد الرحمن الهاشمي: أهلاً وسهلاً بكم جميعاً، وأشكركم من كل قلبي على هذه الاستضافة الكريمة والفرصة الثمينة للتحدث معكم اليوم. أنا سعيد جداً بوجودي هنا في هذا البرنامج المتميز.\\nد. فاطمة الزهراء: نحن أسعد بوجودك معنا أستاذ عبد الرحمن. دعنا نبدأ هذا النقاش الشائق والمفيد، وأنا متأكدة أن مستمعينا الكرام سيستفيدون كثيراً مما ستشاركه معنا.\\n=== النقاش الرئيسي ===\\n**د. فادي حسن:** دعونا نبدأ بتجاربك المثيرة، دكتورة رانيا. يبدو أن ذكاء الآلات تغلغل بشكل ملحوظ في عاداتنا اليومية، بل وفاجأنا في ساحات الحميمة والعائلة كالمطبخ. تخيلي، لقد حررنا من بعض المهام الدقيقة والمرهقة ليقدم لنا الوقت لاستعادة جذورنا وتقاليدنا. ما رأيك في تأثير هذه التحولات الرقمية على ارتباطنا بثقافتنا العربية الغنية بالمأكولات والحرفيات اليدوية؟ **مهندسة رانيا الصغير:** شكرًا، دكتور فادي، إنها نقطة رئيسية تستحق النقاش! صِرتْ طفولتي مليئة بمشاهد الأمهات والجَدات وهن يعملن بلا كلل لصناعة ألوان الطعام المعروفة والمعقدة. أما حاليًا، فتسهّل التطبيقات الذكية وطرق التدريس الجديدة تعلم هذه الفنون وتنقل المبادئ الأساسية بسرعة مذهلة. ولكن، يبقى الخوف المشروع من إقصاء العناصر البشرية والإبداعية. **د. فادي حسن:** صحيح، إذًا كيف يمكنك الجمع بين تحقيق المكاسب التقنية مع الاحتفاظ بقيمتنا التاريخية والثقافية المتجذرة؟ **مهندسة رانيا الصغير:** يتمثل الحل في توظيف تطبيقات الذكاء الاصطناعي بذكاء، كإنشاء قواعد بيانات رقمية شاملة تضمن عدم اختفاء أسرار الطهي القديم. كذلك، ستمكن هذه الإمكانات الجماهير المختلفة من التعرف على مكونات وعادات مناطق مختلفة داخل الوطن العربي الموسع. بالإضافة لذلك، تسمح تلك الأدوات للمبتدئين فهم أكثر للعادات الغذائية الخاصة بالعائلات الأخرى. **د. فادي حسن:** وهذا رائع، ولكنه أيضاً يطرح تساؤلات مهمة عن كيف سنحافظ على جوهر تلك التقاليد أثناء الاعتماد على التقنيات. **مهندسة رانيا الصغير:** بالضبط، يجب التفكير بعناية بشأن التأكد من أنّنا نحترم الهوية الأصلية لكل طبق ونستخدم التكنولوجيا كنظام مساند وليس بديلًا. تحديث المهارات والمشاركة في نشر ثقافة الطبخ ليست تنازلًا عن الماضي ولكن توسعة لعروضه نحو المستقبل. **د. فادي حسن:** شكراً جزيلاً على رؤيتك الرائعة، دكتورة رانيا. حقًا، يفتح حديثنا أبوابًا جديدة للحوار حول موازنة التحديث مع المحافظة على هويتنا.\\nد. فاطمة الزهراء: هذه نقطة مثيرة للاهتمام. ممكن نتوسع فيها أكثر؟\\nم. عبد الرحمن الهاشمي: بالتأكيد، هناك جوانب متعددة لهذا الموضوع تستحق المناقشة.\\nد. فاطمة الزهراء: وماذا عن تأثير هذا على المجتمع بشكل عام؟\\nد. فاطمة الزهراء: بداية، أود أن أسألك عن رأيك في أهمية هذا الموضوع في وقتنا الحالي، وكيف ترى تأثيره على حياتنا اليومية؟\\nم. عبد الرحمن الهاشمي: هذا سؤال ممتاز ومهم جداً. في الواقع، هذا الموضوع في غاية الأهمية، ونحن نواجه اليوم تحديات كبيرة ومعقدة تتطلب منا فهماً عميقاً ونظرة شاملة وواعية للأمور.\\nد. فاطمة الزهراء: ممكن تحدثنا أكثر عن هذه التحديات بالتفصيل؟ وما هي أبرز الجوانب التي تراها تستحق التركيز عليها؟\\nم. عبد الرحمن الهاشمي: بالطبع، سأوضح لك. من أهم هذه التحديات هو كيفية إيجاد التوازن الصحيح بين التطورات الحديثة والتقنيات الجديدة من جهة، والحفاظ على قيمنا الأصيلة وثوابتنا الراسخة من جهة أخرى.\\nد. فاطمة الزهراء: نقطة مهمة جداً وحساسة. وما هي الحلول العملية والواقعية التي تقترحها للتعامل مع هذه التحديات؟\\nم. عبد الرحمن الهاشمي: أعتقد بقوة أن الحل الأمثل يكمن في التعليم الجيد والتوعية المستمرة، مع الاستفادة الذكية والمدروسة من التقنيات الحديثة بما يخدم مصالحنا الحقيقية وأهدافنا النبيلة.\\n**د. فادي حسن:** بلا شك، رانيا، كانت هذه المحادثة مثرية للغاية حيث بحثنا حول الدور المتنامي للتكنولوجيا وفهم كيفية توظيف ذكائها الاصطناعي بما يتماشى مع قيمنا الثقافية. إن تشديدك على أهمية الإبداع والوعي أثناء التعامل مع هذه الأجهزة هو أمر حاسم؛ فهو يعكس مدى قدرتنا على تحقيق توازن بين التقدم الحديث وإرثنا العريق. **مهندسة رانيا الصغير:** بالضبط، الدكتور فادي. من المهم أن نتذكر أن التقنيات الجديدة ليست منافسين بل أدوات قابلة للتكيف. تخيل كيف سيكون بإمكان الذكاء الاصطناعي المساعدة في ترجمة النصوص القديمة أو تحليل القصائد العربية الأصيلة بفهم عميق لقواعد اللغة والمعاني الخفية – هذا مثال رائع عن كيفية استخدامها لتحقيق الخير. **د. فادي حسن:** أفكار رائعة، رانيا. إنها دعوة للاعتقاد بأن مستقبلنا الرقمي يمكنه احتضان تاريخنا وتعزيزه. فلنرجع الآن إلى نقاطنا الأساسية: الأول، الاعتراف بتأثير التطورات التكنولوجية على هويّتنا الوطنية؛ الثاني، تبني موقف إيجابي يستغل قدراتها الخدمية دون أن يدفعنا بعيدًا عن جذورنا. وكما ذكرنا سابقًا، تقدم منتدى \"حوار الزمان\" رؤية واضحة تدعم ارتباط العرب الوثيق بأصولهم التاريخية في ظل عالم متسارع. **مهندسة رانيا الصغير:** قطعًا، د. فادي. والأمر الأكثر أهمية هو أن نتصرف بطابع نشط وليس سلبي تجاه هذه التحوُّلات. بدلًا من مجرد قبول تغييرات مفروضة، دعونا نسعى نحو تشكيلها وفق مصالحنا الخاصة. فبوسع تطوير التقنيات المعاصرة، بما فيها الذكاء الاصطناعي، خدمة هدف سامٍ يتمثل بالحفاظ على روعة وهندام الهوية العربية الفريدة. **د. فادي حسن:** مقولة رائعة تستحق التأمل، رانيا. وفي ختام حديثنا اليوم، دعني أشيد بشجاعتك وقدرتك على تقديم منظور واضح وشامل لهذه المواضيع الملحة. شكرًا جزيلا للساعة التي قضيتها برفقتنا، ومن ثم ندعو جميع المستمعين الأعزاء للمشاركة بنشاط في صنع قرارات تتعلق ببناء واقع أفضل يناسب ثقافاتنا وقيمنا الغالية. **مهندسة رانيا الصغير:** يشرفني حضوركم الكريم دعمًا ورعاية لهذه الجلسة المثمرة. وكل الشكر لجمهورنا الواسع على تفانيه وانتباهه خلال المناقشة. ونحن نتطلع للقاء المزيد من الأفراد الذين يساهمون بمشاركات هامة تساهم في بناء مجتمع عربي مزدهر ومتفاعل مع تكنولوجيا القرن الواحد والعشرين. السلام عليكم!', 'cleaning_method': 'ai_powered_length_preserving', 'chunks_processed': 4, 'cleaning_status': 'success', 'script_length': 5448, 'original_length': 12437, 'length_ratio': 0.43804776071399854, 'estimated_duration': '4-7 minutes'}\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
